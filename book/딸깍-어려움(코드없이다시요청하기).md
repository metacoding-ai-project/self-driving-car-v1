# ğŸš— ììœ¨ì£¼í–‰ ì‹œë®¬ë ˆì´í„° í”„ë¡œì íŠ¸ ì™„ì „ ëª…ì„¸ì„œ

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **DQN(Deep Q-Network) ê°•í™”í•™ìŠµ**ì„ ì‚¬ìš©í•œ **ììœ¨ì£¼í–‰ ì‹œë®¬ë ˆì´í„°**ì…ë‹ˆë‹¤.
5ê°œì˜ ë²„ì „(v1~v5)ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ê° ë²„ì „ì€ ì´ì „ ë²„ì „ì˜ ë¬¸ì œì ì„ í•´ê²°í•˜ë©° ë°œì „í•©ë‹ˆë‹¤.

### ëª©ì 
- ê°•í™”í•™ìŠµì˜ ê³¼ì í•©(Overfitting) ë¬¸ì œ ì´í•´ ë° í•´ê²°
- ì¼ë°˜í™”(Generalization) ë‹¬ì„± ë°©ë²• í•™ìŠµ
- Learning Rate Scheduling ì´í•´
- Policy > Cache ì›ì¹™ì„ í†µí•œ ê°•í™”í•™ìŠµ ì² í•™ ì´í•´

### ê¸°ìˆ  ìŠ¤íƒ
- **ì–¸ì–´**: Python 3.8+
- **í”„ë ˆì„ì›Œí¬**: PyTorch (ë”¥ëŸ¬ë‹), Pygame-CE (ì‹œê°í™”)
- **ë¼ì´ë¸ŒëŸ¬ë¦¬**: NumPy, Matplotlib

---

## ğŸ—ï¸ ì „ì²´ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
python_lab/
â”œâ”€â”€ simulator-v1/          # ê¸°ë³¸ ë²„ì „ (ê³¼ì í•© ë¬¸ì œ ì¡´ì¬)
â”‚   â”œâ”€â”€ agent.py           # DQN ì—ì´ì „íŠ¸
â”‚   â”œâ”€â”€ car.py             # ìë™ì°¨ ê°ì²´
â”‚   â”œâ”€â”€ config.py          # ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ environment.py     # ê²©ì í™˜ê²½
â”‚   â”œâ”€â”€ main.py            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
â”‚   â”œâ”€â”€ train.py           # í•™ìŠµ ì‹¤í–‰
â”‚   â””â”€â”€ README.md          # ë²„ì „ ì„¤ëª…
â”‚
â”œâ”€â”€ simulator-v2/          # ê³¼ì í•© í•´ê²° ë²„ì „
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ car.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ environment.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ simulator-v3/          # ì™„ì „í•œ ì¼ë°˜í™” ë²„ì „
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ car.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ environment.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ simulator-v4/          # Learning Rate Scheduling ë²„ì „
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ car.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ environment.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ simulator-v5/          # Policy > Cache ì›ì¹™ ë²„ì „
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ car.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ environment.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ BOOK.md                # ì „ì²´ íŠœí† ë¦¬ì–¼ í†µí•© ì±…
```

---

## ğŸ“¦ ê³µí†µ íŒŒì¼ êµ¬ì¡° ë° ì—­í• 

ëª¨ë“  ë²„ì „(v1~v5)ì€ ë™ì¼í•œ íŒŒì¼ êµ¬ì¡°ë¥¼ ê°€ì§€ë©°, ê° íŒŒì¼ì˜ ì—­í• ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

### 1. `config.py` - ì„¤ì • íŒŒì¼
**ì—­í• **: ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì‹œë®¬ë ˆì´ì…˜ ì„¤ì • ê´€ë¦¬

**ê³µí†µ ë‚´ìš©**:
```python
# ì†ë„ í”„ë¦¬ì…‹
SPEED_SLOW = 10      # ì²œì²œíˆ ê´€ì°° (ì´ˆë³´ììš©)
SPEED_NORMAL = 30    # ë³´í†µ ì†ë„ (ì¶”ì²œ)
SPEED_FAST = 60      # ë¹ ë¥´ê²Œ (í•™ìŠµ ì´í•´í–ˆì„ ë•Œ)
SPEED_ULTRA = 120    # ì´ˆê³ ì† (í…ŒìŠ¤íŠ¸ìš©)

# í˜„ì¬ ì†ë„ ì„¤ì •
CURRENT_SPEED = SPEED_NORMAL

# ê²©ì ì„¤ì •
GRID_SIZE = 20        # ê° ê²©ì í¬ê¸° (í”½ì…€)
GRID_WIDTH = 30       # ê°€ë¡œ ê²©ì ìˆ˜
GRID_HEIGHT = 30      # ì„¸ë¡œ ê²©ì ìˆ˜

# í•™ìŠµ ì„¤ì •
NUM_EPISODES = ???    # ë²„ì „ë§ˆë‹¤ ë‹¤ë¦„
BATCH_SIZE = ???      # ë²„ì „ë§ˆë‹¤ ë‹¤ë¦„
LEARNING_RATE = ???   # ë²„ì „ë§ˆë‹¤ ë‹¤ë¦„

# í™”ë©´ í‘œì‹œ ì„¤ì •
SHOW_TRAINING = True  # Falseë¡œ í•˜ë©´ í™”ë©´ ì—†ì´ ë¹ ë¥´ê²Œ í•™ìŠµ
```

### 2. `environment.py` - ê²©ì í™˜ê²½
**ì—­í• **: ìë™ì°¨ê°€ ì›€ì§ì¼ ê²©ì í™˜ê²½ ì œê³µ (ë²½, ì‹œì‘ì , ëª©ì ì§€)

**í•µì‹¬ ê¸°ëŠ¥**:
- `__init__()`: í™˜ê²½ ì´ˆê¸°í™”, ë§µ ìƒì„±
- `get_state()`: í˜„ì¬ ìƒíƒœë¥¼ ì‹ ê²½ë§ ì…ë ¥ìœ¼ë¡œ ë³€í™˜ (ì„¼ì„œ ì •ë³´)
- `draw()`: Pygameìœ¼ë¡œ í™”ë©´ì— ê·¸ë¦¬ê¸°
- `is_goal()`: ëª©ì ì§€ ë„ë‹¬ ì²´í¬
- `is_wall()`: ë²½ ì¶©ëŒ ì²´í¬
- `reset_map()`: ë§µ ë¦¬ì…‹ (v2ë¶€í„°)
- `update_dynamic_obstacles()`: ë™ì  ì¥ì• ë¬¼ ì—…ë°ì´íŠ¸ (v5 ì „ìš©)

**ìƒíƒœ í‘œí˜„ (ì„¼ì„œ)**:
```python
# 8ë°©í–¥ ì„¼ì„œ (ê° ë°©í–¥ì˜ ì¥ì• ë¬¼ê¹Œì§€ ê±°ë¦¬)
sensors = [
    distance_up,      # 0: ìœ„
    distance_down,    # 1: ì•„ë˜
    distance_left,    # 2: ì™¼ìª½
    distance_right,   # 3: ì˜¤ë¥¸ìª½
    distance_up_left,    # 4: ëŒ€ê°ì„  ì™¼ìª½ ìœ„
    distance_up_right,   # 5: ëŒ€ê°ì„  ì˜¤ë¥¸ìª½ ìœ„
    distance_down_left,  # 6: ëŒ€ê°ì„  ì™¼ìª½ ì•„ë˜
    distance_down_right  # 7: ëŒ€ê°ì„  ì˜¤ë¥¸ìª½ ì•„ë˜
]

# ëª©ì ì§€ ë°©í–¥
goal_dx = (goal_x - car_x) / GRID_WIDTH   # ì •ê·œí™”ëœ x ë°©í–¥
goal_dy = (goal_y - car_y) / GRID_HEIGHT  # ì •ê·œí™”ëœ y ë°©í–¥

# ìµœì¢… ìƒíƒœ ë²¡í„° (10ì°¨ì›)
state = sensors + [goal_dx, goal_dy]
```

**ë§µ íƒ€ì…**:
- ë¹ˆ ë§µ: ë²½ ì—†ìŒ
- ë‹¨ìˆœ ë§µ: ëª‡ ê°œì˜ ë²½
- ì¤‘ê°„ ë§µ: ë¯¸ë¡œ í˜•íƒœ
- ë³µì¡ ë§µ: ë³µì¡í•œ ë¯¸ë¡œ
- ëœë¤ ë§µ: ì™„ì „íˆ ëœë¤ ìƒì„±

### 3. `car.py` - ìë™ì°¨ ê°ì²´
**ì—­í• **: ìë™ì°¨ì˜ ìƒíƒœ ê´€ë¦¬ ë° í–‰ë™ ìˆ˜í–‰

**í•µì‹¬ ì†ì„±**:
```python
class Car:
    def __init__(self, x, y):
        self.x = x                    # í˜„ì¬ x ìœ„ì¹˜
        self.y = y                    # í˜„ì¬ y ìœ„ì¹˜
        self.direction = 0            # í˜„ì¬ ë°©í–¥ (0:ìœ„, 1:ì•„ë˜, 2:ì™¼, 3:ì˜¤)
        self.score = 0                # ëˆ„ì  ë³´ìƒ
        self.steps = 0                # ì´ë™ ìŠ¤í… ìˆ˜
        self.visited_positions = set()  # ë°©ë¬¸í•œ ìœ„ì¹˜ (v2ë¶€í„°)
```

**í–‰ë™ (Actions)**:
```python
# 3ê°€ì§€ í–‰ë™
ACTION_FORWARD = 0    # ì§ì§„
ACTION_RIGHT = 1      # ìš°íšŒì „ í›„ ì§ì§„
ACTION_LEFT = 2       # ì¢ŒíšŒì „ í›„ ì§ì§„
```

**ë³´ìƒ ì‹œìŠ¤í…œ**:
```python
def move(self, action, environment):
    # 1. ë°©í–¥ ë³€ê²½
    if action == ACTION_RIGHT:
        self.direction = (self.direction + 1) % 4
    elif action == ACTION_LEFT:
        self.direction = (self.direction - 1) % 4

    # 2. ì•ìœ¼ë¡œ ì´ë™
    next_x, next_y = self._get_next_position()

    # 3. ë³´ìƒ ê³„ì‚°
    reward = -0.01  # ê¸°ë³¸ íŒ¨ë„í‹° (ì‹œê°„ ì§€ì—°)
    done = False

    # ë²½ ì¶©ëŒ
    if environment.is_wall(next_x, next_y):
        reward = -1.0
        done = True
    # ëª©ì ì§€ ë„ë‹¬
    elif environment.is_goal(next_x, next_y):
        reward = +10.0
        done = True
    # ì •ìƒ ì´ë™
    else:
        # v2ë¶€í„°: ê²½ë¡œ ë‹¤ì–‘ì„± ë³´ìƒ
        if (next_x, next_y) not in self.visited_positions:
            reward += 0.2  # ìƒˆë¡œìš´ ìœ„ì¹˜ ë°©ë¬¸
            self.visited_positions.add((next_x, next_y))

        # ê°™ì€ ê³³ë§Œ ë°˜ë³µí•˜ë©´ íŒ¨ë„í‹°
        if self.steps > 10 and len(self.visited_positions) < self.steps * 0.5:
            reward -= 0.1

        self.x, self.y = next_x, next_y

    self.score += reward
    self.steps += 1

    # ìµœëŒ€ ìŠ¤í… ì œí•œ
    if self.steps >= 500:
        done = True

    return reward, done
```

### 4. `agent.py` - DQN ì—ì´ì „íŠ¸
**ì—­í• **: ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ (DQN)

**í•µì‹¬ êµ¬ì¡°**:

#### ì‹ ê²½ë§ (DQN)
```python
class DQN(nn.Module):
    def __init__(self, state_size=10, action_size=3):
        super(DQN, self).__init__()

        self.network = nn.Sequential(
            nn.Linear(state_size, 128),   # ì…ë ¥ì¸µ (10 â†’ 128)
            nn.ReLU(),
            nn.Linear(128, 128),          # ì€ë‹‰ì¸µ (128 â†’ 128)
            nn.ReLU(),
            nn.Linear(128, action_size)   # ì¶œë ¥ì¸µ (128 â†’ 3)
        )

    def forward(self, x):
        return self.network(x)
```

#### DQN ì—ì´ì „íŠ¸
```python
class DQNAgent:
    def __init__(self):
        self.state_size = 10
        self.action_size = 3

        # ì‹ ê²½ë§ (Policy Network & Target Network)
        self.policy_net = DQN(self.state_size, self.action_size)
        self.target_net = DQN(self.state_size, self.action_size)
        self.target_net.load_state_dict(self.policy_net.state_dict())

        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.Adam(self.policy_net.parameters(),
                                     lr=LEARNING_RATE)

        # ê²½í—˜ ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬
        self.memory = deque(maxlen=10000)  # v1: 10000, v2+: 20000

        # íƒí—˜ íŒŒë¼ë¯¸í„°
        self.epsilon = 1.0        # ì´ˆê¸° íƒí—˜ í™•ë¥  (100%)
        self.epsilon_min = 0.01   # ìµœì†Œ íƒí—˜ í™•ë¥  (1%)
        self.epsilon_decay = ???  # v1: 0.995, v2+: 0.998

        # ê°ë§ˆ (í• ì¸ìœ¨)
        self.gamma = 0.99

        # Target Network ì—…ë°ì´íŠ¸ ë¹ˆë„
        self.target_update = 10
        self.update_counter = 0

    def select_action(self, state, training=True):
        """í–‰ë™ ì„ íƒ (Epsilon-Greedy)"""
        if training and random.random() < self.epsilon:
            # íƒí—˜: ëœë¤ í–‰ë™
            return random.randint(0, self.action_size - 1)
        else:
            # í™œìš©: ìµœì„ ì˜ í–‰ë™
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            with torch.no_grad():
                q_values = self.policy_net(state_tensor)
            return q_values.argmax().item()

    def remember(self, state, action, reward, next_state, done):
        """ê²½í—˜ ì €ì¥"""
        self.memory.append((state, action, reward, next_state, done))

    def replay(self):
        """ê²½í—˜ ë¦¬í”Œë ˆì´ë¥¼ í†µí•œ í•™ìŠµ"""
        if len(self.memory) < BATCH_SIZE:
            return

        # ë°°ì¹˜ ìƒ˜í”Œë§
        batch = random.sample(self.memory, BATCH_SIZE)

        states = torch.FloatTensor([s for s, a, r, ns, d in batch])
        actions = torch.LongTensor([a for s, a, r, ns, d in batch])
        rewards = torch.FloatTensor([r for s, a, r, ns, d in batch])
        next_states = torch.FloatTensor([ns for s, a, r, ns, d in batch])
        dones = torch.FloatTensor([d for s, a, r, ns, d in batch])

        # í˜„ì¬ Qê°’
        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1))

        # ë‹¤ìŒ Qê°’ (Target Network ì‚¬ìš©)
        with torch.no_grad():
            next_q = self.target_net(next_states).max(1)[0]
            target_q = rewards + (1 - dones) * self.gamma * next_q

        # ì†ì‹¤ ê³„ì‚°
        loss = nn.MSELoss()(current_q.squeeze(), target_q)

        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        loss.backward()

        # v2ë¶€í„°: Gradient Clipping
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)

        self.optimizer.step()

        # Target Network ì—…ë°ì´íŠ¸
        self.update_counter += 1
        if self.update_counter % self.target_update == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())

        # Epsilon ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def save(self, filename):
        """ëª¨ë¸ ì €ì¥"""
        torch.save(self.policy_net.state_dict(), filename)

    def load(self, filename):
        """ëª¨ë¸ ë¡œë“œ"""
        self.policy_net.load_state_dict(torch.load(filename))
        self.target_net.load_state_dict(self.policy_net.state_dict())
```

### 5. `train.py` - í•™ìŠµ ì‹¤í–‰
**ì—­í• **: ê°•í™”í•™ìŠµ í›ˆë ¨ ë£¨í”„ ì‹¤í–‰

**ê¸°ë³¸ êµ¬ì¡°**:
```python
import pygame
import random
from environment import GridEnvironment
from car import Car
from agent import DQNAgent
from config import *

def train():
    # ì´ˆê¸°í™”
    env = GridEnvironment()
    start_x, start_y = env.start_pos
    car = Car(start_x, start_y)
    agent = DQNAgent()

    # í†µê³„
    scores = []
    episode_lengths = []

    # í›ˆë ¨ ë£¨í”„
    for episode in range(NUM_EPISODES):
        # í™˜ê²½ ë¦¬ì…‹ (v2ë¶€í„°: ë§µ ë³€ê²½)
        # ì°¨ëŸ‰ ë¦¬ì…‹
        state = env.get_state(car.x, car.y, car.direction)

        while True:
            # í–‰ë™ ì„ íƒ
            action = agent.select_action(state, training=True)

            # í–‰ë™ ìˆ˜í–‰
            reward, done = car.move(action, env)
            next_state = env.get_state(car.x, car.y, car.direction)

            # ê²½í—˜ ì €ì¥
            agent.remember(state, action, reward, next_state, done)

            # í•™ìŠµ
            agent.replay()

            # í™”ë©´ ì—…ë°ì´íŠ¸ (SHOW_TRAINING=Trueì¸ ê²½ìš°)
            if SHOW_TRAINING:
                env.draw(car)
                # ... ì •ë³´ í‘œì‹œ
                pygame.display.flip()

            # ë‹¤ìŒ ìƒíƒœë¡œ
            state = next_state

            if done:
                break

        # í†µê³„ ê¸°ë¡
        scores.append(car.score)
        episode_lengths.append(car.steps)

        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if (episode + 1) % 100 == 0:
            avg_score = sum(scores[-100:]) / 100
            print(f"Episode {episode+1}/{NUM_EPISODES} | "
                  f"Avg Score: {avg_score:.2f} | "
                  f"Epsilon: {agent.epsilon:.3f}")

    # ëª¨ë¸ ì €ì¥
    agent.save("model_final.pth")

    # í•™ìŠµ ê·¸ë˜í”„ ìƒì„±
    plot_results(scores, episode_lengths)

if __name__ == "__main__":
    train()
```

### 6. `main.py` - í…ŒìŠ¤íŠ¸ ì‹¤í–‰
**ì—­í• **: í•™ìŠµëœ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸

**ê¸°ë³¸ êµ¬ì¡°**:
```python
import pygame
from environment import GridEnvironment
from car import Car
from agent import DQNAgent
from config import *

def test():
    # ì´ˆê¸°í™”
    env = GridEnvironment(random_map=True)  # v3ë¶€í„°: ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ë§µ
    start_x, start_y = env.start_pos
    car = Car(start_x, start_y)
    agent = DQNAgent()

    # ëª¨ë¸ ë¡œë“œ
    try:
        agent.load("model_final.pth")
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    except:
        print("âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (íƒí—˜ ì•ˆí•¨)
    agent.epsilon = 0

    # í†µê³„
    total_episodes = 0
    goal_reached = 0

    # í…ŒìŠ¤íŠ¸ ë£¨í”„
    running = True
    while running:
        # ì´ë²¤íŠ¸ ì²˜ë¦¬ (í‚¤ë³´ë“œ ì…ë ¥)
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                running = False
            if event.type == pygame.KEYDOWN:
                if event.key == pygame.K_ESCAPE:
                    running = False
                if event.key == pygame.K_r:
                    # Rí‚¤: ë¦¬ì…‹
                    env = GridEnvironment(random_map=True)
                    car.reset(env.start_pos[0], env.start_pos[1])

        # í–‰ë™ ì„ íƒ
        state = env.get_state(car.x, car.y, car.direction)
        action = agent.select_action(state, training=False)

        # í–‰ë™ ìˆ˜í–‰
        reward, done = car.move(action, env)

        # ëª©ì ì§€ ë„ë‹¬ ì²´í¬
        if env.is_goal(car.x, car.y):
            goal_reached += 1
            done = True

        # ì—í”¼ì†Œë“œ ì¢…ë£Œ
        if done:
            total_episodes += 1
            print(f"Episode {total_episodes} | Score: {car.score:.1f}")
            # ë¦¬ì…‹
            env = GridEnvironment(random_map=True)
            car.reset(env.start_pos[0], env.start_pos[1])

        # í™”ë©´ í‘œì‹œ
        env.draw(car)
        # ... ì •ë³´ í‘œì‹œ
        pygame.display.flip()

    # ìµœì¢… í†µê³„
    success_rate = (goal_reached / total_episodes) * 100
    print(f"Success Rate: {success_rate:.1f}%")

if __name__ == "__main__":
    test()
```

---

## ğŸ¯ ë²„ì „ë³„ ìƒì„¸ ìš”êµ¬ì‚¬í•­

### ğŸ“Œ Version 1 - ê¸°ë³¸ ë²„ì „ (ê³¼ì í•© ë°œìƒ)

**ëª©í‘œ**: DQN ê¸°ë³¸ êµ¬í˜„

**config.py ì„¤ì •**:
```python
NUM_EPISODES = 500
BATCH_SIZE = 32
LEARNING_RATE = 0.001
```

**agent.py íŠ¹ì§•**:
```python
# Epsilon Decay
self.epsilon_decay = 0.995  # ë¹ ë¥¸ ê°ì†Œ

# ë©”ëª¨ë¦¬ í¬ê¸°
self.memory = deque(maxlen=10000)

# Gradient Clipping ì—†ìŒ
```

**car.py íŠ¹ì§•**:
```python
# ê³ ì •ëœ ì´ˆê¸° ë°©í–¥
def __init__(self, x, y):
    self.direction = 0  # í•­ìƒ ìœ„ìª½

# visited_positions ì—†ìŒ (ê²½ë¡œ ë‹¤ì–‘ì„± ë³´ìƒ ì—†ìŒ)

# ë‹¨ìˆœí•œ ë³´ìƒ
def move(self, action, environment):
    # ë²½: -1.0
    # ëª©ì ì§€: +10.0
    # ì´ë™: -0.01
```

**environment.py íŠ¹ì§•**:
```python
# ê³ ì •ëœ ë‹¨ì¼ ë§µ
def __init__(self):
    self.map_id = 0
    self._create_simple_map()  # í•­ìƒ ê°™ì€ ë§µ

# ê³ ì •ëœ ì‹œì‘ì /ëª©ì ì§€
self.start_pos = (2, 2)
self.goal_pos = (27, 27)
```

**train.py íŠ¹ì§•**:
```python
# ë‹¨ì¼ ë§µì—ì„œë§Œ í•™ìŠµ
env = GridEnvironment()  # í•­ìƒ ê°™ì€ ë§µ

# ë§µ ë³€ê²½ ì—†ìŒ
for episode in range(NUM_EPISODES):
    car.reset(start_x, start_y)  # ê°™ì€ ë§µ, ê°™ì€ ìœ„ì¹˜
```

**main.py íŠ¹ì§•**:
```python
# í…ŒìŠ¤íŠ¸ë„ ê°™ì€ ë§µ
env = GridEnvironment()  # í›ˆë ¨ê³¼ ê°™ì€ ë§µ
```

**README.md ë‚´ìš©**:
```markdown
# Simulator v1 - ê¸°ë³¸ ë²„ì „

## ë¬¸ì œì 
1. í•œìª½ìœ¼ë¡œë§Œ ì´ë™ (í•­ìƒ ìœ„ìª½ë¶€í„° ì‹œì‘)
2. íŠ¹ì • ê²½ë¡œì— ê³ ì°©í™” (ë¹ ë¥¸ Epsilon Decay)
3. ë‹¨ì¼ ë§µ ê³¼ì í•© (ì¼ë°˜í™” ë¶ˆê°€)
4. í•™ìŠµ ë¶ˆì•ˆì •ì„± (Gradient Clipping ì—†ìŒ)

## í•™ìŠµ ê²°ê³¼
- Episode 0-100: ëœë¤í•˜ê²Œ ì›€ì§ì„
- Episode 100-300: íŠ¹ì • ê²½ë¡œ í•™ìŠµ
- Episode 300-500: ê·¸ ê²½ë¡œë§Œ ë°˜ë³µ (ê³¼ì í•©)
```

---

### ğŸ“Œ Version 2 - ê³¼ì í•© í•´ê²° ë²„ì „

**ëª©í‘œ**: v1ì˜ ëª¨ë“  ë¬¸ì œì  í•´ê²°

**config.py ë³€ê²½ì‚¬í•­**:
```python
NUM_EPISODES = 3000  # 500 â†’ 3000
BATCH_SIZE = 64      # 32 â†’ 64
LEARNING_RATE = 0.0005  # 0.001 â†’ 0.0005

# ì¶”ê°€ ì„¤ì •
NUM_MAPS = 20
MAPS_PER_EPISODE = 150  # ë§µë‹¹ ì—í”¼ì†Œë“œ ìˆ˜
RANDOM_SEED = 42
```

**agent.py ë³€ê²½ì‚¬í•­**:
```python
# ëŠë¦° Epsilon Decay
self.epsilon_decay = 0.998  # 0.995 â†’ 0.998

# ë” í° ë©”ëª¨ë¦¬
self.memory = deque(maxlen=20000)  # 10000 â†’ 20000

# Gradient Clipping ì¶”ê°€
def replay(self):
    # ...
    loss.backward()
    torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
    self.optimizer.step()
```

**car.py ë³€ê²½ì‚¬í•­**:
```python
# ëœë¤ ì´ˆê¸° ë°©í–¥
def __init__(self, x, y):
    self.direction = random.randint(0, 3)  # ëœë¤!

# ë°©ë¬¸ ìœ„ì¹˜ ì¶”ì 
def __init__(self, x, y):
    # ...
    self.visited_positions = set()

def reset(self, x, y, direction=None):
    # ...
    self.visited_positions = set()

# ê²½ë¡œ ë‹¤ì–‘ì„± ë³´ìƒ
def move(self, action, environment):
    # ...
    # ìƒˆë¡œìš´ ìœ„ì¹˜ ë°©ë¬¸
    if (next_x, next_y) not in self.visited_positions:
        reward += 0.2
        self.visited_positions.add((next_x, next_y))

    # ê°™ì€ ê³³ë§Œ ë°˜ë³µí•˜ë©´ íŒ¨ë„í‹°
    if self.steps > 10 and len(self.visited_positions) < self.steps * 0.5:
        reward -= 0.1
```

**environment.py ë³€ê²½ì‚¬í•­**:
```python
# ì—¬ëŸ¬ ë§µ ì§€ì›
def __init__(self, random_map=False):
    self.map_id = 0
    if random_map:
        self.map_id = -1
        self._create_random_map()
    else:
        self._create_simple_map()

def reset_map(self, map_id):
    """ë§µ ë³€ê²½"""
    self.map_id = map_id
    if map_id == 0:
        self._create_simple_map()
    elif map_id < 10:
        self._create_medium_map()
    elif map_id < 15:
        self._create_complex_map()
    else:
        self._create_random_map()

    # ëœë¤ ì‹œì‘ì /ëª©ì ì§€
    self.start_pos = self._get_random_empty_pos()
    self.goal_pos = self._get_random_empty_pos()
```

**train.py ë³€ê²½ì‚¬í•­**:
```python
# ì—¬ëŸ¬ ë§µì—ì„œ í•™ìŠµ
for episode in range(NUM_EPISODES):
    # ë§µ ë³€ê²½
    if episode % MAPS_PER_EPISODE == 0:
        current_map_id = random.randint(0, NUM_MAPS - 1)
        env.reset_map(current_map_id)
        start_x, start_y = env.start_pos

    # ëœë¤ ì´ˆê¸° ë°©í–¥
    car.reset(start_x, start_y)  # directionì€ car ë‚´ë¶€ì—ì„œ ëœë¤
```

**main.py ë³€ê²½ì‚¬í•­**:
```python
# í…ŒìŠ¤íŠ¸ëŠ” ìƒˆë¡œìš´ ë§µì—ì„œ
env = GridEnvironment(random_map=True)
```

**README.md ë‚´ìš©**:
```markdown
# Simulator v2 - ê³¼ì í•© í•´ê²° ë²„ì „

## ê°œì„ ì‚¬í•­
1. ëœë¤ ì´ˆê¸° ë°©í–¥ âœ…
2. ëŠë¦° Epsilon Decay (0.998) âœ…
3. Gradient Clipping âœ…
4. ê²½ë¡œ ë‹¤ì–‘ì„± ë³´ìƒ âœ…
5. ì—¬ëŸ¬ ë§µì—ì„œ í•™ìŠµ (20ê°œ) âœ…
6. ëœë¤ ì‹œì‘ì /ëª©ì ì§€ âœ…

## í•´ê²°ëœ ë¬¸ì œ
- í•œìª½ìœ¼ë¡œë§Œ ì´ë™ â†’ í•´ê²°
- íŠ¹ì • ê²½ë¡œ ê³ ì°©í™” â†’ í•´ê²°
- ë‹¨ì¼ ë§µ ê³¼ì í•© â†’ í•´ê²°
- í•™ìŠµ ë¶ˆì•ˆì •ì„± â†’ í•´ê²°
```

---

### ğŸ“Œ Version 3 - ì™„ì „í•œ ì¼ë°˜í™” ë²„ì „

**ëª©í‘œ**: v2ì™€ ë™ì¼í•˜ì§€ë§Œ ë” ì•ˆì •ì ì´ê³  ëª…í™•í•œ êµ¬í˜„

**ì°¨ì´ì **: v2ì™€ ê±°ì˜ ë™ì¼í•˜ì§€ë§Œ ì½”ë“œ êµ¬ì¡°ì™€ ì£¼ì„ì´ ë” ëª…í™•í•¨

**config.py**: v2ì™€ ë™ì¼

**agent.py**: v2ì™€ ë™ì¼

**car.py**: v2ì™€ ë™ì¼

**environment.py**: v2ì™€ ë™ì¼í•˜ì§€ë§Œ ë” ë§ì€ ì£¼ì„

**train.py**: v2ì™€ ë™ì¼í•˜ì§€ë§Œ ë” ìì„¸í•œ ë¡œê·¸

**main.py ê°œì„ ì‚¬í•­**:
```python
# í…ŒìŠ¤íŠ¸ì…‹ê³¼ í›ˆë ¨ì…‹ ëª…í™•íˆ ë¶„ë¦¬
print("âš ï¸ ì¤‘ìš”: í›ˆë ¨ì— ì‚¬ìš©í•˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ë§µì—ì„œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤!")
print("   â†’ ê³¼ì í•© ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³  ì¼ë°˜í™” ëŠ¥ë ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤.")

# ìƒˆë¡œìš´ ë§µ ìƒì„± ê°•ì¡°
env = GridEnvironment(random_map=True)
```

**README.md ë‚´ìš©**:
```markdown
# Simulator v3 - ìµœì¢… ë²„ì „ (ì™„ì „í•œ ì¼ë°˜í™”)

## í•µì‹¬ ê°œë…
### í…ŒìŠ¤íŠ¸ì…‹ê³¼ í›ˆë ¨ì…‹ ë¶„ë¦¬ì˜ ì¤‘ìš”ì„±
- í›ˆë ¨ì…‹: 0-19ë²ˆ ë§µ (í•™ìŠµìš©)
- í…ŒìŠ¤íŠ¸ì…‹: ìƒˆë¡œìš´ ëœë¤ ë§µ (ê²€ì¦ìš©)
- ëª©ì : ê³¼ì í•© ì—¬ë¶€ í™•ì¸

## í•™ìŠµ ê²°ê³¼
- ìƒˆë¡œìš´ ë§µì—ì„œë„ ì‘ë™ âœ…
- ëœë¤ ì‹œì‘ì /ëª©ì ì§€ì—ì„œë„ ì‘ë™ âœ…
- ì™„ì „í•œ ì¼ë°˜í™” ë‹¬ì„± âœ…
```

---

### ğŸ“Œ Version 4 - Learning Rate Scheduling ë²„ì „

**ëª©í‘œ**: ë™ì  í•™ìŠµë¥  ì¡°ì •ìœ¼ë¡œ í•™ìŠµ ì†ë„ í–¥ìƒ

**config.py**: v3ì™€ ë™ì¼

**agent.py ì¶”ê°€ì‚¬í•­**:
```python
class DQNAgent:
    def __init__(self):
        # ... (v3ì™€ ë™ì¼)

        # Learning Rate Scheduling ì¶”ê°€
        self.initial_lr = LEARNING_RATE * 2  # 0.001
        self.min_lr = LEARNING_RATE * 0.2    # 0.0001
        self.current_lr = self.initial_lr
        self.optimizer = optim.Adam(
            self.policy_net.parameters(),
            lr=self.current_lr
        )

    def update_learning_rate(self, episode):
        """ì—í”¼ì†Œë“œë§ˆë‹¤ í•™ìŠµë¥  ê°ì†Œ"""
        decay_rate = 0.998
        self.current_lr = self.initial_lr * (decay_rate ** episode)
        self.current_lr = max(self.current_lr, self.min_lr)

        # ì˜µí‹°ë§ˆì´ì € í•™ìŠµë¥  ì—…ë°ì´íŠ¸
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = self.current_lr
```

**train.py ë³€ê²½ì‚¬í•­**:
```python
for episode in range(NUM_EPISODES):
    # Learning Rate ì—…ë°ì´íŠ¸
    agent.update_learning_rate(episode)

    # ... (v3ì™€ ë™ì¼)

    # ë¡œê·¸ì— í•™ìŠµë¥  í‘œì‹œ
    if (episode + 1) % 100 == 0:
        print(f"Episode {episode+1} | "
              f"Avg Score: {avg_score:.2f} | "
              f"LR: {agent.current_lr:.6f} | "
              f"Epsilon: {agent.epsilon:.3f}")
```

**README.md ë‚´ìš©**:
```markdown
# Simulator v4 - Learning Rate Scheduling ë²„ì „

## í•µì‹¬ ê°œë…
### Learning Rate Scheduling
- ì´ˆê¸°: í° í•™ìŠµë¥  (0.001) â†’ ë¹ ë¥¸ íƒìƒ‰
- ì¤‘ê¸°: ì¤‘ê°„ í•™ìŠµë¥  (0.0005) â†’ ì•ˆì •ì  í•™ìŠµ
- í›„ê¸°: ì‘ì€ í•™ìŠµë¥  (0.0001) â†’ ì •ë°€ ì¡°ì •

## íš¨ê³¼
- í•™ìŠµ ì†ë„ 20-30% í–¥ìƒ âš¡
- ë” ë¹ ë¥¸ ìˆ˜ë ´
- ë” ë‚˜ì€ ì„±ëŠ¥
```

---

### ğŸ“Œ Version 5 - Policy > Cache ì›ì¹™ ë²„ì „

**ëª©í‘œ**: ìºì‹± ì‹œìŠ¤í…œê³¼ ë™ì  ì¥ì• ë¬¼ì„ í†µí•œ ê°•í™”í•™ìŠµ ì² í•™ ì´í•´

**config.py ì¶”ê°€ì‚¬í•­**:
```python
# v5 ì‹ ê¸°ëŠ¥: ìºì‹± ì‹œìŠ¤í…œ
USE_CACHE = False  # Trueë¡œ ì„¤ì •í•˜ë©´ ìºì‹± í™œì„±í™”
CACHE_SIZE = 10000

# v5 ì‹ ê¸°ëŠ¥: ë™ì  ì¥ì• ë¬¼
ENABLE_DYNAMIC_OBSTACLES = False  # Trueë¡œ ì„¤ì •í•˜ë©´ ë™ì  ì¥ì• ë¬¼ í™œì„±í™”
OBSTACLE_SPAWN_INTERVAL = 50
OBSTACLE_LIFETIME = 30
```

**agent.py ì¶”ê°€ì‚¬í•­**:
```python
class ActionCache:
    """
    ì‹¤í–‰ ìºì‹œ ì‹œìŠ¤í…œ (ì‹¤í–‰ ìµœì í™”ìš©, í•™ìŠµ ì•„ë‹˜!)
    """
    def __init__(self, max_size=10000):
        self.cache = {}  # state_key -> {'action', 'confidence', 'count'}
        self.max_size = max_size

    def _state_to_key(self, state):
        """ìƒíƒœë¥¼ ìºì‹œ í‚¤ë¡œ ë³€í™˜"""
        rounded_state = tuple(round(x, 2) for x in state)
        return rounded_state

    def get_cached_action(self, state):
        """ìºì‹œëœ í–‰ë™ ê°€ì ¸ì˜¤ê¸°"""
        key = self._state_to_key(state)
        if key in self.cache:
            return self.cache[key]['action'], self.cache[key]['confidence']
        return None

    def store_action(self, state, action, success=True):
        """í–‰ë™ì„ ìºì‹œì— ì €ì¥"""
        key = self._state_to_key(state)

        if key in self.cache:
            cached = self.cache[key]
            if cached['action'] == action:
                # ì‹ ë¢°ë„ ì¡°ì •
                cached['confidence'] = min(1.0, cached['confidence'] + 0.1) if success else max(0.0, cached['confidence'] - 0.2)
                cached['count'] += 1
            else:
                # í™˜ê²½ì´ ë³€í–ˆì„ ìˆ˜ ìˆìŒ
                cached['action'] = action
                cached['confidence'] = 0.5 if success else 0.0
                cached['count'] = 1
        else:
            if len(self.cache) >= self.max_size:
                self.cache.pop(next(iter(self.cache)))

            self.cache[key] = {
                'action': action,
                'confidence': 0.7 if success else 0.3,
                'count': 1
            }

class DQNAgent:
    def __init__(self, use_cache=False):
        # ... (v4ì™€ ë™ì¼)

        # v5: ìºì‹± ì‹œìŠ¤í…œ
        self.use_cache = use_cache
        self.action_cache = ActionCache() if use_cache else None
        self.cache_stats = {
            'size': 0, 'hits': 0,
            'agreements': 0, 'conflicts': 0
        }

    def select_action(self, state, training=False):
        """
        í–‰ë™ ì„ íƒ - Policy > Cache ì›ì¹™
        1. Policy Networkê°€ ë¨¼ì € íŒë‹¨ (ìµœìš°ì„ )
        2. ìºì‹œëŠ” ì°¸ê³ ë§Œ (Policyì™€ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸)
        3. ìµœì¢… ê²°ì •ì€ í•­ìƒ Policy
        """
        # 1. Policy Network íŒë‹¨ (í•­ìƒ ì‹¤í–‰!)
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            q_values = self.policy_net(state_tensor)
            policy_action = q_values.argmax().item()

        # 2. ìºì‹œ í™•ì¸ (ì°¸ê³ ë§Œ)
        if not training and self.use_cache:
            state_key = tuple(round(x, 2) for x in state)
            cached_result = self.action_cache.get_cached_action(state)

            if cached_result is not None:
                cached_action, confidence = cached_result
                self.cache_stats['hits'] += 1

                # Policyì™€ Cache ë¹„êµ
                if cached_action == policy_action:
                    # ì¼ì¹˜ â†’ ì•ˆì „
                    self.cache_stats['agreements'] += 1
                    return cached_action
                else:
                    # ë¶ˆì¼ì¹˜ â†’ Policy ìš°ì„ 
                    self.cache_stats['conflicts'] += 1
                    return policy_action

        # 3. ìµœì¢… ê²°ì •
        return policy_action

    def update_cache(self, state, action, success):
        """ìºì‹œ ì—…ë°ì´íŠ¸"""
        if self.use_cache:
            self.action_cache.store_action(state, action, success)

    def get_cache_stats(self):
        """ìºì‹œ í†µê³„"""
        if self.use_cache:
            self.cache_stats['size'] = len(self.action_cache.cache)
        return self.cache_stats
```

**environment.py ì¶”ê°€ì‚¬í•­**:
```python
class GridEnvironment:
    def __init__(self, random_map=False, enable_dynamic_obstacles=False):
        # ... (v4ì™€ ë™ì¼)

        # v5: ë™ì  ì¥ì• ë¬¼
        self.enable_dynamic_obstacles = enable_dynamic_obstacles
        self.dynamic_obstacles = []  # [(x, y, lifetime), ...]
        self.obstacle_counter = 0

    def update_dynamic_obstacles(self):
        """ë™ì  ì¥ì• ë¬¼ ì—…ë°ì´íŠ¸"""
        if not self.enable_dynamic_obstacles:
            return

        # ì¥ì• ë¬¼ ìƒì„±
        self.obstacle_counter += 1
        if self.obstacle_counter % OBSTACLE_SPAWN_INTERVAL == 0:
            x, y = self._get_random_empty_pos()
            self.dynamic_obstacles.append([x, y, OBSTACLE_LIFETIME])

        # ì¥ì• ë¬¼ ìˆ˜ëª… ê°ì†Œ
        for obstacle in self.dynamic_obstacles[:]:
            obstacle[2] -= 1
            if obstacle[2] <= 0:
                self.dynamic_obstacles.remove(obstacle)

    def is_wall(self, x, y):
        """ë²½ or ë™ì  ì¥ì• ë¬¼ ì²´í¬"""
        # ê¸°ì¡´ ë²½ ì²´í¬
        if self.walls[y][x] == 1:
            return True

        # ë™ì  ì¥ì• ë¬¼ ì²´í¬
        for ox, oy, _ in self.dynamic_obstacles:
            if x == ox and y == oy:
                return True

        return False

    def draw(self, car):
        # ... (ê¸°ì¡´ ê·¸ë¦¬ê¸°)

        # ë™ì  ì¥ì• ë¬¼ ê·¸ë¦¬ê¸° (ì˜¤ë Œì§€ìƒ‰)
        for ox, oy, lifetime in self.dynamic_obstacles:
            alpha = int(255 * (lifetime / OBSTACLE_LIFETIME))
            color = (255, 165, 0, alpha)  # ì˜¤ë Œì§€ìƒ‰
            pygame.draw.rect(self.screen, color,
                           (ox * GRID_SIZE, oy * GRID_SIZE,
                            GRID_SIZE, GRID_SIZE))
```

**train.py**: v4ì™€ ë™ì¼ (ìºì‹œ ë¯¸ì‚¬ìš©)

**main.py ì¶”ê°€ì‚¬í•­**:
```python
def test():
    # ì´ˆê¸°í™”
    env = GridEnvironment(random_map=True,
                          enable_dynamic_obstacles=ENABLE_DYNAMIC_OBSTACLES)
    agent = DQNAgent(use_cache=USE_CACHE)

    # ... (v4ì™€ ìœ ì‚¬)

    while running:
        # í–‰ë™ ì„ íƒ (Policy > Cache)
        action = agent.select_action(state, training=False)

        # v5: ë™ì  ì¥ì• ë¬¼ ì—…ë°ì´íŠ¸ (í™˜ê²½ ë³€í™”)
        env.update_dynamic_obstacles()

        # í–‰ë™ ì‹¤í–‰
        reward, done = car.move(action, env)
        next_state = env.get_state(car.x, car.y, car.direction)

        # v5: ìºì‹œ ì—…ë°ì´íŠ¸ (ì„±ê³µí•œ ê²½ë¡œë§Œ ì €ì¥)
        if USE_CACHE:
            success = env.is_goal(car.x, car.y) or (not done)
            agent.update_cache(state, action, success)

        # ... (í™”ë©´ í‘œì‹œ)

        # v5: ìºì‹œ í†µê³„ í‘œì‹œ
        if USE_CACHE:
            cache_stats = agent.get_cache_stats()
            print(f"Cache: Size={cache_stats['size']} | "
                  f"Hits={cache_stats['hits']} | "
                  f"Agreements={cache_stats['agreements']} | "
                  f"Conflicts={cache_stats['conflicts']}")
```

**README.md ë‚´ìš©**:
```markdown
# Simulator v5 - Policy > Cache ì›ì¹™ ë²„ì „

## í•µì‹¬ ì² í•™
### Policy > Cache ì›ì¹™
1. Policy Networkê°€ ìµœì¢… ê²°ì •ê¶Œ
2. ìºì‹œëŠ” ì°¸ê³  ìë£Œì¼ ë¿
3. ì¶©ëŒ ì‹œ Policy ìš°ì„ 

## ì‹ ê¸°ëŠ¥
### 1. ActionCache (ì‹¤í–‰ ìµœì í™”)
- ì„±ê³µí•œ ê²½ë¡œ ë©”ëª¨
- ê°™ì€ ë§µ ë°˜ë³µ ì‹œ ë¹ ë¥¸ ì‹¤í–‰
- í•™ìŠµ ì•„ë‹˜! ì‹¤í–‰ ìµœì í™”!

### 2. ë™ì  ì¥ì• ë¬¼
- 50ìŠ¤í…ë§ˆë‹¤ ìƒˆ ì¥ì• ë¬¼ ìƒì„±
- 30ìŠ¤í… í›„ ìë™ ì†Œë©¸
- ìºì‹œì˜ í•œê³„ ì¦ëª…

## 3ë‹¨ê³„ íë¦„
1. í•™ìŠµ (train.py): ìºì‹œ ì—†ì´ ìˆœìˆ˜ í•™ìŠµ
2. ì‹¤í–‰ (main.py): ìºì‹œë¡œ ì†ë„ í–¥ìƒ
3. ì¶©ëŒ (ë™ì  ì¥ì• ë¬¼): Policyê°€ ìµœì¢… ê²°ì •
```

---

## ğŸ“Š ë²„ì „ë³„ ë¹„êµí‘œ

| í•­ëª© | v1 | v2 | v3 | v4 | v5 |
|------|----|----|----|----|-----|
| **ì´ˆê¸° ë°©í–¥** | ê³ ì • (ìœ„ìª½) | ëœë¤ | ëœë¤ | ëœë¤ | ëœë¤ |
| **Epsilon Decay** | 0.995 | 0.998 | 0.998 | 0.998 | 0.998 |
| **Gradient Clip** | ì—†ìŒ | ìˆìŒ | ìˆìŒ | ìˆìŒ | ìˆìŒ |
| **ê²½ë¡œ ë‹¤ì–‘ì„±** | ì—†ìŒ | ìˆìŒ | ìˆìŒ | ìˆìŒ | ìˆìŒ |
| **ë§µ ê°œìˆ˜** | 1ê°œ | 20ê°œ | 20ê°œ | 20ê°œ | 20ê°œ |
| **ì‹œì‘ì /ëª©ì ì§€** | ê³ ì • | ëœë¤ | ëœë¤ | ëœë¤ | ëœë¤ |
| **ì—í”¼ì†Œë“œ ìˆ˜** | 500 | 3000 | 3000 | 3000 | 3000 |
| **ë°°ì¹˜ í¬ê¸°** | 32 | 64 | 64 | 64 | 64 |
| **í•™ìŠµë¥ ** | 0.001 | 0.0005 | 0.0005 | ë™ì  | 0.0005 |
| **ë©”ëª¨ë¦¬** | 10000 | 20000 | 20000 | 20000 | 20000 |
| **LR Scheduling** | ì—†ìŒ | ì—†ìŒ | ì—†ìŒ | ìˆìŒ | ì—†ìŒ |
| **ìºì‹± ì‹œìŠ¤í…œ** | ì—†ìŒ | ì—†ìŒ | ì—†ìŒ | ì—†ìŒ | ìˆìŒ |
| **ë™ì  ì¥ì• ë¬¼** | ì—†ìŒ | ì—†ìŒ | ì—†ìŒ | ì—†ìŒ | ìˆìŒ |
| **í…ŒìŠ¤íŠ¸ì…‹ ë¶„ë¦¬** | ì—†ìŒ | ì—†ìŒ | ìˆìŒ | ìˆìŒ | ìˆìŒ |
| **ì¼ë°˜í™”** | ë¶ˆê°€ëŠ¥ | ê°€ëŠ¥ | ì™„ì „ | ì™„ì „ | ì™„ì „ |
| **í•™ìŠµ ì†ë„** | ëŠë¦¼ | ë³´í†µ | ë³´í†µ | ë¹ ë¦„ | ë³´í†µ |
| **í•µì‹¬ ê°œë…** | DQN ê¸°ë³¸ | ê³¼ì í•© í•´ê²° | ì¼ë°˜í™” | í•™ìŠµ ìµœì í™” | RL ì² í•™ |

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. í™˜ê²½ ì„¤ì •
```bash
# Python 3.8+ í•„ìš”
python --version

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install pygame-ce numpy torch matplotlib
```

### 2. í•™ìŠµ ì‹¤í–‰
```bash
# ì›í•˜ëŠ” ë²„ì „ìœ¼ë¡œ ì´ë™
cd simulator-v1  # ë˜ëŠ” v2, v3, v4, v5

# í•™ìŠµ ì‹œì‘
python train.py

# í™”ë©´ ì—†ì´ ë¹ ë¥´ê²Œ í•™ìŠµí•˜ë ¤ë©´ config.py ìˆ˜ì •:
# SHOW_TRAINING = False
```

### 3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
# í•™ìŠµ ì™„ë£Œ í›„
python main.py

# í‚¤ë³´ë“œ ì¡°ì‘:
# R: ë¦¬ì…‹ (ìƒˆë¡œìš´ ë§µ)
# ESC: ì¢…ë£Œ
```

### 4. v5 íŠ¹ìˆ˜ ê¸°ëŠ¥
```bash
# config.py ìˆ˜ì •:

# ìºì‹± í™œì„±í™”
USE_CACHE = True

# ë™ì  ì¥ì• ë¬¼ í™œì„±í™”
ENABLE_DYNAMIC_OBSTACLES = True

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python main.py
```

---

## ğŸ“š í•™ìŠµ ê³¡ì„  ë° ì˜ˆìƒ ê²°ê³¼

### v1 (ê¸°ë³¸ ë²„ì „)
```
Episode 0-100:   ëœë¤ ì›€ì§ì„, ë²½ ì¶©ëŒ ë§ìŒ
Episode 100-300: íŠ¹ì • ê²½ë¡œ í•™ìŠµ ì‹œì‘
Episode 300-500: ê·¸ ê²½ë¡œë§Œ ë°˜ë³µ (ê³¼ì í•©)
â†’ ì„±ê³µë¥ : 50-70% (ê°™ì€ ë§µì—ì„œë§Œ)
â†’ ìƒˆë¡œìš´ ë§µ: ê±°ì˜ ì‹¤íŒ¨
```

### v2-v3 (ê³¼ì í•© í•´ê²°)
```
Episode 0-500:   ì—¬ëŸ¬ ë§µì—ì„œ íƒí—˜
Episode 500-1500: íŒ¨í„´ ë°œê²¬, ë²½ íšŒí”¼
Episode 1500-3000: ë‹¤ì–‘í•œ ë§µì—ì„œ ì„±ê³µ
â†’ ì„±ê³µë¥ : 60-80% (ìƒˆë¡œìš´ ë§µì—ì„œë„)
```

### v4 (ë¹ ë¥¸ í•™ìŠµ)
```
Episode 0-500:   ë¹ ë¥¸ ì´ˆê¸° íƒìƒ‰ (í° í•™ìŠµë¥ )
Episode 500-1500: ì•ˆì •ì  í•™ìŠµ (ì¤‘ê°„ í•™ìŠµë¥ )
Episode 1500-3000: ì •ë°€ ì¡°ì • (ì‘ì€ í•™ìŠµë¥ )
â†’ ì„±ê³µë¥ : 70-85% (v3ë³´ë‹¤ ë¹ ë¥´ê³  ì •í™•)
```

### v5 (ìºì‹± + ë™ì  ì¥ì• ë¬¼)
```
ìºì‹± ì—†ìŒ:
- ë§¤ë²ˆ Policy ê³„ì‚° (ëŠë¦¼)

ìºì‹± í™œì„±í™”:
Step 1-10:   ìºì‹œ ì—†ìŒ (ëŠë¦¼)
Step 11-50:  ìºì‹œ ì ì§„ì  ì¦ê°€ (ë¹¨ë¼ì§)
Step 51+:    ìºì‹œ ëŒ€ë¶€ë¶„ íˆíŠ¸ (ë§¤ìš° ë¹ ë¦„)

ë™ì  ì¥ì• ë¬¼:
- Policyì™€ Cache ë¶ˆì¼ì¹˜ â†’ Policy ì„ íƒ (ì•ˆì „)
- Conflicts ì¦ê°€ â†’ Policy > Cache ì¦ëª…
```

---

## ğŸ¯ êµ¬í˜„ ì‹œ ì£¼ì˜ì‚¬í•­

### 1. ìƒíƒœ í‘œí˜„ (ì¤‘ìš”!)
```python
# ë°˜ë“œì‹œ 10ì°¨ì› ìƒíƒœ ë²¡í„°
state = [
    sensor_up, sensor_down, sensor_left, sensor_right,
    sensor_up_left, sensor_up_right,
    sensor_down_left, sensor_down_right,
    goal_direction_x, goal_direction_y
]
```

### 2. ë³´ìƒ ì„¤ê³„ (ì¤‘ìš”!)
```python
# ê¸°ë³¸ ë³´ìƒ
ë²½ ì¶©ëŒ: -1.0
ëª©ì ì§€ ë„ë‹¬: +10.0
ì´ë™: -0.01

# v2ë¶€í„° ì¶”ê°€
ìƒˆë¡œìš´ ìœ„ì¹˜ ë°©ë¬¸: +0.2
ê°™ì€ ê³³ ë°˜ë³µ: -0.1
```

### 3. Epsilon Decay
```python
# v1: ë¹ ë¥¸ ê°ì†Œ (ë¬¸ì œ)
epsilon_decay = 0.995

# v2+: ëŠë¦° ê°ì†Œ (í•´ê²°)
epsilon_decay = 0.998
```

### 4. ë§µ ë¦¬ì…‹ íƒ€ì´ë°
```python
# v2+: ë§µë³„ë¡œ ì—¬ëŸ¬ ì—í”¼ì†Œë“œ í•™ìŠµ
if episode % MAPS_PER_EPISODE == 0:
    env.reset_map(new_map_id)
```

### 5. v5 ìºì‹œ ì—…ë°ì´íŠ¸ ìœ„ì¹˜
```python
# train.py: ìºì‹œ ì—…ë°ì´íŠ¸ ì—†ìŒ (í•™ìŠµ ì¤‘)
# main.py: ìºì‹œ ì—…ë°ì´íŠ¸ ìˆìŒ (ì‹¤í–‰ ì¤‘)

# main.pyì—ì„œë§Œ:
if USE_CACHE:
    success = env.is_goal(car.x, car.y) or (not done)
    agent.update_cache(state, action, success)
```

### 6. v5 Policy > Cache ì›ì¹™
```python
# ë°˜ë“œì‹œ ì´ ìˆœì„œ:
# 1. Policy ë¨¼ì € ê³„ì‚°
policy_action = self.policy_net(state).argmax()

# 2. ìºì‹œ í™•ì¸
cached_action = self.action_cache.get(state)

# 3. ë¹„êµ í›„ Policy ìš°ì„ 
if policy_action != cached_action:
    return policy_action  # Policy ì„ íƒ!
```

---

## ğŸ” í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

### v1 í…ŒìŠ¤íŠ¸
- [ ] í•™ìŠµì´ ì§„í–‰ë˜ëŠ”ê°€?
- [ ] Epsilonì´ ê°ì†Œí•˜ëŠ”ê°€?
- [ ] íŠ¹ì • ê²½ë¡œì— ê³ ì°©í™”ë˜ëŠ”ê°€? (ì˜ˆìƒëœ ë™ì‘)
- [ ] ìƒˆë¡œìš´ ë§µì—ì„œ ì‹¤íŒ¨í•˜ëŠ”ê°€? (ì˜ˆìƒëœ ë™ì‘)

### v2 í…ŒìŠ¤íŠ¸
- [ ] ì—¬ëŸ¬ ë°©í–¥ìœ¼ë¡œ íƒí—˜í•˜ëŠ”ê°€?
- [ ] ì—¬ëŸ¬ ë§µì´ ë¡œë“œë˜ëŠ”ê°€?
- [ ] ìƒˆë¡œìš´ ìœ„ì¹˜ ë°©ë¬¸ ë³´ìƒì´ ì‘ë™í•˜ëŠ”ê°€?
- [ ] ìƒˆë¡œìš´ ë§µì—ì„œë„ ì„±ê³µí•˜ëŠ”ê°€?

### v3 í…ŒìŠ¤íŠ¸
- [ ] í…ŒìŠ¤íŠ¸ ì‹œ ìƒˆë¡œìš´ ëœë¤ ë§µ ìƒì„±ë˜ëŠ”ê°€?
- [ ] ì¼ë°˜í™” ë©”ì‹œì§€ê°€ ì¶œë ¥ë˜ëŠ”ê°€?
- [ ] ì„±ê³µë¥ ì´ v2ì™€ ìœ ì‚¬í•œê°€?

### v4 í…ŒìŠ¤íŠ¸
- [ ] í•™ìŠµë¥ ì´ ê°ì†Œí•˜ëŠ”ê°€?
- [ ] ë¡œê·¸ì— LRì´ í‘œì‹œë˜ëŠ”ê°€?
- [ ] í•™ìŠµ ì†ë„ê°€ v3ë³´ë‹¤ ë¹ ë¥¸ê°€?

### v5 í…ŒìŠ¤íŠ¸
- [ ] ìºì‹œ í†µê³„ê°€ í‘œì‹œë˜ëŠ”ê°€?
- [ ] ë™ì  ì¥ì• ë¬¼ì´ ìƒì„±/ì†Œë©¸ë˜ëŠ”ê°€?
- [ ] Conflictsê°€ ë°œìƒí•˜ëŠ”ê°€? (ë™ì  ì¥ì• ë¬¼ ìˆì„ ë•Œ)
- [ ] Policyê°€ ìµœì¢… ê²°ì •ì„ ë‚´ë¦¬ëŠ”ê°€?

---

## ğŸ’¡ í™•ì¥ ì•„ì´ë””ì–´

í”„ë¡œì íŠ¸ë¥¼ ë” ë°œì „ì‹œí‚¤ê³  ì‹¶ë‹¤ë©´:

### 1. ì¶”ê°€ ê¸°ëŠ¥
- ìš°ì„ ìˆœìœ„ ê²½í—˜ ë¦¬í”Œë ˆì´ (Prioritized Experience Replay)
- Double DQN
- Dueling DQN
- ë‹¤ì¤‘ ì—ì´ì „íŠ¸ (ì—¬ëŸ¬ ìë™ì°¨)
- ë” ë³µì¡í•œ ì„¼ì„œ (ë¼ì´ë‹¤ ì‹œë®¬ë ˆì´ì…˜)

### 2. ì‹œê°í™” ê°œì„ 
- ì‹¤ì‹œê°„ Q-value íˆíŠ¸ë§µ
- ë°©ë¬¸ ë¹ˆë„ íˆíŠ¸ë§µ
- ì—ì´ì „íŠ¸ "ìƒê°" ì‹œê°í™”

### 3. ì„±ëŠ¥ ìµœì í™”
- GPU ê°€ì†
- ë³‘ë ¬ í™˜ê²½ ì‹¤í–‰
- ë¶„ì‚° í•™ìŠµ

### 4. ì‹¤ì œ ì ìš©
- ROS í†µí•©
- ì‹¤ì œ ë¡œë´‡/ë“œë¡  ì œì–´
- ì‹œë®¬ë ˆì´ì…˜ â†’ ì‹¤ì œ ì „ì´ (Sim-to-Real)

---

## ğŸ“– ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- DQN: "Playing Atari with Deep Reinforcement Learning" (Mnih et al., 2013)
- Double DQN: "Deep Reinforcement Learning with Double Q-learning" (van Hasselt et al., 2015)

### ê°œë…
- ê°•í™”í•™ìŠµ ê¸°ì´ˆ: Sutton & Barto, "Reinforcement Learning: An Introduction"
- ê³¼ì í•©: ë¨¸ì‹ ëŸ¬ë‹ ì¼ë°˜ ê°œë…
- ì¼ë°˜í™”: í…ŒìŠ¤íŠ¸ì…‹ ë¶„ë¦¬ì˜ ì¤‘ìš”ì„±

---

## âœ… í”„ë¡œì íŠ¸ ìƒì„± ìˆœì„œ (AIì—ê²Œ ìš”ì²­ ì‹œ)

ì´ ëª…ì„¸ì„œë¥¼ AIì—ê²Œ ì œê³µí•˜ê³  ë‹¤ìŒê³¼ ê°™ì´ ìš”ì²­í•˜ì„¸ìš”:

```
ì´ ëª…ì„¸ì„œ(ë”¸ê¹.md)ë¥¼ ë³´ê³  simulator-v1ë¶€í„° v5ê¹Œì§€
5ê°œì˜ í”„ë¡œì íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ê° ë²„ì „ì€:
1. í•´ë‹¹ ë²„ì „ì˜ config.py
2. agent.py
3. car.py
4. environment.py
5. train.py
6. main.py
7. README.md

ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ëª…ì„¸ì„œì˜ "ë²„ì „ë³„ ìƒì„¸ ìš”êµ¬ì‚¬í•­" ì„¹ì…˜ì„ ì°¸ê³ í•˜ì—¬
ê° ë²„ì „ì˜ ì°¨ì´ì ì„ ì •í™•íˆ êµ¬í˜„í•´ì£¼ì„¸ìš”.

íŠ¹íˆ ì£¼ì˜í•  ì :
- v1: ê³¼ì í•© ë°œìƒí•´ì•¼ í•¨
- v2: ê³¼ì í•© í•´ê²°
- v3: v2ì™€ ìœ ì‚¬í•˜ì§€ë§Œ ë” ëª…í™•
- v4: Learning Rate Scheduling êµ¬í˜„
- v5: ActionCache í´ë˜ìŠ¤ì™€ Policy > Cache ì›ì¹™ êµ¬í˜„
```

---

**ì´ ëª…ì„¸ì„œëŠ” ì „ì²´ í”„ë¡œì íŠ¸ë¥¼ ì¬í˜„í•˜ê¸°ì— ì¶©ë¶„í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.**
**ê° ë²„ì „ì˜ ì°¨ì´ì , êµ¬í˜„ ì„¸ë¶€ì‚¬í•­, í…ŒìŠ¤íŠ¸ ë°©ë²•ê¹Œì§€ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.**
