# Simulator v2 - 과적합 해결 버전

## 📋 프로젝트 설명

이 버전은 **과적합(Overfitting) 문제를 해결한 버전**입니다.
v1의 문제점을 해결하기 위해 여러 기법을 적용했습니다.

## ✨ 주요 개선사항

### 1. 랜덤 초기 방향 ✅
- 매 에피소드마다 랜덤한 방향에서 시작
- 모든 방향(위/아래/좌/우) 탐험 가능
- 한쪽으로만 이동하는 문제 해결

### 2. 느린 Epsilon Decay ✅
- `epsilon_decay = 0.998` (v1: 0.995)
- 더 오래 탐험하여 다양한 경로 학습
- 과적합 방지

### 3. Gradient Clipping ✅
- 학습 안정성 향상
- 과도한 학습 방지
- 일반화 능력 향상

### 4. 경로 다양성 보상 ✅ (핵심 개선사항)
- **방문한 위치 추적**: `visited_positions` 집합 사용
- **새로운 위치 방문 보상**: +0.2 보상
- **같은 경로 반복 패널티**: -0.1 패널티
- 다양한 경로 탐험 인센티브 제공

### 5. 여러 맵에서 학습 ✅
- **20개의 다양한 맵**: 단순, 중간, 복잡, 랜덤 맵
- **맵별 학습**: 각 맵에서 약 150 에피소드씩 학습
- **랜덤 시작점/목적지**: 매 에피소드마다 다른 위치
- **완전한 일반화**: 다양한 환경에서 작동

### 6. 개선된 하이퍼파라미터 ✅
- 에피소드 수: **3,000** (v1: 500)
- 배치 크기: **64** (v1: 32)
- 학습률: **0.0005** (v1: 0.001)
- 메모리 용량: **20,000** (v1: 10,000)

## 🎯 경로 다양성 보상 시스템

### 구현 방법

```python
# car.py
class Car:
    def __init__(self, x, y):
        # ...
        self.visited_positions = set()  # 방문한 위치 추적
    
    def reset(self, x, y, direction=None):
        # ...
        self.visited_positions = set()  # 방문 기록 초기화
    
    def move(self, action, environment):
        # ...
        # 경로 다양성 보상
        if (next_x, next_y) not in self.visited_positions:
            reward += 0.2  # 새로운 위치 방문 보상
            self.visited_positions.add((next_x, next_y))
        
        # 너무 같은 경로만 가면 패널티
        if self.steps > 10 and len(self.visited_positions) < self.steps * 0.5:
            reward -= 0.1  # 같은 곳만 돌아다니면 패널티
```

### 효과

- ✅ 다양한 경로 탐험 인센티브
- ✅ 같은 경로 반복 방지
- ✅ 탐험 다양성 증가
- ✅ 과적합 완화

## 📊 예상되는 학습 결과

### Episode 0-500: 초보 단계 🔴
- AI가 여러 맵에서 랜덤하게 움직임 (모든 방향 탐험)
- 벽에 자주 부딪힘
- 평균 3-5스텝 생존
- 성공률: 10-20%

### Episode 500-1500: 학습 시작 🟡
- 다양한 맵에서 패턴 발견
- 벽을 피하기 시작
- 10-50스텝 생존
- **다양한 경로 시도**
- 성공률: 30-50%

### Episode 1500-3000: 마스터! ✅
- 벽을 거의 안 부딪힘
- 100-500스텝 생존
- **다양한 경로 사용**
- **모든 방향으로 이동**
- **새로운 맵에서도 작동**
- 성공률: 60-80%

## 🚀 사용 방법

### 1. 패키지 설치

```bash
python -m pip install pygame-ce numpy torch matplotlib
```

### 2. 훈련 시작

```bash
cd simulator-v2
python train.py
```

### 3. 테스트

```bash
python main.py
```

## 🔄 v1과의 차이점

| 항목 | v1 | v2 |
|------|----|----|
| 초기 방향 | 고정 (항상 위쪽) | 랜덤 |
| Epsilon decay | 0.995 (빠름) | 0.998 (느림) |
| Gradient clipping | 없음 | 있음 |
| 경로 다양성 보상 | 없음 | 있음 |
| 맵 개수 | 1개 | 20개 |
| 시작점/목적지 | 고정 | 랜덤 |
| 에피소드 수 | 500 | 3,000 |
| 한쪽으로만 이동 | 발생 | 해결 |
| 경로 다양성 | 없음 | 있음 |
| 일반화 | 불가능 | 가능 |

## ✅ 해결된 문제점

### 1. 한쪽으로만 이동하는 문제 → 해결!
- 랜덤 초기 방향으로 모든 방향 탐험

### 2. 특정 경로에 고착화 → 해결!
- 느린 Epsilon Decay와 경로 다양성 보상

### 3. 단일 맵 학습 → 해결!
- 20개의 다양한 맵에서 학습

### 4. 고정된 시작점/목적지 → 해결!
- 매 에피소드마다 랜덤한 위치

### 5. 학습 불안정성 → 해결!
- Gradient Clipping으로 안정적인 학습

## 📚 다음 단계

더 나은 성능을 원한다면:

1. **v3 확인**: v2와 동일한 기능이지만 더 안정적인 버전 (권장)
2. **v4 확인**: Learning Rate Scheduling을 적용한 학습 속도 향상 버전
3. **v5 확인**: Policy > Cache 원칙을 구현한 강화학습 철학 이해 버전

## 💡 학습 목표

이 버전을 통해 다음을 학습할 수 있습니다:

- 과적합 완전 해결 방법
- 경로 다양성 보상의 효과
- 랜덤 초기화의 중요성
- Epsilon Decay 조정의 효과
- Gradient Clipping의 역할
- 여러 맵에서의 학습 방법
- 일반화 달성

---

## 🔮 다음 단계 개선 사항

### ✅ v3에서 개선될 사항
- **테스트셋 분리**: 훈련에 사용하지 않은 맵으로 검증
- **코드 안정성**: 더 안정적인 구현
- **문서화**: 더 명확한 설명

### ✅ v4에서 추가될 사항
- **Learning Rate Scheduling**: 동적 학습률 조정으로 학습 속도 향상
- **빠른 초기 탐색**: 큰 보폭으로 빠른 탐색
- **정밀한 후기 학습**: 작은 보폭으로 정밀 조정

**다음 버전**: [`simulator-v3/README.md`](../simulator-v3/README.md)에서 더 안정적인 버전 확인

---

**✅ 완전 개선됨**: v1의 모든 문제점이 해결되었습니다! 여러 맵, 랜덤 시작점/목적지, 완전한 일반화를 달성했습니다.
