# Simulator v2 - 과적합 해결 버전

## 📋 프로젝트 설명

이 버전은 **과적합(Overfitting) 문제를 해결한 버전**입니다.
v1의 문제점을 해결하기 위해 여러 기법을 적용했습니다.

## 🧠 에이전트가 받는 정보 (상태 벡터)

### 에이전트는 무엇을 알고 출발하는가?

**중요**: 에이전트는 목적지의 **절대 좌표를 모릅니다**. 대신 **상대적 방향**만 알고 움직입니다.

#### 상태 벡터 구성 (총 11차원)

```python
상태 = [
    # 1. 주변 8칸 장애물 정보 (8차원)
    wall_up_left, wall_up, wall_up_right,
    wall_left,              wall_right,
    wall_down_left, wall_down, wall_down_right,

    # 2. 현재 방향 (1차원, 정규화)
    direction / 4.0,  # 0=위, 1=오른쪽, 2=아래, 3=왼쪽

    # 3. 목적지 상대 방향 (2차원, 정규화) ⭐ 핵심!
    dx_to_goal / GRID_WIDTH,   # "목적지가 X축으로 저쪽"
    dy_to_goal / GRID_HEIGHT   # "목적지가 Y축으로 저쪽"
]
```

#### ❌ 에이전트가 모르는 것
- 목적지의 절대 좌표 (goal_x, goal_y)
- "목적지가 맵의 (50, 50)이다" 같은 숫자 정보
- 전체 맵의 구조 (전지적 시점 불가)

#### ✅ 에이전트가 아는 것
- **현재 위치에서 목적지가 어느 방향인지** (상대 방향)
  ```python
  dx_to_goal = +0.5  # 오른쪽으로 가야 함
  dy_to_goal = -0.3  # 위쪽으로 가야 함
  ```
- 주변 8칸의 장애물 위치
- 현재 바라보는 방향

#### 🎯 목적 자체는 알고 있는가?

**네, 알고 있습니다!** (보상 설계를 통해)

```python
# 보상 설계
if 목적지 도달:
    reward = +100  # 큰 양의 보상
elif 목적지에 가까워짐:
    reward = +0.5  # 작은 양의 보상
elif 충돌:
    reward = -100  # 큰 음의 보상
```

에이전트는 이 보상을 통해:
- "목적지에 도달하는 것이 좋다"
- "목적지로 가까워지는 것이 좋다"
- "충돌은 나쁘다"

를 학습하므로, **목적 자체(목적지로 가는 것)는 알고 시작**합니다.

#### 🤔 그럼 어떻게 움직이는가?

**매 순간마다:**
1. "지금 위치에서 목적지가 저쪽 방향이다" (상대 방향)
2. "주변에 벽이 이렇게 있다" (장애물 정보)
3. "직진/좌회전/우회전 중 어떤 게 좋을까?" (Policy 판단)
4. 행동 실행
5. 보상 받기 ("목적지에 가까워졌네!" or "충돌했네!")

→ **"저 방향으로 가면 보상이 좋다"는 수준으로 학습하는 구조**

#### 비유
```
❌ GPS 네비게이션 (절대 좌표)
"서울시 강남구 테헤란로 123번지로 가세요"

✅ 나침반 (상대 방향)
"목적지가 북동쪽 방향입니다. 주변 장애물을 피하며 그쪽으로 가세요"
```

---

## ✨ 주요 개선사항

### 1. 랜덤 초기 방향 ✅
- 매 에피소드마다 랜덤한 방향에서 시작
- 모든 방향(위/아래/좌/우) 탐험 가능
- 한쪽으로만 이동하는 문제 해결

### 2. 느린 Epsilon Decay ✅
- `epsilon_decay = 0.998` (v1: 0.995)
- 더 오래 탐험하여 다양한 경로 학습
- 과적합 방지

### 3. Gradient Clipping ✅
- 학습 안정성 향상
- 과도한 학습 방지
- 일반화 능력 향상

### 4. 경로 다양성 보상 ✅ (핵심 개선사항)
- **방문한 위치 추적**: `visited_positions` 집합 사용
- **새로운 위치 방문 보상**: +0.2 보상
- **같은 경로 반복 패널티**: -0.1 패널티
- 다양한 경로 탐험 인센티브 제공

### 5. 여러 맵에서 학습 ✅
- **20개의 다양한 맵**: 단순, 중간, 복잡, 랜덤 맵
- **맵별 학습**: 각 맵에서 약 150 에피소드씩 학습
- **랜덤 시작점/목적지**: 매 에피소드마다 다른 위치
- **완전한 일반화**: 다양한 환경에서 작동

### 6. 개선된 하이퍼파라미터 ✅
- 에피소드 수: **3,000** (v1: 500)
- 배치 크기: **64** (v1: 32)
- 학습률: **0.0005** (v1: 0.001)
- 메모리 용량: **20,000** (v1: 10,000)

## 🎯 경로 다양성 보상 시스템

### 구현 방법

```python
# car.py
class Car:
    def __init__(self, x, y):
        # ...
        self.visited_positions = set()  # 방문한 위치 추적
    
    def reset(self, x, y, direction=None):
        # ...
        self.visited_positions = set()  # 방문 기록 초기화
    
    def move(self, action, environment):
        # ...
        # 경로 다양성 보상
        if (next_x, next_y) not in self.visited_positions:
            reward += 0.2  # 새로운 위치 방문 보상
            self.visited_positions.add((next_x, next_y))
        
        # 너무 같은 경로만 가면 패널티
        if self.steps > 10 and len(self.visited_positions) < self.steps * 0.5:
            reward -= 0.1  # 같은 곳만 돌아다니면 패널티
```

### 효과

- ✅ 다양한 경로 탐험 인센티브
- ✅ 같은 경로 반복 방지
- ✅ 탐험 다양성 증가
- ✅ 과적합 완화

## 📊 예상되는 학습 결과

### Episode 0-500: 초보 단계 🔴
- AI가 여러 맵에서 랜덤하게 움직임 (모든 방향 탐험)
- 벽에 자주 부딪힘
- 평균 3-5스텝 생존
- 성공률: 10-20%

### Episode 500-1500: 학습 시작 🟡
- 다양한 맵에서 패턴 발견
- 벽을 피하기 시작
- 10-50스텝 생존
- **다양한 경로 시도**
- 성공률: 30-50%

### Episode 1500-3000: 마스터! ✅
- 벽을 거의 안 부딪힘
- 100-500스텝 생존
- **다양한 경로 사용**
- **모든 방향으로 이동**
- **새로운 맵에서도 작동**
- 성공률: 60-80%

## 🚀 사용 방법

### 1. 패키지 설치

```bash
python -m pip install pygame-ce numpy torch matplotlib
```

### 2. 훈련 시작

```bash
cd simulator-v2
python train.py
```

### 3. 테스트

```bash
python main.py
```

## 🔄 v1과의 차이점

| 항목 | v1 | v2 |
|------|----|----|
| 초기 방향 | 고정 (항상 위쪽) | 랜덤 |
| Epsilon decay | 0.995 (빠름) | 0.998 (느림) |
| Gradient clipping | 없음 | 있음 |
| 경로 다양성 보상 | 없음 | 있음 |
| 맵 개수 | 1개 | 20개 |
| 시작점/목적지 | 고정 | 랜덤 |
| 에피소드 수 | 500 | 3,000 |
| 한쪽으로만 이동 | 발생 | 해결 |
| 경로 다양성 | 없음 | 있음 |
| 일반화 | 불가능 | 가능 |

## ✅ 해결된 문제점

### 1. 한쪽으로만 이동하는 문제 → 해결!
- 랜덤 초기 방향으로 모든 방향 탐험

### 2. 특정 경로에 고착화 → 해결!
- 느린 Epsilon Decay와 경로 다양성 보상

### 3. 단일 맵 학습 → 해결!
- 20개의 다양한 맵에서 학습

### 4. 고정된 시작점/목적지 → 해결!
- 매 에피소드마다 랜덤한 위치

### 5. 학습 불안정성 → 해결!
- Gradient Clipping으로 안정적인 학습

## 📚 다음 단계

더 나은 성능을 원한다면:

1. **v3 확인**: v2와 동일한 기능이지만 더 안정적인 버전 (권장)
2. **v4 확인**: Learning Rate Scheduling을 적용한 학습 속도 향상 버전
3. **v5 확인**: Policy > Cache 원칙을 구현한 강화학습 철학 이해 버전

## 💡 학습 목표

이 버전을 통해 다음을 학습할 수 있습니다:

- 과적합 완전 해결 방법
- 경로 다양성 보상의 효과
- 랜덤 초기화의 중요성
- Epsilon Decay 조정의 효과
- Gradient Clipping의 역할
- 여러 맵에서의 학습 방법
- 일반화 달성

---

## 🔮 다음 단계 개선 사항

### ✅ v3에서 개선될 사항
- **테스트셋 분리**: 훈련에 사용하지 않은 맵으로 검증
- **코드 안정성**: 더 안정적인 구현
- **문서화**: 더 명확한 설명

### ✅ v4에서 추가될 사항
- **Learning Rate Scheduling**: 동적 학습률 조정으로 학습 속도 향상
- **빠른 초기 탐색**: 큰 보폭으로 빠른 탐색
- **정밀한 후기 학습**: 작은 보폭으로 정밀 조정

**다음 버전**: [`simulator-v3/README.md`](../simulator-v3/README.md)에서 더 안정적인 버전 확인

---

**✅ 완전 개선됨**: v1의 모든 문제점이 해결되었습니다! 여러 맵, 랜덤 시작점/목적지, 완전한 일반화를 달성했습니다.
