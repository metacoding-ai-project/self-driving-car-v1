# Simulator v5 - Policy > Cache (캐싱 시스템 버전)

## 📋 프로젝트 설명

이 버전은 **캐싱 시스템**을 추가하여 **Policy > Cache 원칙**을 구현한 버전입니다.

**핵심 철학:**
- 학습의 목적은 '최적 경로'가 아니라 '환경 적응력'
- 캐싱은 학습이 아닌 실행 최적화
- Policy가 항상 최종 판단자

## 🎯 v5의 핵심 개념

### 1. 학습의 진짜 목적

#### ❌ 강화학습의 목적이 아닌 것
- 특정 맵의 최단 경로 암기
- 한 번에 완벽한 경로 계산
- A* 같은 계획 알고리즘 대체

#### ⭕ 강화학습의 진짜 목적
> **처음 보는 환경에서도**
> - 벽에 부딪히지 않고
> - 막힌 공간에서 빠져나오며
> - 목표가 바뀌어도
> - 의미 있게 움직일 수 있는 **범용 행동 정책(policy)을 학습하는 것**

👉 **경로는 학습 목표가 아니라, 실행 결과(output)**

### 2. 왜 학습된 상태에서 더 빨리 도달하는가?

학습된 정책은 물리적으로 더 빠르지 않습니다.
대신 **'헤매는 시간'을 제거**해서 더 빨리 도달합니다.

**학습되지 않은 상태:**
- 벽에 자주 충돌
- 막다른 길에서 오래 정지
- 의미 없는 왕복 이동

**학습된 상태:**
- 벽 근처에서 사전 회피
- 막히면 즉시 방향 전환
- 목표 방향으로 일관된 이동

→ **실수를 안 해서 도달 시간이 감소**

### 3. 캐싱이란?

새 맵에서 여러 번 실행하면 경로를 기억합니다.
이는 **학습(training)이 아니라 실행 최적화(execution optimization)** 입니다.

#### 캐싱 vs 학습

| 구분 | 학습 (Training) | 캐싱 (Caching) |
|------|----------------|---------------|
| 신경망 변경 | ✅ 파라미터 업데이트 | ❌ 파라미터 변경 없음 |
| 범위 | 모든 맵에 적용 | 특정 맵만 |
| 동적 환경 | ✅ 대응 가능 | ❌ 캐시 무효화 |
| 일반화 | ✅ 가능 | ❌ 불가능 |

> **뇌가 바뀐 게 아니라, 경험을 메모한 것**

### 4. Policy > Cache 원칙

#### 핵심 원칙

> **Policy > Cache**
> - 정책(policy)은 항상 최종 판단자
> - 캐시는 "힌트 / 가이드" 역할만 수행

#### 동작 방식

```python
1. Policy 네트워크가 행동 예측 (항상 실행!)
2. 캐시에서 힌트 가져오기
3. Policy와 캐시가 일치하면 신뢰도 ↑
4. Policy와 캐시가 충돌하면 Policy 우선!
```

#### 왜 Policy 우선인가?

- 환경이 동적으로 변할 수 있음
- 새로운 장애물이 나타날 수 있음
- 캐시된 경로가 더 이상 유효하지 않을 수 있음

→ **Policy만이 현재 환경을 정확히 판단 가능**

## ✨ 주요 특징

### 1. 캐싱 시스템 (NEW!) ⭐

**핵심 아이디어: 경험을 메모하되, 정책이 최종 판단!**

#### 동작 원리:
1. 상태-행동 쌍을 캐시에 저장
2. 캐시는 힌트만 제공
3. Policy 네트워크가 최종 판단
4. Policy와 캐시 충돌 시 Policy 우선

#### 구현 방식:
```python
class ActionCache:
    # 상태 -> 행동 매핑 저장
    # 신뢰도(confidence) 추적
    # Policy와의 일치/충돌 통계
```

#### 캐시 통계:
```
Cache Size: 캐시에 저장된 상태 수
Cache Hits: 캐시가 사용된 횟수
Agreements: Policy와 캐시가 일치한 횟수
Conflicts: Policy와 캐시가 충돌한 횟수 (Policy 우선)
```

### 2. 동적 장애물 시스템 (NEW!) ⚡

**핵심 목적: Policy > Cache 원칙을 실전으로 증명!**

#### 해결 원칙: 캐싱의 지위 낮추기

동적 장애물은 **환경이 실시간으로 변하는 상황**을 시뮬레이션합니다.

**❌ 캐시의 치명적 한계:**
- 캐시된 경로는 정적 환경 기준
- 새로운 장애물이 생기면 캐시 무효화
- 캐시만 믿으면 충돌 발생

**✅ Policy가 필수인 이유:**
- Policy는 현재 상태를 실시간 판단
- 환경 변화에 즉시 대응
- 캐시는 참고만, Policy가 최종 결정

#### 동작 방식:
```python
# 환경이 동적으로 변함
env.update_dynamic_obstacles()  # 50스텝마다 새 장애물 생성

# Policy > Cache: 항상 Policy가 최종 판단
action = agent.select_action(state)  # Policy가 현재 환경 판단

# 캐시는 힌트만 제공, Policy가 최종 결정
if cache_action != policy_action:
    return policy_action  # Policy 우선!
```

#### 동적 장애물 특징:
- 🔴 **50스텝마다 랜덤 위치에 장애물 생성** (오렌지색)
- ⏰ **30스텝 후 자동 소멸**
- 🚨 **캐시된 경로 무효화**
- ✅ **Policy가 실시간 회피**

이를 통해 **Policy > Cache 원칙**이 단순한 이론이 아니라 **실전에서 필수**임을 증명합니다!

### 3. v4의 모든 기능 포함 ✅

- ✅ Learning Rate Scheduling
- ✅ 여러 맵에서 학습 (20개 이상)
- ✅ 랜덤 시작점/목적지
- ✅ 랜덤 초기 방향
- ✅ 느린 Epsilon Decay (0.998)
- ✅ Gradient Clipping
- ✅ 경로 다양성 보상
- ✅ 테스트셋 분리

## 🚀 사용 방법

### 1. 패키지 설치

```bash
python -m pip install pygame-ce numpy torch matplotlib
```

### 2. 훈련 시작

```bash
cd simulator-v5
python train.py
```

**특징:**
- 기본적으로 캐싱 비활성화 (학습 목적)
- `config.py`에서 `USE_CACHE = True`로 변경하여 활성화 가능

### 3. 테스트

```bash
python main.py
```

**특징:**
- 훈련에 사용하지 않은 새로운 맵에서 테스트
- 캐싱 활성화 시 반복 실행하면 경로 최적화
- 일반화 능력 검증

## 📈 학습 결과

훈련이 완료되면:
- `model_final.pth`: 최종 모델 (캐시는 저장 안 됨)
- `training_results.png`: 학습 그래프

## 🔄 버전 비교

| 항목 | v1 | v2 | v3 | v4 | v5 |
|------|----|----|----|----|---|
| 초기 방향 | 고정 | 랜덤 | 랜덤 | 랜덤 | 랜덤 |
| Epsilon decay | 0.995 | 0.998 | 0.998 | 0.998 | 0.998 |
| Gradient clipping | 없음 | 있음 | 있음 | 있음 | 있음 |
| 경로 다양성 보상 | 없음 | 있음 | 있음 | 있음 | 있음 |
| 맵 개수 | 1개 | 1개 | 20개 이상 | 20개 이상 | 20개 이상 |
| 시작점/목적지 | 고정 | 고정 | 랜덤 | 랜덤 | 랜덤 |
| 테스트셋 분리 | 없음 | 없음 | ✅ | ✅ | ✅ |
| Learning Rate Scheduling | 없음 | 없음 | 없음 | ✅ | ✅ |
| **캐싱 시스템** | 없음 | 없음 | 없음 | 없음 | ✅ |
| **Policy > Cache 원칙** | 없음 | 없음 | 없음 | 없음 | ✅ |
| **동적 장애물 시스템** | 없음 | 없음 | 없음 | 없음 | ✅ |

## 💡 캐싱 시스템의 효과

### 언제 유용한가?

- 같은 맵에서 반복 실행할 때
- 테스트/데모 시나리오
- 실행 시간 최적화가 필요할 때

### 언제 사용하지 말아야 하는가?

- 훈련 중 (일반화 방해)
- 동적 환경 (캐시 무효화)
- 새로운 맵 (캐시 없음)

## ⚙️ 설정 변경

`config.py` 파일에서 설정을 변경할 수 있습니다:

```python
# 캐싱 활성화/비활성화
USE_CACHE = False  # True로 변경하면 캐싱 활성화

# 캐시 크기
CACHE_SIZE = 10000  # 캐시 최대 크기

# 동적 장애물 시스템 (NEW!)
ENABLE_DYNAMIC_OBSTACLES = False  # True로 변경하면 동적 장애물 활성화
OBSTACLE_SPAWN_INTERVAL = 50  # 장애물 생성 주기 (스텝)
OBSTACLE_LIFETIME = 30  # 장애물 유지 시간 (스텝)

# 에피소드 수
NUM_EPISODES = 3000  # 더 많이 학습하려면 증가

# 맵 개수
NUM_MAPS = 20  # 더 많은 맵으로 학습하려면 증가
```

## 📚 학습 내용

이 버전을 통해 다음을 학습할 수 있습니다:

- 강화학습의 진짜 목적 이해
- 학습 vs 캐싱의 차이
- Policy > Cache 원칙
- 동적 환경 대응
- 실행 최적화 기법

## 🎓 핵심 원칙 요약

### 1. 학습의 목적
- ❌ 최적 경로 찾기
- ⭕ 환경 적응력 획득

### 2. 도달 속도
- ❌ 물리적으로 빠른 것
- ⭕ 실수를 안 해서 빠른 것

### 3. 캐싱의 역할
- ❌ 학습 (Training)
- ⭕ 실행 최적화 (Execution Optimization)

### 4. 최종 판단
- ❌ 캐시 우선
- ⭕ Policy 우선 (Policy > Cache)

### 5. 동적 환경
- ❌ 캐시만 사용
- ⭕ Policy가 실시간 판단

## 🔮 다음 단계

v5를 마스터하면:
- 강화학습의 본질 이해
- 캐싱과 학습의 차이 이해
- 동적 환경 대응 능력 이해
- 실무에서의 올바른 적용 방법 이해

---

**✅ v5 버전**: Policy > Cache 원칙을 구현한 버전입니다!

**핵심 메시지:**
- 🧠 **학습의 목적은 적응력, 경로가 아님**
- 💾 **캐싱은 실행 최적화, 학습이 아님**
- 🎯 **Policy가 항상 최종 판단자**
- 🌍 **환경이 변하면 Policy가 대응**
