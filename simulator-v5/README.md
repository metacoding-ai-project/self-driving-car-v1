# Simulator v5 - Policy > Cache (캐싱 시스템 버전)

## 📋 프로젝트 설명

이 버전은 **캐싱 시스템**을 추가하여 **Policy > Cache 원칙**을 구현한 버전입니다.

**핵심 철학:**
- 학습의 목적은 '최적 경로'가 아니라 '환경 적응력'
- **더 빠른 도달은 '잘못된 행동을 제거한 결과'다**
- 캐싱은 학습이 아닌 실행 최적화 (최적 경로에 빠르게 도달 가능하지만 학습이 아님!)
- Policy가 항상 최종 판단자

## 🎯 v5의 핵심 개념

### 1. 학습의 진짜 목적

#### ❌ 강화학습의 목적이 아닌 것
- 특정 맵의 최단 경로 암기
- 한 번에 완벽한 경로 계산
- A* 같은 계획 알고리즘 대체

#### ⭕ 강화학습의 진짜 목적
> **처음 보는 환경에서도**
> - 벽에 부딪히지 않고
> - 막힌 공간에서 빠져나오며
> - 목표가 바뀌어도
> - 의미 있게 움직일 수 있는 **범용 행동 정책(policy)을 학습하는 것**

👉 **경로는 학습 목표가 아니라, 실행 결과(output)**

### 2. 왜 학습된 상태에서 더 빨리 도달하는가?

학습된 정책은 물리적으로 더 빠르지 않습니다.
대신 **'헤매는 시간'을 제거**해서 더 빨리 도달합니다.

**학습되지 않은 상태:**
- 벽에 자주 충돌
- 막다른 길에서 오래 정지
- 의미 없는 왕복 이동

**학습된 상태:**
- 벽 근처에서 사전 회피
- 막히면 즉시 방향 전환
- 목표 방향으로 일관된 이동

→ **실수를 안 해서 도달 시간이 감소**

**핵심:** 더 빠른 도달은 물리적으로 빠른 것이 아니라, **잘못된 행동을 제거한 결과**입니다.

### 3. 캐싱이란?

새 맵에서 여러 번 실행하면 경로를 기억합니다.
이는 **학습(training)이 아니라 실행 최적화(execution optimization)** 입니다.

**캐싱의 역할:**
- ✅ **최적 경로에 빠르게 도달 가능**: 같은 맵에서 반복 실행 시 이전 경험을 활용하여 빠르게 도달
- ❌ **하지만 학습이 아님**: 신경망 파라미터는 변경되지 않음

#### 캐싱 vs 학습

| 구분 | 학습 (Training) | 캐싱 (Caching) |
|------|----------------|---------------|
| 신경망 변경 | ✅ 파라미터 업데이트 | ❌ 파라미터 변경 없음 |
| 범위 | 모든 맵에 적용 | 특정 맵만 |
| 동적 환경 | ✅ 대응 가능 | ❌ 캐시 무효화 |
| 일반화 | ✅ 가능 | ❌ 불가능 |

> **뇌가 바뀐 게 아니라, 경험을 메모한 것**

### 4. Policy > Cache 원칙

#### 핵심 원칙

> **Policy > Cache**
> - 정책(policy)은 항상 최종 판단자
> - 캐시는 "힌트 / 가이드" 역할만 수행

#### 동작 방식

```python
1. Policy 네트워크가 행동 예측 (항상 실행!)
2. 캐시에서 힌트 가져오기
3. Policy와 캐시가 일치하면 신뢰도 ↑
4. Policy와 캐시가 충돌하면 Policy 우선!
```

#### 왜 Policy 우선인가?

- 환경이 동적으로 변할 수 있음
- 새로운 장애물이 나타날 수 있음
- 캐시된 경로가 더 이상 유효하지 않을 수 있음

→ **Policy만이 현재 환경을 정확히 판단 가능**

## 🧠 에이전트가 받는 정보 (상태 벡터)

### 에이전트는 무엇을 알고 출발하는가?

**중요**: 에이전트는 목적지의 **절대 좌표를 모릅니다**. 대신 **상대적 방향**만 알고 움직입니다.

#### 상태 벡터 구성 (총 11차원)

```python
상태 = [
    # 1. 주변 8칸 장애물 정보 (8차원)
    wall_up_left, wall_up, wall_up_right,
    wall_left,              wall_right,
    wall_down_left, wall_down, wall_down_right,

    # 2. 현재 방향 (1차원, 정규화)
    direction / 4.0,  # 0=위, 1=오른쪽, 2=아래, 3=왼쪽

    # 3. 목적지 상대 방향 (2차원, 정규화) ⭐ 핵심!
    dx_to_goal / GRID_WIDTH,   # "목적지가 X축으로 저쪽"
    dy_to_goal / GRID_HEIGHT   # "목적지가 Y축으로 저쪽"
]
```

#### ❌ 에이전트가 모르는 것
- 목적지의 절대 좌표 (goal_x, goal_y)
- "목적지가 맵의 (50, 50)이다" 같은 숫자 정보
- 전체 맵의 구조 (전지적 시점 불가)

#### ✅ 에이전트가 아는 것
- **현재 위치에서 목적지가 어느 방향인지** (상대 방향)
  ```python
  dx_to_goal = +0.5  # 오른쪽으로 가야 함
  dy_to_goal = -0.3  # 위쪽으로 가야 함
  ```
- 주변 8칸의 장애물 위치
- 현재 바라보는 방향

#### 🎯 목적 자체는 알고 있는가?

**네, 알고 있습니다!** (보상 설계를 통해)

```python
# 보상 설계
if 목적지 도달:
    reward = +100  # 큰 양의 보상
elif 목적지에 가까워짐:
    reward = +0.5  # 작은 양의 보상
elif 충돌:
    reward = -100  # 큰 음의 보상
```

에이전트는 이 보상을 통해:
- "목적지에 도달하는 것이 좋다"
- "목적지로 가까워지는 것이 좋다"
- "충돌은 나쁘다"

를 학습하므로, **목적 자체(목적지로 가는 것)는 알고 시작**합니다.

#### 🤔 그럼 어떻게 움직이는가?

**매 순간마다:**
1. "지금 위치에서 목적지가 저쪽 방향이다" (상대 방향)
2. "주변에 벽이 이렇게 있다" (장애물 정보)
3. "직진/좌회전/우회전 중 어떤 게 좋을까?" (Policy 판단)
4. 행동 실행
5. 보상 받기 ("목적지에 가까워졌네!" or "충돌했네!")

→ **"저 방향으로 가면 보상이 좋다"는 수준으로 학습하는 구조**

#### 비유
```
❌ GPS 네비게이션 (절대 좌표)
"서울시 강남구 테헤란로 123번지로 가세요"

✅ 나침반 (상대 방향)
"목적지가 북동쪽 방향입니다. 주변 장애물을 피하며 그쪽으로 가세요"
```

---

## ✨ 주요 특징

### 1. 캐싱 시스템 (NEW!) ⭐

**핵심 아이디어: 경험을 메모하되, 정책이 최종 판단!**

#### 동작 원리:
1. 상태-행동 쌍을 캐시에 저장
2. 캐시는 힌트만 제공
3. Policy 네트워크가 최종 판단
4. Policy와 캐시 충돌 시 Policy 우선
5. **실행 후 캐시 업데이트** (NEW!)

#### 구현 방식:
```python
# agent.py - 캐시 클래스
class ActionCache:
    # 상태 -> 행동 매핑 저장
    # 신뢰도(confidence) 추적
    # Policy와의 일치/충돌 통계

# main.py - 캐시 업데이트
action = agent.select_action(state)  # Policy > Cache 원칙
reward, done = car.move(action, env)

# 캐시 업데이트 (성공 여부 기록)
if USE_CACHE:
    success = env.is_goal(car.x, car.y) or (not done)
    agent.update_cache(state, action, success)  # ✅ 경험 메모
```

#### ⚠️ 현재 코드 구조와 실무 차이 (중요!)

**현재 코드 (교육용 설계):**
```python
# agent.py - select_action 메서드
def select_action(self, state, training=False):
    # 1. Policy 먼저 계산 (항상 실행!)
    policy_action = self._compute_policy_action(state)  # 45ms

    # 2. 캐시 확인
    cached_action = self.cache.get(state)  # 1ms

    # 3. Policy 우선
    if cached_action is not None and cached_action == policy_action:
        # 일치하면 캐시 히트로 기록
        self.cache_hits += 1

    return policy_action  # Policy가 최종 결정
```

**문제점:**
- Policy를 **항상 계산**하므로 속도 향상이 없음 (45ms 고정)
- 캐시를 확인해도 Policy 계산을 스킵하지 않음

**왜 이렇게 설계했나요?**

→ **교육 목적: 정적 환경과 동적 환경 두 가지를 모두 보여주기 위해!**

#### 🎯 환경에 따른 사용법

**1️⃣ 정적 환경 (장애물 고정) → 캐시 우선 코드로 수정**

실무에서 맵이 변하지 않는 환경이라면:

```python
# 실무 버전 1: 캐시 우선 (속도 최적화)
def select_action_static(self, state, training=False):
    # 1. 캐시 먼저 확인
    cached_action = self.cache.get(state)
    if cached_action is not None:
        return cached_action  # 12ms (캐시 히트!)

    # 2. 캐시 미스 시에만 Policy 계산
    policy_action = self._compute_policy_action(state)  # 45ms

    # 3. 캐시 저장
    self.cache.store(state, policy_action)

    return policy_action

# 속도: 12ms (캐시 히트율 67% 가정)
```

**장점:**
- **3.75배 빠름** (45ms → 12ms)
- 정적 환경에서 최적
- 창고 로봇, 공장 AGV 등에 적합

**단점:**
- 환경 변화 시 대응 불가
- 동적 장애물 등장하면 충돌 위험

**2️⃣ 동적 환경 (장애물 변화) → Policy 우선 (현재 코드)**

실무에서 환경이 계속 변하는 경우:

```python
# 실무 버전 2: Policy 우선 (안전 보장)
def select_action_dynamic(self, state, training=False):
    # 1. Policy 먼저 계산 (현재 환경 실시간 판단)
    policy_action = self._compute_policy_action(state)  # 45ms

    # 2. 캐시는 힌트로만 사용
    cached_action = self.cache.get(state)
    if cached_action is not None and cached_action != policy_action:
        # 충돌 감지: 환경이 변했음!
        self.cache.invalidate(state)  # 캐시 무효화

    # 3. Policy가 최종 결정
    return policy_action

# 속도: 45ms (안전 우선)
```

**장점:**
- 환경 변화에 즉시 대응
- 안전성 보장
- 사람이 다니는 공간, 동적 환경에 적합

**단점:**
- 속도 이점 없음
- 캐시는 통계/디버깅 용도

#### 📊 캐시 통계

**화면 표시 (USE_CACHE=True일 때):**
```
📦 Cache: Size=5000 | Hits=1200 | Agreements=900 | Conflicts=300
```

**의미:**
- **Size**: 캐시에 저장된 상태 수
- **Hits**: 캐시가 사용된 횟수
- **Agreements**: Policy와 캐시가 일치한 횟수 (신뢰도 높음)
- **Conflicts**: Policy와 캐시가 충돌한 횟수 (Policy 우선!)

**해석:**
```
Agreements가 높으면: Policy와 캐시가 일치 → 정적 환경, 캐시 우선 가능
Conflicts가 높으면: 환경이 동적으로 변함 → Policy 우선 필수
```

#### 🏭 실무 적용 예시

| 환경 | 코드 전략 | 속도 | 안전성 | 예시 |
|------|----------|------|--------|------|
| **정적** | 캐시 우선 | 12ms ⚡ | 보통 | 창고 로봇, 공장 AGV |
| **준동적** | 하이브리드 | 25ms | 높음 | 사무실 청소 로봇 |
| **동적** | Policy 우선 | 45ms | 매우 높음 | 사람 많은 공간 |

**하이브리드 전략 (권장):**
```python
def select_action_hybrid(self, state, training=False):
    cached_action = self.cache.get(state)

    # 캐시 신뢰도가 높으면 캐시 사용
    if cached_action is not None and self.cache.confidence(state) > 0.9:
        return cached_action  # 빠름

    # 신뢰도 낮으면 Policy 확인
    policy_action = self._compute_policy_action(state)

    # 불일치 시 캐시 무효화
    if cached_action is not None and cached_action != policy_action:
        self.cache.invalidate(state)

    return policy_action
```

#### 💡 핵심 정리

**현재 v5 코드:**
- **교육용 설계**: 정적 환경과 동적 환경을 모두 이해하기 위한 구조
- Policy를 항상 계산하여 두 가지 상황을 모두 관찰 가능
- 실무에서는 환경에 따라 코드 수정 필요

**실무 적용:**
- **정적 환경**: 캐시 먼저 확인 → 속도 3.75배 향상
- **동적 환경**: Policy 먼저 계산 → 안전성 보장
- **하이브리드**: 신뢰도 기반 선택 → 속도와 안전성 균형

### 2. 동적 장애물 시스템 (NEW!) ⚡

**핵심 목적: Policy > Cache 원칙을 실전으로 증명!**

#### 해결 원칙: 캐싱의 지위 낮추기

동적 장애물은 **환경이 실시간으로 변하는 상황**을 시뮬레이션합니다.

**❌ 캐시의 치명적 한계:**
- 캐시된 경로는 정적 환경 기준
- 새로운 장애물이 생기면 캐시 무효화
- 캐시만 믿으면 충돌 발생

**✅ Policy가 필수인 이유:**
- Policy는 현재 상태를 실시간 판단
- 환경 변화에 즉시 대응
- 캐시는 참고만, Policy가 최종 결정

#### 동작 방식:
```python
# 환경이 동적으로 변함
env.update_dynamic_obstacles()  # 50스텝마다 새 장애물 생성

# Policy > Cache: 항상 Policy가 최종 판단
action = agent.select_action(state)  # Policy가 현재 환경 판단

# 캐시는 힌트만 제공, Policy가 최종 결정
if cache_action != policy_action:
    return policy_action  # Policy 우선!
```

#### 동적 장애물 특징:
- 🔴 **50스텝마다 랜덤 위치에 장애물 생성** (오렌지색)
- ⏰ **30스텝 후 자동 소멸**
- 🚨 **캐시된 경로 무효화**
- ✅ **Policy가 실시간 회피**

이를 통해 **Policy > Cache 원칙**이 단순한 이론이 아니라 **실전에서 필수**임을 증명합니다!

### 3. v4의 모든 기능 포함 ✅

- ✅ Learning Rate Scheduling
- ✅ 여러 맵에서 학습 (20개 이상)
- ✅ 랜덤 시작점/목적지
- ✅ 랜덤 초기 방향
- ✅ 느린 Epsilon Decay (0.998)
- ✅ Gradient Clipping
- ✅ 경로 다양성 보상
- ✅ 테스트셋 분리

## 🚀 사용 방법

### 1. 패키지 설치

```bash
python -m pip install pygame-ce numpy torch matplotlib
```

### 2. 훈련 시작

```bash
cd simulator-v5
python train.py
```

**특징:**
- 기본적으로 캐싱 비활성화 (학습 목적)
- `config.py`에서 `USE_CACHE = True`로 변경하여 활성화 가능

### 3. 테스트

```bash
python main.py
```

**특징:**
- 훈련에 사용하지 않은 새로운 맵에서 테스트
- 캐싱 활성화 시 반복 실행하면 경로 최적화
- 일반화 능력 검증

## 📈 학습 결과

훈련이 완료되면:
- `model_final.pth`: 최종 모델 (캐시는 저장 안 됨)
- `training_results.png`: 학습 그래프

## 🔄 버전 비교

| 항목 | v1 | v2 | v3 | v4 | v5 |
|------|----|----|----|----|---|
| 초기 방향 | 고정 | 랜덤 | 랜덤 | 랜덤 | 랜덤 |
| Epsilon decay | 0.995 | 0.998 | 0.998 | 0.998 | 0.998 |
| Gradient clipping | 없음 | 있음 | 있음 | 있음 | 있음 |
| 경로 다양성 보상 | 없음 | 있음 | 있음 | 있음 | 있음 |
| 맵 개수 | 1개 | 1개 | 20개 이상 | 20개 이상 | 20개 이상 |
| 시작점/목적지 | 고정 | 고정 | 랜덤 | 랜덤 | 랜덤 |
| 테스트셋 분리 | 없음 | 없음 | ✅ | ✅ | ✅ |
| Learning Rate Scheduling | 없음 | 없음 | 없음 | ✅ | ✅ |
| **캐싱 시스템** | 없음 | 없음 | 없음 | 없음 | ✅ |
| **Policy > Cache 원칙** | 없음 | 없음 | 없음 | 없음 | ✅ |
| **동적 장애물 시스템** | 없음 | 없음 | 없음 | 없음 | ✅ |

## 💡 캐싱 시스템의 효과

### 언제 유용한가?

- 같은 맵에서 반복 실행할 때
- 테스트/데모 시나리오
- 실행 시간 최적화가 필요할 때
- **최적 경로에 빠르게 도달하고 싶을 때** (하지만 학습이 아님!)

### 언제 사용하지 말아야 하는가?

- 훈련 중 (일반화 방해)
- 동적 환경 (캐시 무효화)
- 새로운 맵 (캐시 없음)

### 💡 캐싱 지위를 낮추고 실행하면 어떤 이점이 있는가?

v5에서는 **캐싱의 지위를 낮추고 Policy를 우선시**하는 설계를 채택했습니다. 이는 단순한 기술적 선택이 아니라 **강화학습의 본질을 이해하기 위한 중요한 설계 결정**입니다.

#### ✅ 캐싱 지위를 낮춘 설계의 이점

1. **동적 환경 대응 능력**
   - 환경이 실시간으로 변할 때 (예: 새로운 장애물 등장)
   - 캐시는 과거 정보만 가지고 있어 무효화됨
   - **Policy는 현재 상태를 실시간으로 판단**하여 즉시 대응 가능

2. **일반화 능력 유지**
   - 캐싱을 우선시하면 특정 맵에만 최적화됨
   - **Policy 우선 설계는 모든 맵에서 작동하는 범용 능력 유지**

3. **학습 목적 명확화**
   - 캐싱은 "경험 메모"일 뿐, 학습이 아님
   - **Policy > Cache 원칙으로 학습과 실행 최적화를 명확히 구분**

4. **실전 적용 가능성**
   - 실제 자율주행 환경은 예측 불가능
   - **Policy 우선 설계는 실제 환경에서도 안정적으로 작동**

5. **철학적 이해**
   - 강화학습의 목적이 "경로 암기"가 아님을 명확히 인식
   - **"환경 적응력"이라는 본질적 목표를 이해**

#### 🔄 캐싱의 올바른 역할

캐싱은 **Policy의 보조 도구**로만 사용됩니다:

- ✅ **힌트 제공**: Policy가 결정할 때 참고 정보로 활용
- ✅ **실행 최적화**: 같은 맵에서 반복 실행 시 속도 향상
- ✅ **최적 경로 도달**: 이전 경험을 활용하여 빠르게 도달 가능
- ❌ **최종 판단**: 절대 Policy를 대체하지 않음

**결론:** 캐싱 지위를 낮춘 설계는 단기적 성능 향상보다 **장기적 안정성과 일반화 능력**을 우선시하는 설계입니다. 이는 실제 자율주행 시스템에서도 중요한 원칙입니다.

## ⚙️ 설정 변경

`config.py` 파일에서 설정을 변경할 수 있습니다:

```python
# 캐싱 활성화/비활성화
USE_CACHE = False  # True로 변경하면 캐싱 활성화

# 캐시 크기
CACHE_SIZE = 10000  # 캐시 최대 크기

# 동적 장애물 시스템 (NEW!)
ENABLE_DYNAMIC_OBSTACLES = False  # True로 변경하면 동적 장애물 활성화
OBSTACLE_SPAWN_INTERVAL = 50  # 장애물 생성 주기 (스텝)
OBSTACLE_LIFETIME = 30  # 장애물 유지 시간 (스텝)

# 에피소드 수
NUM_EPISODES = 3000  # 더 많이 학습하려면 증가

# 맵 개수
NUM_MAPS = 20  # 더 많은 맵으로 학습하려면 증가
```

## 📚 학습 내용

이 버전을 통해 다음을 학습할 수 있습니다:

- 강화학습의 진짜 목적 이해
- 학습 vs 캐싱의 차이
- Policy > Cache 원칙
- 동적 환경 대응
- 실행 최적화 기법

## 🎓 핵심 원칙 요약

### 1. 학습의 목적
- ❌ 최적 경로 찾기
- ⭕ 환경 적응력 획득

### 2. 도달 속도
- ❌ 물리적으로 빠른 것
- ⭕ 실수를 안 해서 빠른 것

### 3. 캐싱의 역할
- ❌ 학습 (Training)
- ⭕ 실행 최적화 (Execution Optimization)

### 4. 최종 판단
- ❌ 캐시 우선
- ⭕ Policy 우선 (Policy > Cache)

### 5. 동적 환경
- ❌ 캐시만 사용
- ⭕ Policy가 실시간 판단

## 🔮 다음 단계

v5를 마스터하면:
- 강화학습의 본질 이해
- 캐싱과 학습의 차이 이해
- 동적 환경 대응 능력 이해
- 실무에서의 올바른 적용 방법 이해

---

**✅ v5 버전**: Policy > Cache 원칙을 구현한 버전입니다!

**핵심 메시지:**
- 🧠 **학습의 목적은 적응력, 경로가 아님**
- 💾 **캐싱은 실행 최적화, 학습이 아님**
- 🎯 **Policy가 항상 최종 판단자**
- 🌍 **환경이 변하면 Policy가 대응**
