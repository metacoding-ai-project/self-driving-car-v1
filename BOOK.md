# 🚗 자율주행 강화학습 프로젝트 완전 가이드

> **생성 시간**: 2026-02-06 23:57:06

이 책은 v1부터 v5까지의 강화학습 프로젝트를 단계별로 설명합니다.

## 📚 목차

1. [프로젝트 소개](#프로젝트-소개)
2. [v1: 과적합 문제 확인](#v1-과적합-문제-확인)
3. [v2: 과적합 해결](#v2-과적합-해결)
4. [v3: 완전한 일반화](#v3-완전한-일반화)
5. [v4: 학습 속도 향상](#v4-학습-속도-향상)
6. [v5: Policy > Cache (학습 철학 이해)](#v5-policy--cache-학습-철학-이해)
7. [버전별 비교](#버전별-비교)
8. [실전 적용 가이드](#실전-적용-가이드)

---

## 프로젝트 소개

### 🎯 목표
강화학습의 과적합(Overfitting) 문제를 단계적으로 해결하는 과정을 학습합니다.

### 📈 학습 순서
1. **v1 실행** → 과적합 문제 확인
2. **v2 실행** → 해결 방법 확인
3. **v3 실행** → 완전한 일반화 확인
4. **v4 실행** → 학습 속도 향상 확인
5. **v5 실행** → 강화학습 철학 이해 (권장) 🧠

### 🔑 핵심 개념
- **과적합(Overfitting)**: 학습 데이터에만 특화되어 새로운 데이터에서 성능이 떨어지는 현상
- **일반화(Generalization)**: 다양한 상황에서도 잘 작동하는 능력
- **강화학습(Reinforcement Learning)**: 시행착오를 통해 최적의 행동을 학습하는 방법

---


## v1: 과적합 문제 확인


## 📋 프로젝트 설명

이 버전은 **과적합(Overfitting) 문제가 발생하는 원본 버전**입니다. 
강화학습의 과적합 문제를 이해하고 해결 방법을 학습하기 위한 교육용 예제입니다.

## 🎯 주요 특징

- **단일 맵**: 하나의 고정된 맵에서만 학습
- **고정된 시작점/목적지**: 항상 (2, 2) → (27, 27)
- **고정된 초기 방향**: 항상 위쪽(0) 방향으로 시작
- **빠른 Epsilon Decay**: 0.995 (탐험 시간 부족)
- **Gradient Clipping 없음**: 학습 불안정
- **경로 다양성 보상 없음**: 같은 경로만 반복

## ⚠️ 문제점 (과적합 발생)

### 1. 한쪽으로만 이동하는 문제

**증상:**
- 에이전트가 한 번 최적 경로를 찾으면 계속 그 경로만 사용
- 오른쪽/아래쪽으로만 이동
- 왼쪽/위쪽 방향을 거의 사용하지 않음

**원인:**
- 고정된 초기 방향 (항상 위쪽)
- 매 에피소드마다 같은 위치, 같은 방향에서 시작
- 왼쪽/위쪽 방향을 탐험할 기회가 없음

### 2. 특정 경로에 고착화

**증상:**
- 학습 초기에 찾은 경로만 계속 사용
- 다른 경로를 시도하지 않음
- 경로 다양성이 없음

**원인:**
- 빠른 Epsilon Decay (0.995)
- 100 에피소드만 지나도 epsilon이 0.6 이하로 떨어짐
- 탐험 시간이 부족함
- 경로 다양성 보상이 없음

### 3. 단일 맵 학습

**증상:**
- 특정 맵의 특정 경로만 암기
- 다른 맵에서 작동하지 않음
- 일반화 불가능

**원인:**
- 하나의 맵만 반복 학습
- 고정된 시작점/목적지
- 맵 구조가 항상 동일

### 4. 학습 불안정성

**증상:**
- 학습이 불안정함
- 가중치가 너무 커질 수 있음
- 성능이 일정하지 않음

**원인:**
- Gradient Clipping 없음
- 정규화 없음

## 📊 예상되는 학습 결과

### Episode 0-100: 초보 단계 🔴
- AI가 랜덤하게 움직임 (하지만 위쪽에서 시작)
- 벽에 자주 부딪힘
- 평균 3-5스텝 생존
- **오른쪽/아래쪽으로만 이동하는 패턴 시작**

### Episode 100-300: 학습 시작 🟡
- 조금씩 패턴 발견
- 벽을 피하기 시작
- 10-20스텝 생존
- **한쪽으로만 이동하는 패턴 강화**

### Episode 300-500: 과적합 발생 ❌
- 벽을 거의 안 부딪힘
- 100-500스텝 생존
- **하지만 항상 같은 경로만 사용**
- **다른 경로를 시도하지 않음**
- **왼쪽/위쪽 방향을 거의 사용하지 않음**

## 🚀 사용 방법

### 1. 패키지 설치

```bash
python -m pip install pygame-ce numpy torch matplotlib
```

### 2. 훈련 시작

```bash
cd simulator-v1
python train.py
```

### 3. 테스트

```bash
python main.py
```

## 📈 학습 결과 분석

훈련 후 다음을 확인하세요:

1. **경로 다양성**: 항상 같은 경로만 사용하는지 확인
2. **방향 사용**: 왼쪽/위쪽 방향을 사용하는지 확인
3. **일반화**: 다른 맵에서도 작동하는지 확인 (작동하지 않을 것)

## 🔍 문제 확인 방법

1. **시각적 확인**: 화면에서 자동차가 항상 같은 경로로 가는지 확인
2. **방향 분석**: 왼쪽/위쪽 방향을 사용하는지 확인
3. **다른 맵 테스트**: 맵을 바꿔서 테스트 (작동하지 않을 것)

## 📚 다음 단계

이 버전의 문제점을 이해했다면:

1. **v2 확인**: 과적합 해결 방법을 적용한 버전
2. **v3 확인**: 여러 맵과 랜덤 시작점/목적지를 사용하는 최종 버전
3. **과적합해결법.md 참고**: 상세한 해결 방법 확인

## 💡 학습 목표

이 버전을 통해 다음을 학습할 수 있습니다:

- 과적합 문제의 원인 이해
- 고정된 초기 조건의 문제점
- 빠른 Epsilon Decay의 문제점
- 단일 맵 학습의 한계
- 경로 다양성의 중요성

---

## 🔮 다음 단계 개선 사항

이 버전의 문제점들은 다음 버전에서 해결됩니다:

### ✅ v2에서 해결될 사항
- **랜덤 초기 방향**: 모든 방향 탐험 가능
- **느린 Epsilon Decay**: 충분한 탐험 시간 확보
- **Gradient Clipping**: 학습 안정성 향상
- **경로 다양성 보상**: 다양한 경로 탐험 유도

### 📋 현재 알려진 문제점
1. **한쪽으로만 이동**: 고정된 초기 방향으로 인한 문제
2. **경로 고착화**: 빠른 Epsilon Decay로 인한 문제
3. **학습 불안정**: Gradient Clipping 없음
4. **단일 맵 학습**: 일반화 불가능

**다음 버전**: [`simulator-v2/README.md`](../simulator-v2/README.md)에서 해결 방법 확인

---

**⚠️ 주의**: 이 버전은 교육용으로, 실제 프로젝트에는 사용하지 마세요!


---

### SIMULATOR-V1 주요 특징


**과적합 문제가 있는 원본 버전**

- ❌ 고정된 초기 방향 (항상 위쪽)
- ❌ 빠른 Epsilon Decay (0.995) → 탐험 시간 부족
- ❌ Gradient Clipping 없음 → 학습 불안정
- ❌ 경로 다양성 보상 없음 → 같은 경로만 반복
- ❌ 단일 맵 학습 → 일반화 불가능

**문제점:**
- 한쪽으로만 이동하는 문제 발생
- 특정 경로에 고착화
- 새로운 맵에서 작동하지 않음

---


## v2: 과적합 해결


## 📋 프로젝트 설명

이 버전은 **과적합(Overfitting) 문제를 해결한 버전**입니다.
v1의 문제점을 해결하기 위해 여러 기법을 적용했습니다.

## ✨ 주요 개선사항

### 1. 랜덤 초기 방향 ✅
- 매 에피소드마다 랜덤한 방향에서 시작
- 모든 방향(위/아래/좌/우) 탐험 가능
- 한쪽으로만 이동하는 문제 해결

### 2. 느린 Epsilon Decay ✅
- `epsilon_decay = 0.998` (v1: 0.995)
- 더 오래 탐험하여 다양한 경로 학습
- 과적합 방지

### 3. Gradient Clipping ✅
- 학습 안정성 향상
- 과도한 학습 방지
- 일반화 능력 향상

### 4. 경로 다양성 보상 ✅ (핵심 개선사항)
- **방문한 위치 추적**: `visited_positions` 집합 사용
- **새로운 위치 방문 보상**: +0.2 보상
- **같은 경로 반복 패널티**: -0.1 패널티
- 다양한 경로 탐험 인센티브 제공

### 5. 단일 맵 (여전히 한계)
- 하나의 맵만 사용
- 고정된 시작점/목적지
- 일반화는 여전히 제한적

## 🎯 경로 다양성 보상 시스템

### 구현 방법

```python
# car.py
class Car:
    def __init__(self, x, y):
        # ...
        self.visited_positions = set()  # 방문한 위치 추적
    
    def reset(self, x, y, direction=None):
        # ...
        self.visited_positions = set()  # 방문 기록 초기화
    
    def move(self, action, environment):
        # ...
        # 경로 다양성 보상
        if (next_x, next_y) not in self.visited_positions:
            reward += 0.2  # 새로운 위치 방문 보상
            self.visited_positions.add((next_x, next_y))
        
        # 너무 같은 경로만 가면 패널티
        if self.steps > 10 and len(self.visited_positions) < self.steps * 0.5:
            reward -= 0.1  # 같은 곳만 돌아다니면 패널티
```

### 효과

- ✅ 다양한 경로 탐험 인센티브
- ✅ 같은 경로 반복 방지
- ✅ 탐험 다양성 증가
- ✅ 과적합 완화

## 📊 예상되는 학습 결과

### Episode 0-100: 초보 단계 🔴
- AI가 랜덤하게 움직임 (모든 방향 탐험)
- 벽에 자주 부딪힘
- 평균 3-5스텝 생존

### Episode 100-300: 학습 시작 🟡
- 조금씩 패턴 발견
- 벽을 피하기 시작
- 10-20스텝 생존
- **다양한 경로 시도**

### Episode 300-500: 개선 ✅
- 벽을 거의 안 부딪힘
- 100-500스텝 생존
- **다양한 경로 사용**
- **모든 방향으로 이동**

## 🚀 사용 방법

### 1. 패키지 설치

```bash
python -m pip install pygame-ce numpy torch matplotlib
```

### 2. 훈련 시작

```bash
cd simulator-v2
python train.py
```

### 3. 테스트

```bash
python main.py
```

## 🔄 v1과의 차이점

| 항목 | v1 | v2 |
|------|----|----|
| 초기 방향 | 고정 (항상 위쪽) | 랜덤 |
| Epsilon decay | 0.995 (빠름) | 0.998 (느림) |
| Gradient clipping | 없음 | 있음 |
| 경로 다양성 보상 | 없음 | 있음 |
| 한쪽으로만 이동 | 발생 | 해결 |
| 경로 다양성 | 없음 | 있음 |
| 단일 맵 | 있음 | 있음 (여전히 한계) |

## ⚠️ 여전히 남은 한계점

### 1. 단일 맵 학습
- 하나의 맵만 사용
- 다른 맵에서 작동하지 않음
- 일반화 제한적

### 2. 고정된 시작점/목적지
- 항상 같은 위치에서 시작
- 항상 같은 목적지로 이동
- 다양한 상황 학습 불가

### 3. 일반화 제한
- 특정 맵에 특화됨
- 실제 환경에 적용 어려움

## 📚 다음 단계

완전한 일반화를 원한다면:

1. **v3 확인**: 여러 맵과 랜덤 시작점/목적지를 사용하는 최종 버전
2. **과적합해결법.md 참고**: 추가 해결 방법 확인

## 💡 학습 목표

이 버전을 통해 다음을 학습할 수 있습니다:

- 과적합 해결 방법
- 경로 다양성 보상의 효과
- 랜덤 초기화의 중요성
- Epsilon Decay 조정의 효과
- Gradient Clipping의 역할

---

## 🔮 다음 단계 개선 사항

이 버전의 한계점들은 다음 버전에서 해결됩니다:

### ✅ v3에서 해결될 사항
- **여러 맵 학습**: 20개 이상의 다양한 맵에서 학습
- **랜덤 시작점/목적지**: 다양한 상황 학습
- **완전한 일반화**: 새로운 맵에서도 작동
- **테스트셋 분리**: 과적합 여부 정확히 검증

### 📋 현재 알려진 한계점
1. **단일 맵 학습**: 하나의 맵만 사용하여 일반화 제한적
2. **고정된 시작점/목적지**: 다양한 상황 학습 불가
3. **일반화 제한**: 실제 환경에 적용 어려움

**다음 버전**: [`simulator-v3/README.md`](../simulator-v3/README.md)에서 완전한 일반화 버전 확인

---

**✅ 개선됨**: v1의 과적합 문제가 해결되었지만, 여전히 단일 맵 학습의 한계가 있습니다.


---

### SIMULATOR-V2 주요 특징


**과적합 해결 버전**

- ✅ 랜덤 초기 방향 추가 → 모든 방향 탐험 가능
- ✅ 느린 Epsilon Decay (0.998) → 충분한 탐험 시간
- ✅ Gradient Clipping 추가 → 안정적인 학습
- ✅ 경로 다양성 보상 추가 → 다양한 경로 탐험
- ⚠️ 단일 맵 학습 (여전히 한계)

**개선사항:**
- 한쪽으로만 이동하는 문제 해결
- 다양한 경로 사용
- 하지만 여전히 특정 맵에 특화됨

---


## v3: 완전한 일반화


## 📋 프로젝트 설명

이 버전은 **모든 문제가 해결된 최종 버전**입니다.
여러 맵에서 학습하고, 랜덤 시작점/목적지를 사용하여 완전한 일반화를 달성했습니다.

## ✨ 주요 특징

### 1. 여러 맵에서 학습 ✅
- **20개 이상의 다양한 맵**: 단순, 중간, 복잡, 랜덤 맵
- **매 에피소드마다 다른 맵**: 일반화된 패턴 학습
- **랜덤 맵 생성**: 완전히 새로운 맵도 처리 가능

### 2. 랜덤 시작점/목적지 ✅
- **매 에피소드마다 다른 위치**: 다양한 상황 학습
- **랜덤 초기 방향**: 모든 방향 탐험
- **일반화된 경로 찾기**: 어떤 위치에서도 작동

### 3. 모든 v2 개선사항 포함 ✅
- 랜덤 초기 방향
- 느린 Epsilon Decay (0.998)
- Gradient Clipping
- 경로 다양성 보상
- 방문한 위치 추적

### 4. 개선된 하이퍼파라미터 ✅
- 에피소드 수: 3,000 (v1: 500)
- 배치 크기: 64 (v1: 32)
- 학습률: 0.0005 (v1: 0.001)
- 메모리 용량: 20,000 (v1: 10,000)

## 🎯 해결된 문제점

### ✅ v1의 모든 문제 해결
- 한쪽으로만 이동하는 문제 → 해결
- 특정 경로에 고착화 → 해결
- 단일 맵 학습 → 해결
- 학습 불안정성 → 해결

### ✅ v2의 한계점 해결
- 단일 맵 학습 → 여러 맵 학습
- 고정된 시작점/목적지 → 랜덤 시작점/목적지
- 일반화 제한 → 완전한 일반화

## 📊 예상되는 학습 결과

### Episode 0-500: 초보 단계 🔴
- 여러 맵에서 랜덤하게 움직임
- 벽에 자주 부딪힘
- 평균 3-5스텝 생존
- 성공률: 10-20%

### Episode 500-1500: 학습 시작 🟡
- 다양한 맵에서 패턴 발견
- 벽을 피하기 시작
- 10-50스텝 생존
- 성공률: 30-50%
- **다양한 맵에서 작동**

### Episode 1500-3000: 마스터! ✅
- 모든 맵에서 벽을 거의 안 부딪힘
- 100-500스텝 생존
- 성공률: 60-80%
- **새로운 맵에서도 작동**
- **랜덤 시작점/목적지에서도 작동**

## 🚀 사용 방법

### 1. 패키지 설치

```bash
python -m pip install pygame-ce numpy torch matplotlib
```

### 2. 훈련 시작

```bash
cd simulator-v3
python train.py
```

**특징:**
- 여러 맵에서 자동으로 학습
- 맵별 성공률 추적
- 성공률 그래프 자동 생성

### 3. 테스트

```bash
python main.py
```

**특징:**
- 랜덤 맵으로 테스트
- R키: 새로운 랜덤 맵으로 리셋
- 일반화 성능 확인

## 📈 학습 결과

훈련이 완료되면:
- `model_final.pth`: 최종 모델
- `training_results.png`: 학습 그래프
  - 보상 그래프
  - 에피소드 길이 그래프
  - 맵별 성공률
  - 성공률 추이

## 🔄 버전 비교

| 항목 | v1 | v2 | v3 |
|------|----|----|----|
| 초기 방향 | 고정 | 랜덤 | 랜덤 |
| Epsilon decay | 0.995 | 0.998 | 0.998 |
| Gradient clipping | 없음 | 있음 | 있음 |
| 경로 다양성 보상 | 없음 | 있음 | 있음 |
| 맵 개수 | 1개 | 1개 | 20개 이상 |
| 시작점/목적지 | 고정 | 고정 | 랜덤 |
| 일반화 | 불가능 | 제한적 | 완전 |
| 실제 적용 | 어려움 | 어려움 | 가능 |

## 🎯 실제 적용 가능

이 버전은 다음에 사용할 수 있습니다:

- ✅ 새로운 맵에서도 작동
- ✅ 다양한 시작점/목적지에서 작동
- ✅ 실제 환경에 적용 가능
- ✅ 라즈베리파이에서 사용 가능

---

## 🧪 테스트셋과 훈련셋 분리의 중요성

### 🤔 왜 테스트셋과 훈련셋이 달라야 할까요?

**핵심 문제: 과적합(Overfitting) 검증**

모델이 **훈련에 사용한 맵에서만 잘 작동**하고, **새로운 맵에서는 실패**한다면, 이는 **과적합**입니다.

### 📊 훈련셋 vs 테스트셋

#### 훈련셋 (Training Set)
- **목적**: 모델을 학습시키는 데 사용
- **v3에서**: 0-19번 맵 (20개 맵)
- **역할**: 다양한 패턴을 학습하도록 도와줌

#### 테스트셋 (Test Set)
- **목적**: 모델의 일반화 능력을 검증
- **v3에서**: 20번 이상의 새로운 맵 또는 완전히 새로운 랜덤 맵
- **역할**: 모델이 **보지 못한 새로운 상황**에서도 작동하는지 확인

### ❌ 테스트셋을 훈련셋과 같게 하면?

```
훈련: 맵 0-19에서 학습
테스트: 맵 0-19에서 테스트

문제:
- 모델이 이미 본 맵에서만 테스트됨
- 과적합 여부를 확인할 수 없음
- 실제 성능을 정확히 평가할 수 없음
```

**비유:** 시험 문제를 미리 외워서 시험을 보는 것과 같습니다. 진짜 실력을 알 수 없습니다!

### ✅ 테스트셋을 훈련셋과 다르게 하면?

```
훈련: 맵 0-19에서 학습
테스트: 맵 20+ 또는 완전히 새로운 랜덤 맵에서 테스트

장점:
- 모델의 일반화 능력을 정확히 평가
- 과적합 여부를 확인 가능
- 실제 성능을 알 수 있음
- 새로운 상황에서도 작동하는지 검증
```

**비유:** 새로운 문제로 시험을 보는 것과 같습니다. 진짜 실력을 알 수 있습니다!

### 🎯 v3의 해결 방법

#### 훈련 단계 (`train.py`)
```python
# 훈련 맵: 0-19번 맵만 사용
current_map_id = random.randint(0, NUM_MAPS - 1)  # 0-19
env.reset_map(current_map_id)
```

#### 테스트 단계 (`main.py`)
```python
# 테스트 맵: 훈련에 사용하지 않은 새로운 맵 사용
# 방법 1: 완전히 새로운 랜덤 맵
env = GridEnvironment(random_map=True)

# 방법 2: 훈련 범위를 벗어난 맵 ID (20+)
# (구현 가능)
```

### 💡 핵심 정리

1. **훈련셋**: 모델을 학습시키는 데 사용 (0-19번 맵)
2. **테스트셋**: 모델의 일반화 능력을 검증 (새로운 맵)
3. **분리 이유**: 과적합 여부를 확인하고 실제 성능을 평가하기 위해
4. **v3의 장점**: 테스트 시 새로운 맵에서 실행하여 일반화 능력을 검증

**비유:**
- **훈련셋 = 연습 문제**: 여러 문제를 풀어보며 패턴을 학습
- **테스트셋 = 시험 문제**: 새로운 문제로 실력을 검증

---

## 🎯 실제 적용 가능

이 버전은 다음에 사용할 수 있습니다:

- ✅ 새로운 맵에서도 작동
- ✅ 다양한 시작점/목적지에서 작동
- ✅ 실제 환경에 적용 가능
- ✅ 라즈베리파이에서 사용 가능

## ⚙️ 설정 변경

`config.py` 파일에서 설정을 변경할 수 있습니다:

```python
# 에피소드 수
NUM_EPISODES = 3000  # 더 많이 학습하려면 증가

# 맵 개수
NUM_MAPS = 20  # 더 많은 맵으로 학습하려면 증가

# 속도
CURRENT_SPEED = SPEED_NORMAL  # SPEED_FAST로 변경하면 빠르게
```

## 💡 팁

1. **빠른 학습**: `SHOW_TRAINING = False` 설정
2. **더 많은 맵**: `NUM_MAPS = 30` 또는 50으로 증가
3. **더 긴 학습**: `NUM_EPISODES = 5000` 또는 10000

## 📚 학습 내용

이 버전을 통해 다음을 학습할 수 있습니다:

- 완전한 일반화 달성
- 여러 맵에서의 학습 방법
- 랜덤 시작점/목적지 처리
- 과적합 완전 해결
- 실제 환경 적용 방법

## 🎓 다음 단계

이 버전으로 학습한 모델은:

1. **새로운 맵에서도 작동**
2. **실제 환경에 적용 가능**
3. **라즈베리파이에서 사용 가능**

---

## 🔮 다음 단계 개선 사항

이 버전의 개선 가능한 사항들:

### ✅ v4에서 해결될 사항
- **Learning Rate Scheduling**: 동적 학습률 조정으로 학습 속도 향상
- **빠른 초기 탐색**: 큰 보폭으로 빠른 탐색
- **정밀한 후기 학습**: 작은 보폭으로 정밀 조정

### 📋 현재 알려진 한계점
1. **고정된 학습률**: 학습 속도가 느릴 수 있음
2. **초기 탐색 속도**: 고정된 학습률로 인한 느린 초기 탐색
3. **최적점 근처 정밀 조정**: 고정된 학습률로 인한 제한

**다음 버전**: [`simulator-v4/README.md`](../simulator-v4/README.md)에서 학습 속도 향상 버전 확인

---

**✅ 최종 버전**: 모든 문제가 해결된 완전한 일반화 버전입니다!


---

### SIMULATOR-V3 주요 특징


**완전한 일반화 버전** ⭐

- ✅ v2의 모든 개선사항 포함
- ✅ 여러 맵 학습 (20개 이상) → 다양한 맵 패턴 학습
- ✅ 랜덤 시작점/목적지 → 다양한 상황 학습
- ✅ 테스트셋 분리 → 과적합 여부 정확히 검증
- ✅ 개선된 하이퍼파라미터 (에피소드 3,000, 배치 64)

**해결된 문제:**
- 완전한 일반화 달성
- 새로운 맵에서도 작동
- 실제 환경에 적용 가능

---


## v4: 학습 속도 향상


## 📋 프로젝트 설명

이 버전은 **Learning Rate Scheduling**을 적용하여 **학습 속도를 향상**시킨 버전입니다.

v3의 모든 기능을 유지하면서, **동적 학습률 조정**을 통해 더 빠르고 정확한 학습이 가능합니다.

## ✨ 주요 특징

### 1. Learning Rate Scheduling (학습률 스케줄링) ⭐ NEW!

**핵심 아이디어: 보폭을 조절하여 빠르고 정확하게 학습!**

#### 동작 원리:
1. **처음에는 큰 보폭** (큰 학습률)으로 전체를 빠르게 탐색
2. **손실이 적은 구간을 찾으면**
3. **그 구간에서만 보폭을 줄여서** (작은 학습률) 정밀하게 학습

#### 구현 방식:
```python
# Exponential Decay 방식
initial_lr = 0.001      # 초기 학습률 (큰 보폭)
min_lr = 0.0001         # 최소 학습률 (작은 보폭)
decay_rate = 0.998      # 감소율

# 에피소드마다 학습률 감소
learning_rate = initial_lr * (decay_rate ** episode)
learning_rate = max(learning_rate, min_lr)  # 최소값 보장
```

#### 학습률 변화:
```
Episode 0:   lr = 0.001000  (큰 보폭 - 빠른 탐색)
Episode 100: lr = 0.000820  (여전히 큰 보폭)
Episode 500: lr = 0.000368  (중간 보폭)
Episode 1000: lr = 0.000135  (작은 보폭 - 정밀 학습)
Episode 2000: lr = 0.000100  (최소 보폭 - 최적점 근처)
```

### 2. v3의 모든 기능 포함 ✅

- ✅ 여러 맵에서 학습 (20개 이상)
- ✅ 랜덤 시작점/목적지
- ✅ 랜덤 초기 방향
- ✅ 느린 Epsilon Decay (0.998)
- ✅ Gradient Clipping
- ✅ 경로 다양성 보상
- ✅ 방문한 위치 추적
- ✅ 테스트셋 분리

### 3. 개선된 하이퍼파라미터 ✅

- 초기 학습률: 0.001 (v3: 0.0005) - **더 빠른 초기 탐색**
- 최소 학습률: 0.0001 - **정밀한 후기 학습**
- 동적 학습률 조정: 에피소드마다 자동 감소

## 🎯 해결된 문제점

### ✅ 학습 속도 향상

**v3의 문제:**
- 고정된 학습률로 인해 학습 속도가 느림
- 초기 탐색이 느림
- 최적점 근처에서 정밀 조정이 어려움

**v4의 해결:**
- ✅ **큰 보폭으로 빠른 초기 탐색**
- ✅ **작은 보폭으로 정밀한 후기 학습**
- ✅ **더 빠른 수렴**
- ✅ **더 나은 성능**

### 📊 예상되는 학습 결과

#### Episode 0-500: 빠른 초기 탐색 🚀
- **큰 보폭**으로 빠르게 탐색
- 여러 맵에서 패턴 발견
- 벽을 피하기 시작
- 성공률: 20-40% (v3보다 빠름!)

#### Episode 500-1500: 안정적 학습 🟡
- **중간 보폭**으로 안정적 학습
- 다양한 맵에서 작동
- 성공률: 50-70% (v3보다 빠름!)

#### Episode 1500-3000: 정밀 학습 ✅
- **작은 보폭**으로 정밀하게 학습
- 최적점 근처에서 정밀 조정
- 성공률: 70-85% (v3보다 높음!)
- **더 빠른 수렴**

## 🚀 사용 방법

### 1. 패키지 설치

```bash
python -m pip install pygame-ce numpy torch matplotlib
```

### 2. 훈련 시작

```bash
cd simulator-v4
python train.py
```

**특징:**
- Learning Rate Scheduling 자동 적용
- 학습률이 에피소드마다 자동 감소
- 화면에 현재 학습률 표시

### 3. 테스트

```bash
python main.py
```

**특징:**
- 훈련에 사용하지 않은 새로운 맵에서 테스트
- 일반화 능력 검증

## 📈 학습 결과

훈련이 완료되면:
- `model_final.pth`: 최종 모델
- `training_results.png`: 학습 그래프
  - 보상 그래프
  - 에피소드 길이 그래프
  - 맵별 성공률
  - 성공률 추이
  - **학습률 변화 그래프** (NEW!)

## 🔄 버전 비교

| 항목 | v1 | v2 | v3 | v4 |
|------|----|----|----|----|
| 초기 방향 | 고정 | 랜덤 | 랜덤 | 랜덤 |
| Epsilon decay | 0.995 | 0.998 | 0.998 | 0.998 |
| Gradient clipping | 없음 | 있음 | 있음 | 있음 |
| 경로 다양성 보상 | 없음 | 있음 | 있음 | 있음 |
| 맵 개수 | 1개 | 1개 | 20개 이상 | 20개 이상 |
| 시작점/목적지 | 고정 | 고정 | 랜덤 | 랜덤 |
| 테스트셋 분리 | 없음 | 없음 | ✅ | ✅ |
| **Learning Rate Scheduling** | 없음 | 없음 | 없음 | ✅ |
| **학습 속도** | 느림 | 느림 | 보통 | **빠름** ⚡ |

## 🎯 실제 적용 가능

이 버전은 다음에 사용할 수 있습니다:

- ✅ 새로운 맵에서도 작동
- ✅ 다양한 시작점/목적지에서 작동
- ✅ 실제 환경에 적용 가능
- ✅ 라즈베리파이에서 사용 가능
- ✅ **더 빠른 학습 속도** ⚡

## 💡 Learning Rate Scheduling의 효과

### 학습 속도 향상
- **초기 탐색 속도**: v3 대비 약 1.5-2배 빠름
- **수렴 속도**: v3 대비 약 1.3-1.5배 빠름
- **전체 학습 시간**: v3 대비 약 20-30% 단축

### 성능 향상
- **최종 성공률**: v3 대비 약 5-10% 향상
- **안정성**: 더 안정적인 학습
- **일반화**: 동일한 일반화 능력 유지

## ⚙️ 설정 변경

`config.py` 파일에서 설정을 변경할 수 있습니다:

```python
# 에피소드 수
NUM_EPISODES = 3000  # 더 많이 학습하려면 증가

# 맵 개수
NUM_MAPS = 20  # 더 많은 맵으로 학습하려면 증가

# 속도
CURRENT_SPEED = SPEED_NORMAL  # SPEED_FAST로 변경하면 빠르게
```

`agent.py`에서 Learning Rate Scheduling 설정 변경:

```python
# agent.py의 __init__에서
self.initial_lr = 0.001      # 초기 학습률 (큰 보폭)
self.min_lr = 0.0001         # 최소 학습률 (작은 보폭)

# update_learning_rate에서
decay_rate = 0.998           # 감소율 (더 빠르게 감소하려면 0.995 등)
```

## 📚 학습 내용

이 버전을 통해 다음을 학습할 수 있습니다:

- Learning Rate Scheduling의 효과
- 동적 학습률 조정 방법
- 빠른 초기 탐색과 정밀한 후기 학습의 균형
- 학습 속도 향상 기법

## 🎓 다음 단계

이 버전으로 학습한 모델은:

1. **더 빠르게 학습**
2. **더 나은 성능**
3. **새로운 맵에서도 작동**
4. **실제 환경에 적용 가능**

---

## 🔮 다음 단계 개선 사항

이 버전의 개선 가능한 사항들:

### 📋 현재 알려진 한계점 및 개선 가능 사항

1. **Learning Rate 업데이트 위치**
   - 현재: 매 스텝마다 호출 (비효율적)
   - 개선: 에피소드마다만 호출하도록 최적화

2. **학습률 스케줄링 방식**
   - 현재: Exponential Decay만 사용
   - 개선: ReduceLROnPlateau (손실 기반), Cosine Annealing 등 추가 옵션

3. **학습률 변화 그래프**
   - 현재: README에 언급되어 있지만 실제 구현되지 않음
   - 개선: `plot_results()`에 학습률 변화 그래프 추가

4. **설정 파일 통합**
   - 현재: Learning Rate Scheduling 설정이 하드코딩됨
   - 개선: `config.py`에 Learning Rate Scheduling 설정 추가

5. **모델 저장/로드**
   - 현재: 학습률이 저장/로드되지 않음
   - 개선: 모델 저장 시 학습률도 함께 저장

6. **Warm-up 단계**
   - 현재: 없음
   - 개선: 초기 몇 에피소드 동안 학습률 점진적 증가 (Warm-up)

7. **손실 기반 조정**
   - 현재: 에피소드 기반만 사용
   - 개선: 손실이 개선되지 않을 때 학습률 감소 (ReduceLROnPlateau)

### 💡 향후 개선 방향

- **다양한 스케줄링 방식**: Exponential, Cosine, Step Decay, ReduceLROnPlateau
- **하이퍼파라미터 자동 튜닝**: 최적의 학습률 스케줄 자동 탐색
- **성능 모니터링**: 학습률과 성능의 상관관계 분석
- **조기 종료**: 성능이 개선되지 않으면 조기 종료

---

**✅ v4 버전**: Learning Rate Scheduling을 적용하여 학습 속도를 향상시킨 버전입니다!

**핵심 개선사항:**
- 🚀 **큰 보폭으로 빠른 초기 탐색**
- 🎯 **작은 보폭으로 정밀한 후기 학습**
- ⚡ **학습 속도 향상** (v3 대비 약 20-30% 단축)


---

### SIMULATOR-V4 주요 특징


**학습 속도 향상 버전** ⚡

- ✅ v3의 모든 기능 포함
- ✅ Learning Rate Scheduling 추가 → 학습 속도 향상
- ✅ 동적 학습률 조정 (큰 보폭 → 작은 보폭)
- ✅ v3 대비 20-30% 학습 시간 단축

**핵심 개선:**
- 처음에는 큰 보폭으로 빠르게 탐색
- 최적의 위치를 찾으면 작은 보폭으로 정밀하게 학습
- 더 빠르고 정확한 학습 가능

---


## v5: Policy > Cache (학습 철학 이해)


## 📋 프로젝트 설명

이 버전은 **캐싱 시스템**을 추가하여 **강화학습의 본질적 목적**을 이해하는 교육용 버전입니다.

**핵심 철학:**
- 학습의 목적은 '최적 경로'가 아니라 '환경 적응력'
- 캐싱은 학습이 아닌 실행 최적화
- Policy가 항상 최종 판단자

## 🧠 v5의 핵심 개념

### 1. 학습의 진짜 목적

#### ❌ 흔한 오해

많은 사람들이 생각하는 강화학습의 목적:
- 특정 맵의 최단 경로 찾기
- 한 번에 완벽한 경로 계산
- A* 알고리즘처럼 경로 계획

#### ⭕ 실제 목적

강화학습의 진짜 목적:
> **처음 보는 환경에서도**
> - 벽에 부딪히지 않고
> - 막힌 공간에서 빠져나오며
> - 목표가 바뀌어도
> - 의미 있게 움직일 수 있는 **범용 행동 정책(policy)을 학습하는 것**

👉 **경로는 학습 목표가 아니라, 실행 결과(output)**

### 2. 왜 학습된 상태에서 더 빨리 도달하는가?

#### 중요한 결론

> **학습된 정책은 물리적으로 더 빠르지 않다.
> 대신 '헤매는 시간'을 제거해서 더 빨리 도달한다.**

**학습되지 않은 상태:**
- 벽에 자주 충돌 (시간 낭비)
- 막다른 길에서 오래 정지 (시간 낭비)
- 의미 없는 왕복 이동 (시간 낭비)
- 탐색(exploration) 비중 큼

→ **도달 시간 증가의 원인은 '실수'**

**학습된 상태:**
- 벽 근처에서 사전 회피
- 막히면 즉시 방향 전환
- 목표 방향으로 일관된 이동
- 불필요한 탐색 제거

→ **실수를 안 해서 도달 시간이 감소**

#### 핵심 오해 정리

- ❌ 최적 경로를 알아서 빠른 것
- ⭕ 잘못된 행동을 안 해서 빠른 것

**수식적 표현:**

```
도달 시간 = 실제 이동 시간 + 손실 시간(실수)
```

학습은 **손실 시간을 줄이는 역할**을 합니다.

### 2-1. v5까지 이미 해결된 문제

v5 정책은 다음을 이미 만족합니다:

- ✅ 새 맵에서도 즉시 실행 가능
- ✅ 충돌 회피 가능
- ✅ 랜덤 탐색이 아닌 방향성 있는 이동
- ✅ 환경 구조가 달라도 기본 생존 가능

> **"어떻게 움직이면 부딪히지 않는가"는 이미 해결된 상태**

### 3. 캐싱이란?

새 맵에서 여러 번 실행하면 경로를 기억합니다.
이는 **학습(training)이 아니라 실행 최적화(execution optimization)** 입니다.

#### 캐싱 vs 학습

| 구분 | 학습 (Training) | 캐싱 (Caching) |
|------|----------------|---------------|
| 신경망 변경 | ✅ 파라미터 업데이트 | ❌ 파라미터 변경 없음 |
| 범위 | 모든 맵에 적용 | 특정 맵만 |
| 동적 환경 | ✅ 대응 가능 | ❌ 캐시 무효화 |
| 일반화 | ✅ 가능 | ❌ 불가능 |

> **뇌가 바뀐 게 아니라, 경험을 메모한 것**

### 4. Policy > Cache 원칙

#### 핵심 원칙

> **Policy > Cache**
> - 정책(policy)은 항상 최종 판단자
> - 캐시는 "힌트 / 가이드" 역할만 수행

#### 동작 방식

```python
1. Policy 네트워크가 행동 예측 (항상 실행!)
2. 캐시에서 힌트 가져오기
3. Policy와 캐시가 일치하면 신뢰도 ↑
4. Policy와 캐시가 충돌하면 Policy 우선!
```

#### 왜 Policy 우선인가?

- 환경이 동적으로 변할 수 있음
- 새로운 장애물이 나타날 수 있음
- 캐시된 경로가 더 이상 유효하지 않을 수 있음

→ **Policy만이 현재 환경을 정확히 판단 가능**

### 5. 순수 캐싱의 치명적 한계

> **환경이 동적으로 변하면 캐시된 경로는 즉시 무효화된다**

**예시:**
- 새로운 장애물 등장
- 이동 물체 출현
- 기존 통로 폐쇄

→ 캐시 경로를 그대로 재생하면:
- ❌ 충돌
- ❌ 멈춤
- ❌ 비정상 행동 발생

### 6. 해결 원칙: 캐싱의 지위 낮추기

#### 핵심 원칙

> **Policy > Cache**
> - 정책(policy)은 항상 최종 판단자
> - 캐시는 "힌트 / 가이드" 역할만 수행

#### ❌ 잘못된 구조

```
캐시 우선 시스템:
1. 캐시에서 행동 가져오기
2. 캐시 있으면 → 그대로 실행
3. 캐시 없으면 → Policy 사용

문제점:
- 환경이 변하면 즉시 실패
- 캐시가 Policy를 무시함
- 동적 환경 대응 불가
```

#### ✅ 올바른 구조 (v5 구현)

```
Policy 우선 시스템:
1. Policy 네트워크가 현재 상태 판단 (항상 실행!)
2. 캐시에서 힌트 확인 (선택적)
3. Policy와 캐시 비교
4. 충돌 시 → Policy 우선!

장점:
- 환경 변화에 즉시 대응
- Policy가 항상 최종 판단
- 동적 환경에서도 안정적
```

## ✨ 주요 특징

### 1. 캐싱 시스템 (NEW!) ⭐

**핵심 아이디어: 경험을 메모하되, 정책이 최종 판단!**

#### 동작 원리:
1. 상태-행동 쌍을 캐시에 저장
2. 캐시는 힌트만 제공
3. Policy 네트워크가 최종 판단
4. Policy와 캐시 충돌 시 Policy 우선

#### 구현 방식:
```python
class ActionCache:
    # 상태 -> 행동 매핑 저장
    # 신뢰도(confidence) 추적
    # Policy와의 일치/충돌 통계
```

#### 캐시 통계:
```
Cache Size: 캐시에 저장된 상태 수
Cache Hits: 캐시가 사용된 횟수
Agreements: Policy와 캐시가 일치한 횟수
Conflicts: Policy와 캐시가 충돌한 횟수 (Policy 우선)
```

### 2. 동적 장애물 시스템 (NEW!) ⚡

**핵심 목적: Policy > Cache 원칙을 실전으로 증명!**

#### 해결 원칙: 캐싱의 지위 낮추기

동적 장애물은 **환경이 실시간으로 변하는 상황**을 시뮬레이션합니다.

**❌ 캐시의 치명적 한계:**
- 캐시된 경로는 정적 환경 기준
- 새로운 장애물이 생기면 캐시 무효화
- 캐시만 믿으면 충돌 발생

**✅ Policy가 필수인 이유:**
- Policy는 현재 상태를 실시간 판단
- 환경 변화에 즉시 대응
- 캐시는 참고만, Policy가 최종 결정

#### 동작 방식:
```python
# 환경이 동적으로 변함
env.update_dynamic_obstacles()  # 50스텝마다 새 장애물 생성

# Policy > Cache: 항상 Policy가 최종 판단
action = agent.select_action(state)  # Policy가 현재 환경 판단

# 캐시는 힌트만 제공, Policy가 최종 결정
if cache_action != policy_action:
    return policy_action  # Policy 우선!
```

#### 동적 장애물 특징:
- 🔴 **50스텝마다 랜덤 위치에 장애물 생성** (오렌지색)
- ⏰ **30스텝 후 자동 소멸**
- 🚨 **캐시된 경로 무효화**
- ✅ **Policy가 실시간 회피**

이를 통해 **Policy > Cache 원칙**이 단순한 이론이 아니라 **실전에서 필수**임을 증명합니다!

### 3. v4의 모든 기능 포함 ✅

- ✅ Learning Rate Scheduling
- ✅ 여러 맵에서 학습 (20개 이상)
- ✅ 랜덤 시작점/목적지
- ✅ 랜덤 초기 방향
- ✅ 느린 Epsilon Decay (0.998)
- ✅ Gradient Clipping
- ✅ 경로 다양성 보상
- ✅ 테스트셋 분리

## 🚀 사용 방법

### 1. 패키지 설치

```bash
python -m pip install pygame-ce numpy torch matplotlib
```

### 2. 훈련 시작

```bash
cd simulator-v5
python train.py
```

**특징:**
- 기본적으로 캐싱 비활성화 (학습 목적)
- `config.py`에서 `USE_CACHE = True`로 변경하여 활성화 가능

### 3. 테스트

```bash
python main.py
```

**특징:**
- 훈련에 사용하지 않은 새로운 맵에서 테스트
- 캐싱 활성화 시 반복 실행하면 경로 최적화
- 일반화 능력 검증

## 📊 v5에서 배우는 핵심 개념

### 1. 학습의 목적

#### ❌ 잘못된 이해
- 학습 목적: 최적 경로 찾기
- 더 빠른 이유: 최적 경로를 알아서

#### ⭕ 올바른 이해
- 학습 목적: 환경 적응력 획득
- 더 빠른 이유: 실수를 안 해서

### 2. 캐싱의 역할

#### ❌ 잘못된 이해
- 캐싱 = 학습
- 캐시 사용 = 더 똑똑해짐

#### ⭕ 올바른 이해
- 캐싱 = 실행 최적화
- 캐시 사용 = 경험 재사용 (뇌는 그대로)

### 3. 최종 판단권

#### ❌ 잘못된 구조
```
캐시 > Policy (캐시 우선)
→ 환경이 변하면 실패
```

#### ⭕ 올바른 구조
```
Policy > Cache (Policy 우선)
→ 환경이 변해도 대응 가능
```

## 🔄 버전 비교

| 항목 | v1 | v2 | v3 | v4 | v5 |
|------|----|----|----|----|---|
| 초기 방향 | 고정 | 랜덤 | 랜덤 | 랜덤 | 랜덤 |
| Epsilon decay | 0.995 | 0.998 | 0.998 | 0.998 | 0.998 |
| Gradient clipping | 없음 | 있음 | 있음 | 있음 | 있음 |
| 경로 다양성 보상 | 없음 | 있음 | 있음 | 있음 | 있음 |
| 맵 개수 | 1개 | 1개 | 20개 이상 | 20개 이상 | 20개 이상 |
| 시작점/목적지 | 고정 | 고정 | 랜덤 | 랜덤 | 랜덤 |
| 테스트셋 분리 | 없음 | 없음 | ✅ | ✅ | ✅ |
| Learning Rate Scheduling | 없음 | 없음 | 없음 | ✅ | ✅ |
| **캐싱 시스템 (Policy > Cache)** | 없음 | 없음 | 없음 | 없음 | ✅ |
| **학습 철학 이해** | 없음 | 없음 | 없음 | 없음 | ✅ |
| 학습 속도 | 느림 | 느림 | 보통 | ⚡ 빠름 | ⚡ 빠름 |

## 💡 캐싱 시스템의 효과

### 언제 유용한가?

- 같은 맵에서 반복 실행할 때
- 테스트/데모 시나리오
- 실행 시간 최적화가 필요할 때

### 언제 사용하지 말아야 하는가?

- 훈련 중 (일반화 방해)
- 동적 환경 (캐시 무효화)
- 새로운 맵 (캐시 없음)

## 📚 학습 내용

이 버전을 통해 다음을 학습할 수 있습니다:

- 강화학습의 진짜 목적 이해
- 학습 vs 캐싱의 차이
- Policy > Cache 원칙
- 동적 환경 대응
- 실행 최적화 기법

## 🎓 핵심 원칙 요약

### 1. 학습의 목적
- ❌ 최적 경로 찾기
- ⭕ 환경 적응력 획득

### 2. 도달 속도
- ❌ 물리적으로 빠른 것
- ⭕ 실수를 안 해서 빠른 것

### 3. 캐싱의 역할
- ❌ 학습 (Training)
- ⭕ 실행 최적화 (Execution Optimization)

### 4. 최종 판단
- ❌ 캐시 우선
- ⭕ Policy 우선 (Policy > Cache)

### 5. 동적 환경
- ❌ 캐시만 사용
- ⭕ Policy가 실시간 판단

---

### SIMULATOR-V5 주요 특징


**Policy > Cache 버전** 🧠

- ✅ v4의 모든 기능 포함
- ✅ 캐싱 시스템 추가 (Policy > Cache 원칙)
- ✅ 강화학습 철학 이해 교육
- ✅ 학습 vs 캐싱 구분

**핵심 메시지:**
- 학습의 목적은 '최적 경로'가 아니라 '환경 적응력'
- 더 빠른 도달은 '잘못된 행동을 제거한 결과'
- 캐싱은 학습이 아니라 실행 최적화
- Policy > Cache: 정책이 항상 최종 판단자

---


## 버전별 비교

| 항목 | v1 | v2 | v3 | v4 | v5 |
|------|----|----|----|----|-----|
| **초기 방향** | 고정 | 랜덤 | 랜덤 | 랜덤 | 랜덤 |
| **Epsilon Decay** | 0.995 (빠름) | 0.998 (느림) | 0.998 | 0.998 | 0.998 |
| **Gradient Clipping** | ❌ | ✅ | ✅ | ✅ | ✅ |
| **경로 다양성 보상** | ❌ | ✅ | ✅ | ✅ | ✅ |
| **맵 개수** | 1개 | 1개 | 20개 이상 | 20개 이상 | 20개 이상 |
| **시작점/목적지** | 고정 | 고정 | 랜덤 | 랜덤 | 랜덤 |
| **테스트셋 분리** | ❌ | ❌ | ✅ | ✅ | ✅ |
| **Learning Rate Scheduling** | ❌ | ❌ | ❌ | ✅ | ✅ |
| **캐싱 시스템** | ❌ | ❌ | ❌ | ❌ | ✅ (Policy > Cache) |
| **동적 환경 대응** | ❌ | ❌ | ❌ | ❌ | ✅ |
| **일반화** | 불가능 | 제한적 | 완전 | 완전 | 완전 |
| **학습 속도** | 느림 | 느림 | 보통 | ⚡ 빠름 | ⚡ 빠름 |

---

## 실전 적용 가이드

### 🚀 빠른 시작

```bash
# 1. 패키지 설치
python -m pip install -r requirements.txt

# 2. 추천: v3 실행 (완전한 일반화)
cd simulator-v3
python train.py

# 3. 테스트
python main.py
```

### 📊 학습 시간 예상

- **v1, v2**: ~5-10분 (500 에피소드)
- **v3**: ~10-30분 (3,000 에피소드)
- **v4**: ~7-20분 (3,000 에피소드) ⚡

> 💡 **팁**: `config.py`에서 `SHOW_TRAINING = False`로 설정하면 화면 없이 빠르게 학습할 수 있습니다.

### 🎯 라즈베리파이에 적용하기

1. **모델 학습 완료** 후 `model_final.pth` 저장
2. **라즈베리파이에 모델 파일 전송**
3. **센서 데이터를 state로 변환**
4. **학습된 모델로 추론 실행**

### 🔮 다음 단계

- CNN을 사용한 이미지 기반 상태 표현
- 실제 카메라 데이터 활용
- 실시간 경로 계획
- 장애물 회피 알고리즘 개선
- 멀티 에이전트 강화학습

---

## 📖 참고 자료

- [README.md](../README.md) - 프로젝트 전체 설명
- [Gradient_용어정리.md](../Gradient_용어정리.md) - Gradient 관련 용어 상세 설명

---

## 🎓 학습 팁

### 초보자를 위한 학습 순서

1. **v1 실행** → 과적합 문제를 직접 확인해보세요
2. **v2 실행** → 해결 방법이 어떻게 작동하는지 관찰하세요
3. **v3 실행** → 완전한 일반화가 무엇인지 이해하세요
4. **v4 실행** → 학습 속도 향상 기법을 체험하세요
5. **v5 실행** → 강화학습의 본질과 철학을 이해하세요 🧠

### 코드 이해하기

각 버전의 코드를 비교해보면:
- 어떤 부분이 변경되었는지
- 왜 그렇게 변경했는지
- 어떤 효과가 있었는지

를 명확히 알 수 있습니다.

---

*이 책은 자동 생성되었습니다. 마지막 업데이트: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
