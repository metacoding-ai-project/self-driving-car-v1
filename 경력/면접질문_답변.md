# 자율주행 RC카 프로젝트 면접 질문 & 답변

## 📋 목차
1. [기술 개념 질문](#1-기술-개념-질문)
2. [구현 및 설계 질문](#2-구현-및-설계-질문)
3. [트러블슈팅 질문](#3-트러블슈팅-질문)
4. [프로젝트 관리 질문](#4-프로젝트-관리-질문)
5. [하드웨어 질문](#5-하드웨어-질문)
6. [성장 및 학습 질문](#6-성장-및-학습-질문)

---

## 1. 기술 개념 질문

### Q1. DQN이 뭐에요? 간단하게 설명해주세요.

**답변:**
DQN은 Deep Q-Network의 약자로, 강화학습과 딥러닝을 결합한 알고리즘입니다.

기존 Q-learning은 상태-행동 쌍마다 Q-value를 테이블에 저장하는데, 상태 공간이 커지면 메모리 문제가 생깁니다. DQN은 이 Q-테이블을 신경망으로 대체해서, 상태를 입력으로 받아 각 행동의 가치를 출력합니다.

제 프로젝트에서는 8차원 상태 벡터(초음파 센서 거리 4개 + 위험 플래그 4개)를 입력받아, 3가지 행동(직진, 좌회전, 우회전)의 Q-value를 출력하는 구조로 만들었습니다.

DQN의 핵심 기술 2가지는:
1. **Experience Replay**: 과거 경험을 메모리에 저장하고 랜덤하게 샘플링해서 학습 - 데이터 간 상관관계 제거
2. **Target Network**: 고정된 타겟 네트워크를 사용해 학습 안정화 - 목표값이 계속 변하는 문제 해결

---

### Q2. 강화학습이 뭐라고 생각하세요? 지도학습과 차이점은?

**답변:**
강화학습은 에이전트가 환경과 상호작용하면서 시행착오를 통해 학습하는 방식입니다. 마치 게임을 처음 하는 사람이 여러 번 죽어가면서 클리어하는 법을 배우는 것과 같습니다.

**지도학습과의 핵심 차이:**

| 구분 | 지도학습 | 강화학습 |
|------|---------|---------|
| 데이터 | 정답이 있는 라벨링된 데이터 | 보상(reward)만 존재 |
| 학습 방식 | 정답과 예측의 차이를 최소화 | 누적 보상을 최대화 |
| 피드백 | 즉각적인 정답 제공 | 지연된 보상 (나중에 결과 확인) |

제 프로젝트의 예를 들면:
- **지도학습이었다면**: "이 상황에서는 좌회전이 정답이야"라고 모든 상황에 라벨링 필요
- **강화학습으로 해결**: RC카가 직접 움직이면서 충돌하면 -100점, 안전하게 가면 +1점을 받으며 스스로 학습

자율주행처럼 정답을 정의하기 어렵거나, 환경이 계속 변하는 경우 강화학습이 더 적합합니다.

---

### Q3. Q-learning과 DQN의 차이가 뭔가요?

**답변:**
Q-learning은 테이블 기반, DQN은 신경망 기반이라는 점이 가장 큰 차이입니다.

**Q-learning (전통적 방식):**
```
상태: (x=3, y=5, 방향=북)
Q-table[3][5][북][직진] = 0.8
Q-table[3][5][북][좌회전] = 0.3
```
- 모든 상태-행동 조합을 테이블에 저장
- 문제: 상태가 조금만 복잡해져도 메모리 폭발

**DQN (딥러닝 방식):**
```
입력: [전방거리, 후방거리, 좌측거리, 우측거리, ...]
신경망 처리
출력: [Q(직진)=0.8, Q(좌회전)=0.3, Q(우회전)=0.5]
```
- 신경망이 상태를 입력받아 Q-value 예측
- 장점: 본 적 없는 상태도 일반화 가능, 연속적인 상태 공간 처리 가능

제 프로젝트에서 v1은 간단한 Q-learning으로 시작했지만, 격자 크기를 20x20에서 50x50으로 늘리니 메모리 문제가 발생해서 v3부터 DQN으로 전환했습니다.

---

### Q4. Exploration vs Exploitation이 뭐고, 어떻게 조절했나요?

**답변:**
강화학습의 핵심 딜레마입니다.

- **Exploitation (활용)**: 지금까지 알아낸 최선의 행동을 선택 (안전하지만 새로운 발견 없음)
- **Exploration (탐험)**: 랜덤하게 새로운 행동 시도 (위험하지만 더 좋은 방법 발견 가능)

**제 프로젝트에서의 구현:**
```python
# Epsilon-Greedy 전략 사용
if random.random() < epsilon:
    action = random.choice([0, 1, 2])  # 탐험 (랜덤 행동)
else:
    action = model.predict(state).argmax()  # 활용 (최선의 행동)
```

**Epsilon 감소 전략:**
- 초기 학습: epsilon = 1.0 (100% 랜덤 탐험)
- 중간: epsilon = 0.5 (50% 탐험)
- 후기: epsilon = 0.01 (거의 활용만)

학습 초반에는 많이 탐험해서 다양한 경험을 쌓고, 후반에는 학습한 내용을 활용하는 방식입니다.

**실제 RC카 테스트 시:**
```python
agent.epsilon = 0  # 100% 활용 (탐험 안함)
```
실제 하드웨어에서는 충돌 위험이 있으므로 탐험 비활성화했습니다.

---

### Q5. 왜 Vision(카메라)을 안 쓰고 초음파 센서만 썼나요?

**답변:**
비용과 교육 목적 때문입니다.

**비용 문제:**
- AWS Rekognition: 실시간 스트리밍 시 월 $1,000 이상
- 라즈베리파이 온디바이스 비전: 가능하지만 추가 개발 필요
- 초음파 센서: 개당 $2, 4개 사용해도 $8

**교육 목적:**
교육용 제품이다 보니 학생들이 이해하기 쉬운 센서가 필요했습니다.
- 초음파: "거리가 10cm면 가까운 거야" - 직관적
- 카메라: "이미지에서 특징 추출해서 CNN으로..." - 복잡함

**실용적 제약:**
- 라즈베리파이 4에서 실시간 비전 처리는 FPS 제한 있음
- YOLO, MobileNet 같은 경량 모델도 추론 시간 100ms+
- 초음파는 45ms로 더 빠름

**하지만 한계도 인지:**
- 투명 물체 감지 불가 (유리판)
- 좁은 빔각 (15도) - 얇은 막대 놓치기 쉬움
- 표면 각도에 따라 반사 실패

향후 개선안으로 저해상도 카메라(64x64) + TFLite MobileNet 조합을 고려 중입니다. 비용 $15 추가로 제한적인 객체 인식 가능합니다.

---

### Q6. 보상(Reward) 설계를 어떻게 했나요?

**답변:**
보상 설계가 강화학습에서 가장 중요하면서도 어려운 부분이었습니다.

**초기 설계 (단순):**
```python
if collision:
    reward = -100  # 충돌 시 큰 패널티
elif reached_goal:
    reward = +100  # 목적지 도달 시 큰 보상
else:
    reward = -1  # 시간 페널티
```

**문제점:**
RC카가 제자리에서 빙글빙글 돌거나, 벽에서 멀리 떨어져서 너무 안전하게만 움직임

**개선된 설계:**
```python
# 1. 충돌 페널티
if collision:
    reward = -100

# 2. 목적지까지의 거리 기반 보상
distance_to_goal = calculate_distance(current_pos, goal_pos)
prev_distance = calculate_distance(prev_pos, goal_pos)
reward = (prev_distance - distance_to_goal) * 10  # 가까워지면 양수

# 3. 전방 거리 유지 보상 (너무 가깝지도, 멀지도 않게)
if 20 < front_distance < 100:
    reward += 1  # 적절한 거리 유지
elif front_distance < 10:
    reward -= 10  # 위험하게 가까움

# 4. 시간 효율성
reward -= 0.1  # 매 스텝마다 작은 패널티 (빨리 도착하도록)
```

**Reward Shaping의 원칙:**
1. **명확한 목표 제공**: 충돌 회피 + 목적지 도달
2. **중간 보상**: 목적지로 가까워질 때마다 작은 보상 (학습 신호 제공)
3. **적절한 균형**: 너무 보수적이지도, 무모하지도 않게

**실제 튜닝 과정:**
초기 -100/+100만 있을 때는 2000 에피소드 학습 후에도 성공률 30%였는데,
거리 기반 보상 추가 후 500 에피소드만에 80% 달성했습니다.

---

### Q7. CNN이 뭐고, 이 프로젝트에서 왜 안 썼나요?

**답변:**
CNN(Convolutional Neural Network)은 **합성곱 신경망**으로, 이미지나 공간 데이터 처리에 특화된 딥러닝 모델입니다.

**CNN의 핵심 개념:**
```
입력 이미지 (예: 카메라 영상)
    ↓
Convolution Layer (특징 추출: 엣지, 모서리, 패턴)
    ↓
Pooling Layer (차원 축소)
    ↓
Fully Connected Layer (분류/예측)
    ↓
출력 (예: "사람", "차", "장애물")
```

**동작 원리:**
- **Convolution(합성곱)**: 작은 필터(3x3, 5x5)를 이미지 위에 슬라이딩하며 특징 추출
- **Pooling**: 중요한 특징만 남기고 크기 축소 (예: Max Pooling)
- **장점**: 이미지의 공간적 구조 보존, 파라미터 수 감소

**왜 이 프로젝트에서 안 썼나요?**

**1. 입력 데이터가 이미지가 아님**
```
[내 프로젝트]
입력: 초음파 센서 거리 4개 → [85cm, 120cm, 45cm, 90cm]
→ 1차원 벡터 (8개 값)
→ CNN 불필요, Fully Connected Network로 충분

[만약 카메라 사용했다면]
입력: 카메라 이미지 → 640x480 RGB
→ 3차원 텐서 (높이 x 넓이 x 채널)
→ CNN 필요
```

**2. 비용 문제**
- 카메라: $15~30 추가
- CNN 추론: 라즈베리파이에서 100ms+ (실시간 제어 어려움)
- 초음파: $8, 추론 45ms (충분히 빠름)

**3. 교육 목적**
- 초음파 센서: 학생들이 이해하기 쉬움 ("거리만 측정")
- CNN: 복잡한 개념 (합성곱, 특징맵 등) - 교육용으로 과함

**CNN을 사용하는 경우:**
```python
# 자율주행에서 카메라 + CNN 사용 예시
import torch.nn as nn

class CNNDriver(nn.Module):
    def __init__(self):
        super().__init__()
        # 합성곱 레이어
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)  # RGB → 32 채널
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)
        self.pool = nn.MaxPool2d(2, 2)

        # 완전연결 레이어
        self.fc1 = nn.Linear(64 * 28 * 28, 128)
        self.fc2 = nn.Linear(128, 3)  # 3가지 행동

    def forward(self, image):
        x = self.pool(F.relu(self.conv1(image)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64 * 28 * 28)
        x = F.relu(self.fc1(x))
        return self.fc2(x)
```

**향후 개선안:**
만약 예산이 있다면:
- 저해상도 카메라 (64x64) + 경량 CNN (MobileNet)
- 초음파와 병행 (Sensor Fusion)
- 투명 물체, 얇은 막대 같은 초음파 약점 보완

---

### Q8. PPO가 뭐고, DQN과 어떤 차이가 있나요?

**답변:**
PPO(Proximal Policy Optimization)는 **정책 경사(Policy Gradient)** 기반의 강화학습 알고리즘으로, DQN과는 다른 접근 방식입니다.

**핵심 차이:**

| 구분 | DQN (Value-based) | PPO (Policy-based) |
|------|-------------------|-------------------|
| 학습 대상 | Q-value (가치 함수) | Policy (정책 직접) |
| 출력 | 각 행동의 Q-value | 각 행동의 확률 |
| 행동 선택 | argmax(Q-value) | 확률적 샘플링 |
| 탐험 방식 | Epsilon-greedy | 확률 분포 자체에 포함 |
| 안정성 | Experience Replay로 안정화 | Clipping으로 안정화 |

**DQN 방식:**
```python
# Q-value 예측
Q_values = model(state)  # [2.5, 1.8, 3.2]
action = argmax(Q_values)  # 2번 행동 선택 (결정적)
```

**PPO 방식:**
```python
# 정책(확률) 예측
policy = model(state)  # [0.2, 0.3, 0.5] (확률 합=1)
action = sample(policy)  # 확률에 따라 샘플링 (확률적)
```

**PPO의 핵심: Clipping**
```python
# 정책을 너무 크게 변경하지 않도록 제한
ratio = new_policy / old_policy
clipped_ratio = clip(ratio, 1-epsilon, 1+epsilon)  # 0.8 ~ 1.2 범위 제한
loss = min(ratio * advantage, clipped_ratio * advantage)
```
→ 학습이 안정적이지만 느림

**왜 DQN을 선택했나요?**

**1. 이산 행동 공간 (Discrete Action)**
```
내 프로젝트: 3가지 행동만 (직진, 좌회전, 우회전)
→ DQN이 더 효율적

연속 행동 공간 (예: 조향각 -45° ~ +45°):
→ PPO가 더 적합
```

**2. 구현 복잡도**
- DQN: 구현이 상대적으로 단순 (200줄)
- PPO: Actor-Critic 구조, Advantage 계산 등 복잡 (500줄+)

**3. 샘플 효율성**
- DQN: Experience Replay로 데이터 재사용 → 샘플 효율적
- PPO: On-policy (현재 정책으로만 학습) → 더 많은 데이터 필요

**4. 시뮬레이터 제약**
- 시뮬레이터에서 데이터 수집이 제한적
- DQN이 적은 데이터로도 학습 가능

**PPO의 장점 (사용하지 않은 이유와 반대):**
- **연속 제어**: 조향각, 속도를 연속값으로 제어할 때 유리
- **안정성**: 정책이 급격히 나빠지지 않음 (Clipping)
- **최신 알고리즘**: OpenAI가 로봇 제어에 주로 사용

**실제 사용 예시:**
```python
# DQN - 이산 행동
actions = [0, 1, 2]  # 직진, 좌회전, 우회전
q_values = model(state)  # [2.1, 1.5, 3.0]
action = argmax(q_values)  # 2번 (우회전)

# PPO - 연속 행동
steering = model(state)  # 평균: 15°, 표준편차: 5°
action = sample(Normal(15, 5))  # 예: 17.3° (확률적)
```

**만약 다시 한다면?**
- 현재 프로젝트: DQN 유지 (최적 선택)
- 실제 차량 크기 RC카: PPO 고려 (부드러운 조향 제어)
- 복잡한 환경: PPO + CNN 조합

**결론:**
- DQN: 이산 행동, 적은 데이터, 빠른 학습
- PPO: 연속 행동, 안정성 중시, 복잡한 제어

이 프로젝트는 DQN이 더 적합했습니다! ✅

---

### Q9. TFLite가 뭔가요? PyTorch와 차이는?

**답변:**
TFLite(TensorFlow Lite)는 **모바일/임베디드 기기용 경량 추론 엔진**입니다. PyTorch와는 목적이 다릅니다.

**핵심 차이:**

| 구분 | PyTorch | TensorFlow Lite |
|------|---------|----------------|
| 목적 | 학습(Training) + 추론 | 추론(Inference)만 |
| 사용 환경 | 개발 PC, 서버 (GPU) | 모바일, 라즈베리파이 |
| 크기 | 1.2GB+ | 10MB (Runtime만) |
| 속도 | 빠름 (GPU 활용) | 더 빠름 (최적화됨) |
| 메모리 | 850MB | 420MB |
| 유연성 | 높음 (동적 그래프) | 낮음 (고정 그래프) |

**비유:**
```
PyTorch = 건축 설계 사무소
- 건물(모델) 설계하고 건축(학습)하는 곳
- 큰 공간, 많은 장비 필요

TFLite = 건물 관리실
- 완성된 건물을 운영만 함
- 작은 공간, 최소 장비로 충분
```

**TFLite의 특징:**

**1. 경량화 (Lightweight)**
```
[PyTorch 모델]
- 동적 그래프 (유연하지만 오버헤드 큼)
- Python 인터프리터 필요
- 크기: 50MB

[TFLite 모델]
- 정적 그래프 (고정되어 있어 빠름)
- C++ 런타임만 필요
- 크기: 12MB (양자화 후)
```

**2. 최적화 (Optimization)**
```python
# TFLite 변환 시 최적화 적용
converter = tf.lite.TFLiteConverter.from_saved_model("model")

# 양자화: Float32 → Float16
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]

# 연산 융합: Conv + ReLU → 하나로 합침
# 불필요한 연산 제거
tflite_model = converter.convert()
```

**3. 하드웨어 가속**
- GPU Delegate: 모바일 GPU 활용
- NNAPI: Android 신경망 API
- Edge TPU: Google Coral 가속기

**내 프로젝트에서의 사용:**

**워크플로우:**
```
[개발 PC - PyTorch]
1. 모델 설계: nn.Module 정의
2. 학습: optimizer.step() 수천 번 반복
3. 저장: model_final.pth (50MB)
    ↓
4. 변환: PyTorch → ONNX → TFLite
5. 결과: model_final.tflite (12MB)
    ↓ scp 파일 전송
[라즈베리파이 - TFLite]
6. TFLite Runtime 설치 (10MB)
7. 추론만 반복: interpreter.invoke()
8. 속도: 45ms (실시간 제어 가능)
```

**코드 비교:**

**PyTorch (개발 PC에서 학습):**
```python
import torch
import torch.nn as nn

# 모델 정의
model = nn.Sequential(
    nn.Linear(8, 128),
    nn.ReLU(),
    nn.Linear(128, 3)
)

# 학습
optimizer = torch.optim.Adam(model.parameters())
for epoch in range(1000):
    output = model(state)
    loss = criterion(output, target)
    loss.backward()  # 역전파 (학습)
    optimizer.step()

# 저장
torch.save(model, 'model.pth')  # 50MB
```

**TFLite (라즈베리파이에서 추론):**
```python
import tflite_runtime.interpreter as tflite

# 모델 로드 (학습 기능 없음!)
interpreter = tflite.Interpreter(model_path='model.tflite')
interpreter.allocate_tensors()

# 추론만 가능
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

interpreter.set_tensor(input_details[0]['index'], state)
interpreter.invoke()  # 순전파만 (역전파 불가)
output = interpreter.get_tensor(output_details[0]['index'])
```

**왜 변환이 필요한가?**

**시도 1: PyTorch 모델을 라즈베리파이에 그대로 사용**
```
문제:
- PyTorch 설치: 1.2GB (SD 카드 용량 부족)
- 추론 시간: 200ms (너무 느림)
- 메모리: 850MB (다른 프로그램 실행 어려움)
→ 실패!
```

**시도 2: TFLite 변환 후 사용**
```
해결:
- TFLite Runtime: 10MB (120배 작음)
- 추론 시간: 45ms (4.4배 빠름)
- 메모리: 420MB (2배 절약)
→ 성공! ✅
```

**TFLite의 제약사항:**
1. **학습 불가**: 추론만 가능 (가중치 업데이트 안 됨)
2. **고정 입력 크기**: 동적 배치 사이즈 어려움
3. **디버깅 어려움**: 에러 메시지 불친절

**대안들:**
| 프레임워크 | 크기 | 속도 | 호환성 |
|-----------|------|------|--------|
| TFLite | 10MB | ⭐⭐⭐⭐⭐ | TensorFlow 모델 |
| ONNX Runtime | 30MB | ⭐⭐⭐⭐ | 다양한 프레임워크 |
| PyTorch Mobile | 50MB | ⭐⭐⭐ | PyTorch 모델 |
| ncnn | 8MB | ⭐⭐⭐⭐⭐ | 모바일 특화 |

**선택한 이유: TFLite**
- 가장 경량 (10MB)
- 라즈베리파이 공식 지원
- 변환 도구 성숙 (PyTorch → ONNX → TFLite)

**결론:**
- **PyTorch**: 학습용 (개발 PC)
- **TFLite**: 추론용 (라즈베리파이)
- 각자 역할이 다르고, 상호 보완적!

---

## 2. 구현 및 설계 질문

### Q10. 왜 라즈베리파이를 선택했나요? 아두이노는 왜 안 됐나요?

**답변:**
PyTorch 모델을 실행해야 했기 때문입니다.

**아두이노의 한계:**
- RAM: 2KB (Arduino Uno) - PyTorch 모델 로드 자체가 불가능
- CPU: 16MHz - 딥러닝 추론은 꿈도 못 꿈
- 언어: C/C++ - Python 생태계 사용 불가

**라즈베리파이의 장점:**
- RAM: 4GB - PyTorch 모델 여유롭게 로드
- CPU: 1.5GHz Quad-core - 추론 시간 45ms 달성
- OS: Linux - Python, PyTorch 지원
- GPIO: 40핀 - 센서/모터 제어 가능

**고려했던 대안:**
1. **Jetson Nano** ($99): GPU 있어서 더 빠르지만 교육용으로는 과함
2. **ESP32** ($5): WiFi 내장이지만 RAM 520KB로 부족
3. **STM32 + 클라우드**: 센서 데이터만 전송 → 하지만 실시간 제어 지연 문제

**최종 결정:**
라즈베리파이 4 (4GB)가 가격($55) 대비 성능이 가장 적절했고, 교육 자료도 풍부해서 학습 목적에 적합했습니다.

---

### Q11. 시뮬레이터를 왜 5번이나 만들었나요? 처음부터 완벽하게 못 만들었나요?

**답변:**
처음부터 완벽한 설계는 불가능했고, 점진적 개선이 더 효율적이었습니다.

**버전별 진화 과정:**

**v1 (기본 검증):**
- 목적: 강화학습 개념 검증 - "과연 학습이 될까?"
- 알고리즘: Q-learning (테이블)
- 환경: 10x10 격자, 장애물 5개
- 결과: 작동은 하는데 느리고 확장성 없음

**v2 (센서 추가):**
- 목적: 초음파 센서 시뮬레이션 추가
- 상태 공간: 4방향 거리 측정 추가
- 문제: 상태 공간 폭발 (10^8개 이상)

**v3 (DQN 도입):**
- 목적: 신경망으로 상태 공간 문제 해결
- 알고리즘: DQN (딥러닝)
- 결과: 일반화 능력 향상, 맵 크기 50x50으로 확장 가능

**v4 (학습 안정화):**
- 목적: 학습 속도 개선
- 추가: Experience Replay, Target Network
- 결과: 학습 시간 2000 에피소드 → 500 에피소드로 단축

**v5 (실행 최적화):**
- 목적: 실제 RC카 적용 준비
- 추가: Action Cache (실행 시 빠른 결정)
- 동적 장애물 지원
- 결과: 추론 시간 단축, 실제 환경 대비 강건성 향상

**왜 이렇게 했나:**
각 버전마다 명확한 목표를 설정하고, 작동하는 상태를 유지하면서 점진적으로 개선했습니다.
한 번에 모든 걸 넣으면 어디서 문제가 생기는지 파악이 어렵습니다.

이런 반복적 개선 방식(Iterative Development)이 실무에서도 리스크 관리에 효과적이라고 생각합니다.

---

### Q12. ActionCache가 뭐고, 왜 만들었나요? 학습과는 다른 건가요?

**답변:**
네, 학습과는 완전히 다릅니다. ActionCache는 "실행 최적화"를 위한 메모리 시스템입니다.

**배경:**
DQN 신경망 추론이 200ms나 걸렸습니다. 실시간 제어에는 너무 느립니다.
목표는 50ms 이내인데, 모델 경량화만으로는 한계가 있었습니다.

**ActionCache의 개념:**
```
"이 상태는 전에도 봤어! 그때 좌회전했는데 잘 됐잖아?"
```
같은 상태를 다시 만나면 신경망을 거치지 않고 캐시에서 바로 행동을 가져오는 방식입니다.

**구현 방식:**
```python
# 상태를 해시 키로 변환
state_key = (round(front, 1), round(back, 1), ...)

# 캐시 확인
if state_key in cache:
    cached_action = cache[state_key]
    if confidence > 0.7:
        return cached_action  # 캐시 사용

# 캐시 미스 시 신경망 사용
action = neural_network.predict(state)

# 성공 시 캐시에 저장
if success:
    cache[state_key] = action
```

**학습 vs 캐시의 차이:**

| 구분 | 학습 (Training) | 캐시 (Cache) |
|------|----------------|-------------|
| 시점 | 훈련 단계 | 실행 단계 |
| 목적 | 신경망 가중치 업데이트 | 추론 속도 향상 |
| 영향 범위 | 모든 상태에 일반화 | 특정 맵/상황에만 적용 |
| 지속성 | 영구적 (모델 파일 저장) | 휘발성 (세션 종료 시 초기화) |

**Policy > Cache 원칙:**
캐시와 신경망이 다른 행동을 추천하면, 항상 신경망을 우선합니다.
캐시는 "힌트"일 뿐, 최종 결정권은 학습된 모델에 있습니다.

**실제 성능 개선:**
- 캐시 미적용: 평균 추론 45ms
- 캐시 적용: 평균 추론 12ms (히트율 67%)
- 동일 맵 반복 시 실시간 제어 가능

---

### Q13. 모델 경량화는 어떻게 했나요? (PyTorch → TFLite)

**답변:**
라즈베리파이에서 실시간 동작을 위해 필수적이었습니다. **중요한 점은 모든 변환 작업은 개발 PC에서 수행하고, 라즈베리파이에는 변환된 TFLite 파일만 배포**한다는 것입니다.

**워크플로우:**
```
[개발 PC - GPU 있는 컴퓨터]
1. PyTorch로 학습 (model_final.pth 생성)
2. 모델 변환 (PyTorch → ONNX → TFLite)
3. 변환된 파일 생성 (model_final.tflite)
    ↓ scp로 파일 전송
[라즈베리파이]
4. TFLite Runtime만 설치 (경량)
5. model_final.tflite로 추론 실행
```

**변환 과정 (개발 PC에서 실행):**
```
PyTorch 모델 (50MB) - 개발 PC에서 학습
    ↓ ONNX 변환 (개발 PC)
ONNX 모델 (48MB)
    ↓ 최적화 + 양자화 (개발 PC)
TFLite 모델 (12MB)
    ↓ 파일 복사
라즈베리파이 (추론 45ms)
```

**1단계: ONNX 변환 (개발 PC에서 실행)**
```python
# convert_model.py - 개발 PC에서 실행!
import torch

# 학습된 모델 로드
model = torch.load('model_final.pth')
model.eval()

# PyTorch → ONNX (호환성 확보)
dummy_input = torch.randn(1, 8)  # 배치 1, 입력 차원 8
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    opset_version=11,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'}}
)
print("✅ ONNX 변환 완료")
```

**2단계: ONNX 최적화 (개발 PC에서 실행)**
```python
# 불필요한 연산 제거, 그래프 간소화
from onnxsim import simplify
import onnx

onnx_model = onnx.load("model.onnx")
simplified_model, check = simplify(onnx_model)
onnx.save(simplified_model, "model_simplified.onnx")
print("✅ 최적화 완료: 연산 노드 127개 → 89개")
```

**3단계: TFLite 변환 + 양자화 (개발 PC에서 실행)**
```python
# ONNX → TensorFlow → TFLite
import onnx_tf
import tensorflow as tf

# ONNX → TensorFlow
onnx_model = onnx.load("model_simplified.onnx")
tf_rep = onnx_tf.backend.prepare(onnx_model)
tf_rep.export_graph("model_tf")

# TensorFlow → TFLite + Float16 양자화
converter = tf.lite.TFLiteConverter.from_saved_model("model_tf")
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]
tflite_model = converter.convert()

with open('model_final.tflite', 'wb') as f:
    f.write(tflite_model)
print("✅ TFLite 변환 완료: model_final.tflite")
```

**4단계: 라즈베리파이로 배포**
```bash
# 개발 PC에서 실행
scp model_final.tflite pi@192.168.1.100:~/rc_car/
scp rc_car_main.py pi@192.168.1.100:~/rc_car/

# 라즈베리파이에 접속
ssh pi@192.168.1.100

# 라즈베리파이에는 TFLite Runtime만 설치 (PyTorch 불필요!)
pip3 install tflite-runtime  # 10MB만 설치

# 실행
cd ~/rc_car
python3 rc_car_main.py
```

**양자화란:**
가중치의 정밀도를 낮추는 것입니다.
- Float32 (32비트): 3.14159265...
- Float16 (16비트): 3.141
- 정확도 손실: 미미 (최종 성능 변화 < 2%)
- 크기 절감: 50%
- 속도 향상: 2배

**최종 결과:**
| 메트릭 | PyTorch (개발 PC) | TFLite (라즈베리파이) |
|--------|---------|--------|
| 모델 크기 | 50MB | 12MB |
| 추론 시간 | 200ms | 45ms |
| 메모리 사용 | 850MB | 420MB |
| 정확도 손실 | - | 1.3% |
| 설치 크기 | PyTorch 1.2GB | TFLite Runtime 10MB |

**라즈베리파이 추론 코드:**
```python
# rc_car_main.py - 라즈베리파이에서 실행
import tflite_runtime.interpreter as tflite
import numpy as np

# TFLite 모델 로드 (PyTorch 불필요!)
interpreter = tflite.Interpreter(model_path="model_final.tflite")
interpreter.allocate_tensors()

# 추론
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

state = np.array([sensor_data], dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], state)
interpreter.invoke()
q_values = interpreter.get_tensor(output_details[0]['index'])
action = np.argmax(q_values)
```

**핵심 포인트:**
- 라즈베리파이에는 **PyTorch 설치 안 함** (1.2GB 절약)
- 변환은 **개발 PC에서 한 번만** 수행
- 라즈베리파이는 **추론만** (TFLite Runtime 10MB)
- 학습은 절대 라즈베리파이에서 하지 않음 (너무 느림)

---

## 3. 트러블슈팅 질문

### Q14. 프로젝트 하면서 가장 머리 아팠던 문제가 뭐였나요?

**답변:**
**초음파 센서 4개의 상호 간섭 문제**가 정말 힘들었습니다.

**문제 상황:**
처음 4개 센서를 달고 테스트했을 때:
```
전방: 85cm → 23cm → 150cm → 42cm (계속 튀는 값)
좌측: 40cm → 300cm → 30cm (불안정)
```
값이 미친 듯이 튀었습니다. RC카는 제자리에서 경련하듯 움직였습니다.

**원인 분석 (3일 소요):**
1. **전기적 간섭?** → 오실로스코프로 확인했지만 신호는 깨끗함
2. **코드 버그?** → 센서 하나만 연결하면 정상 작동
3. **초음파 간섭!** → 4개 동시 측정 시 다른 센서의 초음파를 감지함

**왜 이런 일이?**
```
[전방 센서] ------ 초음파 발사 ------>
                                    |
                                    | 반사
                                    ↓
[좌측 센서] ←----- 엉뚱한 신호 수신
```
HC-SR04 센서는 40kHz 초음파를 사용하는데, 4개 모두 같은 주파수라 서로 구분이 안 됩니다.

**시도한 해결책들:**

**실패한 시도 1: 센서 각도 조정**
- 각 센서를 45도씩 틀어서 배치
- 결과: 여전히 간섭 발생 (초음파 빔각이 15도이지만 측면 반사가 있음)

**실패한 시도 2: 소프트웨어 필터링**
- 이동평균 필터 적용
- 결과: 반응 속도만 느려짐, 근본 해결 안됨

**성공한 해결책: 시분할 측정 + 물리적 차폐**
```python
def get_all_distances(self):
    distances = []
    for direction in ['front', 'back', 'left', 'right']:
        dist = self.sensors[direction].get_distance()
        distances.append(dist)
        time.sleep(0.015)  # 15ms 대기 (핵심!)
    return distances
```
- 각 센서를 순차적으로 측정 (동시 측정 금지)
- 측정 간 15ms 대기 (초음파가 완전히 사라질 때까지)
- 센서 사이에 폼보드로 물리적 차폐벽 설치

**결과:**
- 안정성: 측정값 표준편차 ±2cm 이내
- 반응 속도: 4개 센서 측정에 60ms (15ms x 4) - 허용 범위

**교훈:**
하드웨어 문제는 소프트웨어만으로 해결 안 될 수 있습니다.
물리적 원리를 이해하고 하드웨어+소프트웨어 통합 해결책이 필요합니다.

---

### Q15. 시뮬레이터에서는 잘 되는데 실제 RC카에서 안 될 때 어떻게 디버깅했나요?

**답변:**
이게 정말 큰 문제였습니다. **Sim-to-Real Gap**이라고 부르는 현상입니다.

**문제 상황:**
- 시뮬레이터: 목적지 도달률 90%
- 실제 RC카: 충돌률 70% (거의 반대...)

**원인 분석:**

**1. 센서 노이즈**
```python
# 시뮬레이터: 완벽한 값
distance = calculate_perfect_distance(pos, obstacle)

# 실제: 노이즈 섞인 값
distance = 52, 48, 55, 51, 49...  # ±5cm 오차
```

**2. 행동 지연 (Latency)**
```
[시뮬레이터] 결정 → 즉시 실행 (0ms)
[실제 RC카] 결정 → 모터 신호 → 관성 → 움직임 (약 200ms)
```
이 200ms 동안 RC카는 계속 직진하고 있었습니다!

**3. 물리적 슬립 (미끄러짐)**
시뮬레이터에는 마찰이 없지만, 실제 바닥은 미끄럽습니다.
좌회전 명령 → 실제로는 약간 직진하면서 회전 → 예상 위치와 다름

**해결 과정:**

**Step 1: 데이터 수집 (가시화)**
```python
# 실제 RC카의 센서 데이터를 실시간으로 시뮬레이터에 표시
# Wi-Fi로 데이터 전송
import socket
sock = socket.socket()
while True:
    real_sensor_data = get_sensors()
    sock.send(json.dumps(real_sensor_data))
```
시뮬레이터 화면에서 실제 센서 값을 보며 차이 확인

**Step 2: 센서 노이즈 모델링**
```python
# 시뮬레이터에 노이즈 추가
def get_state_with_noise(self, x, y, direction):
    distances = self.calculate_distances(x, y, direction)

    # 실제 센서와 동일한 노이즈 추가
    noisy = [d + random.gauss(0, 5) for d in distances]  # 평균 0, 표준편차 5cm
    noisy = [max(2, min(400, d)) for d in noisy]  # 범위 제한

    return normalize(noisy)
```

**Step 3: 행동 지연 보상**
```python
# RC카에 look-ahead 적용
# 현재 상태가 아니라 200ms 후 예측 상태로 판단
future_state = predict_state_after_delay(current_state, current_action, delay=0.2)
action = agent.select_action(future_state)
```

**Step 4: 안전 마진 확대**
```python
# 충돌 임계값 상향 (여유 공간 확보)
self.collision_threshold = 10  # 기존
self.collision_threshold = 15  # 변경 (50% 증가)
```

**Step 5: 재학습 (Domain Adaptation)**
시뮬레이터에 노이즈와 지연을 추가한 "현실적인 시뮬레이터"에서 재학습
- 기존: 500 에피소드 학습
- 재학습: 추가 300 에피소드 (노이즈 환경)

**최종 결과:**
- 충돌률 70% → 15%로 개선
- 평균 주행 시간 12초 → 87초

**핵심 교훈:**
시뮬레이터와 현실의 차이를 최소화하려면:
1. 시뮬레이터를 현실에 가깝게 만들기 (노이즈, 지연 추가)
2. 실제 환경 데이터로 Fine-tuning
3. 안전 마진 확보

---

### Q16. 좌우 바퀴 속도 차이 문제는 어떻게 해결했나요?

**답변:**
이건 제조 공차 때문에 생기는 고전적인 로봇 문제입니다.

**문제 발견:**
```python
motor.forward(speed=60)  # 직진 명령
```
→ RC카가 계속 오른쪽으로 치우침 (3m 직진 시 50cm 편차)

**원인:**
동일한 PWM 신호(60%)를 줘도 좌우 모터의 실제 RPM이 다릅니다.
- 좌측 모터: 180 RPM
- 우측 모터: 172 RPM (약 4.5% 느림)

**캘리브레이션 과정:**

**1단계: 문제 정량화**
```python
# 직진 테스트 코드
motor.forward(speed=60)
time.sleep(5)  # 5초 직진
motor.stop()

# 결과 측정
# 시작점: (0, 0)
# 도착점: (3.2m, 0.5m)  ← 50cm 오른쪽으로 치우침
```

**2단계: 보정 계수 찾기 (Binary Search)**
```python
# 우측 모터가 느리므로 좌측을 줄여야 함
left_correction = 1.0  # 기준
right_correction = 1.0  # 초기값

# 반복 테스트
for i in range(10):
    left_correction -= 0.02  # 5% 감소 시도
    test_straight_drive()
    measure_deviation()

# 최적값: left_correction = 0.95
```

**3단계: 코드 적용**
```python
class MotorController:
    def __init__(self):
        # 캘리브레이션 계수
        self.left_correction = 0.95   # 좌측 5% 감속
        self.right_correction = 1.0   # 우측 기준

    def forward(self, speed=60):
        left_speed = speed * self.left_correction   # 57
        right_speed = speed * self.right_correction  # 60

        self.left_pwm.ChangeDutyCycle(left_speed)
        self.right_pwm.ChangeDutyCycle(right_speed)
```

**결과:**
- 3m 직진 시 편차: 50cm → 5cm 이내
- 직진 안정성 10배 향상

**추가 문제: 배터리 전압 변화**
문제: 배터리 전압이 7.4V → 6.5V로 떨어지면 다시 치우침
이유: 전압 낮으면 모터 특성 변화

**동적 보정 (선택사항):**
```python
def get_dynamic_correction(self, battery_voltage):
    # 전압에 따라 보정 계수 조정
    if battery_voltage > 7.0:
        return 0.95
    elif battery_voltage > 6.5:
        return 0.93  # 전압 낮으면 더 많이 보정
    else:
        return 0.90
```

하지만 교육용 제품이라 복잡도를 높이지 않고,
사용자 매뉴얼에 "배터리 70% 이하 시 재충전 권장"으로 해결했습니다.

---

### Q17. 라즈베리파이 재부팅 문제는 어떻게 해결했나요?

**답변:**
이거 찾는데 이틀 걸렸습니다. 정말 황당한 원인이었습니다.

**문제 상황:**
RC카가 직진 중에 갑자기 멈춤 → 5초 후 라즈베리파이 LED 깜빡이며 재부팅

**디버깅 과정:**

**의심 1: 코드 버그?**
```bash
# 로그 확인
sudo journalctl -xe

# 결과: "Under-voltage detected!" 경고
```
→ 전압 부족 문제!

**의심 2: 전원 어댑터 문제?**
- 5V 2.5A 정격 어댑터 사용 중
- 멀티미터로 측정: 5.1V (정상)
→ 어댑터는 문제없음

**진짜 원인 발견:**
```
타이밍 분석:
- RC카 정지 중: 라즈베리파이 정상 (5.1V)
- 직진 명령 순간: 전압 강하 4.3V → 재부팅!
```

**문제 구조:**
```
[7.4V 배터리]
    ├─→ [L298N 모터 드라이버] → 모터 (큰 전류 소모)
    └─→ [5V 레귤레이터] → 라즈베리파이

모터 시작 순간 → 배터리 전압 순간적으로 강하 (6.8V)
                → 레귤레이터 출력 4.3V
                → 라즈베리파이 최소 전압 4.8V 이하로 떨어짐
                → 재부팅!
```

**해결책: 전원 분리**
```
Before (단일 전원):
[7.4V 배터리] → 모터 + 라즈베리파이 (전압 간섭)

After (전원 분리):
[7.4V 배터리] → 모터만
[5V USB 어댑터] → 라즈베리파이만
[공통 GND 연결]  (전압 기준 통일)
```

**주의사항: GND 반드시 연결**
```
라즈베리파이 GND ━━━ 공통 GND ━━━ 모터 드라이버 GND
```
GND 안 연결하면 GPIO 신호 기준 전압이 달라서 오작동합니다.

**추가 개선: 캐패시터 추가**
```
[배터리] ─┬─ [모터 드라이버]
          │
          └─ [1000μF 캐패시터] (전압 안정화)
```
모터 시작 순간의 전류 급등을 캐패시터가 흡수해서 전압 안정화

**결과:**
- 재부팅 문제 100% 해결
- 추가 비용: $3 (USB 어댑터는 기존 것 사용)

**교훈:**
임베디드 시스템에서 전원 설계가 정말 중요합니다.
"소프트웨어는 완벽한데 하드웨어가 불안정" - 흔한 함정입니다.

---

## 4. 프로젝트 관리 질문

### Q18. 이 프로젝트를 혼자 했나요? 기간은 얼마나 걸렸나요?

**답변:**
네, 1인 프로젝트였고 총 **6개월** 소요되었습니다.

**타임라인:**

**1-3개월: 시뮬레이터 개발 (순조로움)**
- 1개월차: v1-v2 개발, Q-learning 검증
- 2개월차: v3-v4 개발, DQN 전환
- 3개월차: v5 개발, 캐시 시스템 추가
- 이 단계는 비교적 순조로웠음 (소프트웨어만 다룸)

**4-5개월: 하드웨어 통합 (힘들었음)**
- 4개월차:
  - 1-2주: 하드웨어 선정 및 주문 (배송 대기)
  - 3주: 센서 연결 → **센서 간섭 문제로 1주 소비**
  - 4주: 모터 제어 → **전원 재부팅 문제로 3일 소비**
- 5개월차:
  - 1-2주: 코드 이식
  - 3주: **Sim-to-Real Gap 해결로 2주 소비**
  - 4주: 모델 경량화 및 최적화

**6개월: 마무리 및 문서화**
- 최종 테스트 및 튜닝
- 교육 자료 작성 (Jupyter 노트북)
- 사용자 매뉴얼 작성
- 베타 테스트 (3개 교육기관)

**시간 분배:**
- 순수 개발: 60% (시뮬레이터 + 구현)
- 디버깅/트러블슈팅: 30% (예상보다 많이 소모)
- 문서화: 10%

**혼자 한 이유:**
- 회사 규모: 스타트업 (인력 제약)
- IoT 경험: 이전에 IoT 책 집필 → 관련 경험 있음
- 학습 기회: 처음부터 끝까지 경험하고 싶었음

**아쉬운 점:**
- 하드웨어 전문가가 있었다면 센서 간섭, 전원 문제를 1주일 만에 해결 가능했을 것
- 하지만 혼자 해결하면서 더 많이 배움

---

### Q19. 프로젝트 우선순위를 어떻게 정했나요?

**답변:**
명확한 우선순위 원칙이 있었습니다.

**핵심 원칙: "작동하는 것 먼저" (Working First)**

**Phase 1: 개념 검증 (PoC)**
- 목표: "강화학습으로 자율주행이 가능한가?" 검증
- 우선순위: 빠른 프로토타입 > 완벽한 설계
- 결과: v1 (Q-learning)로 2주 만에 검증 완료

**Phase 2: 확장성 확보**
- 목표: 실제 환경에 적용 가능한 수준으로 개선
- 우선순위:
  1. DQN 전환 (확장성)
  2. 센서 시뮬레이션 (현실성)
  3. 학습 안정화 (신뢰성)

**Phase 3: 하드웨어 적용**
- 목표: 실제 RC카 작동
- 우선순위:
  1. **안전성** (충돌 방지) - 최우선!
  2. 성능 (목적지 도달률)
  3. 속도 (실시간성)
  4. 비용 (교육용 가격)

**의사결정 예시:**

**선택 1: 카메라 vs 초음파**
- 성능: 카메라 > 초음파
- 비용: 초음파 > 카메라 ($8 vs $80+)
- 교육 효과: 초음파 > 카메라 (이해 쉬움)
→ **결정: 초음파 선택** (교육용 목적에 부합)

**선택 2: Jetson Nano vs 라즈베리파이**
- 성능: Jetson Nano > 라즈베리파이
- 비용: 라즈베리파이 > Jetson Nano ($55 vs $99)
- 교육 자료: 라즈베리파이 > Jetson Nano (풍부함)
→ **결정: 라즈베리파이** (충분한 성능 + 저비용)

**선택 3: 완벽한 성능 vs 빠른 출시**
- 목적지 도달률 85% 달성 (목표: 95%)
- 추가 2개월 투자하면 95% 가능할 것으로 예상
- 하지만 교육용으로 85%도 충분
→ **결정: 현재 수준에서 출시** (80/20 법칙)

**우선순위 변경 경험:**
초기에는 "성능"을 최우선으로 생각했지만,
실제 하드웨어 작업하면서 **"안정성"**이 더 중요함을 깨달음.
- 90% 성능이지만 5분에 1번 재부팅 → 사용 불가
- 80% 성능이지만 안정적 → 교육용으로 충분

---

### Q20. 만약 다시 한다면 뭘 다르게 하시겠어요?

**답변:**
몇 가지 아쉬운 점과 개선 방향이 있습니다.

**1. 하드웨어 프로토타입을 더 빨리 만들기**
- 실제: 3개월 시뮬레이터 → 2개월 하드웨어
- 개선: 1개월 시뮬레이터 → 병행하며 하드웨어 테스트

**이유:**
시뮬레이터에서 완벽하게 만들었다고 생각했는데, 실제 환경은 완전히 달랐습니다.
센서 노이즈, 모터 지연 같은 문제는 실제로 만들어봐야만 알 수 있습니다.

→ **"Fail Fast" 원칙**: 일찍 실패하고 빨리 배우기

**2. 전원 설계를 처음부터 분리**
- 실제: 단일 전원으로 시작 → 재부팅 문제 → 전원 분리
- 개선: 처음부터 라즈베리파이와 모터 전원 분리 설계

**이유:**
전원 문제로 2일 날렸습니다. 하드웨어 경험자에게 미리 리뷰받았으면 피할 수 있었을 것입니다.

→ **교훈**: 모르는 영역은 전문가 조언 구하기

**3. 로깅 시스템 먼저 구축**
- 실제: 문제 생긴 후에 로그 추가
- 개선: 처음부터 센서 데이터, 행동, 보상을 모두 기록

```python
# 이렇게 했어야 함
class DataLogger:
    def log(self, timestamp, sensor_data, action, reward, state):
        # CSV로 저장
        # 나중에 분석 가능
        pass
```

**이유:**
"왜 여기서 충돌했지?" → 데이터가 없어서 추측만 가능
로그가 있었다면 패턴 분석으로 빠르게 원인 파악 가능

**4. 단위 테스트 작성**
- 실제: 통합 테스트 위주
- 개선: 각 모듈별 단위 테스트

```python
# 센서 테스트
def test_ultrasonic_sensor():
    sensor = UltrasonicSensor(trig=23, echo=24)
    distance = sensor.get_distance()
    assert 2 <= distance <= 400  # 유효 범위

# 모터 테스트
def test_motor_forward():
    motor = MotorController()
    motor.forward(speed=50)
    # 실제 RPM 측정
    assert 170 <= rpm <= 190
```

**이유:**
센서 문제인지, 모터 문제인지, 알고리즘 문제인지 구분이 어려웠습니다.
각 부분을 독립적으로 검증했다면 디버깅이 훨씬 쉬웠을 것입니다.

**5. 버전 관리 더 체계적으로**
- 실제: Git 사용했지만 커밋 메시지 대충
- 개선: 의미 있는 커밋 메시지 + 태그

```bash
# 이렇게 하지 말고
git commit -m "fix"

# 이렇게
git commit -m "fix: 센서 간섭 문제 해결 - 15ms 대기 추가"
git tag -a v5.2-sensor-fix -m "센서 안정화 버전"
```

**하지만 잘한 점도:**
- 시뮬레이터부터 만든 것: 하드웨어 없이도 알고리즘 검증 가능
- 점진적 개선 (v1→v5): 각 단계마다 작동하는 버전 유지
- 문서화: 나중에 교육 자료로 활용 가능

---

## 5. 하드웨어 질문

### Q21. GPIO 프로그래밍 경험이 있었나요?

**답변:**
이전에 IoT 책을 쓰면서 기본적인 GPIO 경험은 있었지만, 이렇게 복잡한 프로젝트는 처음이었습니다.

**이전 경험 (IoT 책 집필):**
- LED 제어: digitalWrite (ON/OFF)
- 버튼 입력: digitalRead
- 온도 센서(DHT11): 단순 데이터 읽기
- 수준: "입문자용 예제" 수준

**이번 프로젝트의 도전:**
- PWM 제어: 모터 속도 조절 (복잡도 UP)
- 타이밍 민감: 초음파 센서 10μs 펄스 (정밀도 필요)
- 다중 센서: 4개 센서 동시 관리 (간섭 고려)
- 인터럽트: 실시간 응답 필요

**새로 배운 것들:**

**1. PWM (Pulse Width Modulation)**
```python
# 이전: ON/OFF만
GPIO.output(LED_PIN, GPIO.HIGH)

# 이번: 속도 조절 (Duty Cycle)
pwm = GPIO.PWM(MOTOR_PIN, 1000)  # 1000Hz
pwm.ChangeDutyCycle(60)  # 60% 출력
```
- Duty Cycle 50% → 평균 전압 3.3V
- Duty Cycle 100% → 평균 전압 5V
- 모터 속도 제어의 핵심

**2. 타이밍 제어**
```python
# 초음파 센서: 10μs 트리거 펄스
GPIO.output(TRIG, True)
time.sleep(0.00001)  # 10μs (매우 짧음!)
GPIO.output(TRIG, False)

# 에코 측정 (μs 단위 정밀도)
pulse_start = time.time()
while GPIO.input(ECHO) == 0:
    pulse_start = time.time()
pulse_duration = pulse_end - pulse_start
```

**3. 전원 관리**
- 3.3V vs 5V 로직 레벨 차이
- 전류 소모 계산 (센서 4개 + 모터)
- Under-voltage 문제 대응

**학습 방법:**
1. **공식 문서**: RPi.GPIO 라이브러리 문서 정독
2. **커뮤니티**: Raspberry Pi 포럼에서 유사 사례 검색
3. **실험**: 오실로스코프로 신호 파형 확인
4. **삽질**: 센서 태우기 (5V → 3.3V 핀에 연결... RIP)

**팁:**
GPIO 프로그래밍은 "책으로만" 배우기 어렵습니다.
실제로 부품 연결하고, 잘못 연결해서 부품 태워보고, 다시 시도하면서 배웁니다.

---

### Q22. L298N 모터 드라이버는 왜 필요한가요? GPIO로 직접 제어 안 되나요?

**답변:**
전류 문제 때문입니다. GPIO 핀은 모터를 직접 구동하기에 전류가 부족합니다.

**전류 비교:**
```
라즈베리파이 GPIO:
- 출력 전류: 16mA (1개 핀 기준)
- 전체 GPIO 최대: 50mA (모든 핀 합계)

DC 모터:
- 작동 전류: 200~300mA (정상)
- 시동 전류: 500~800mA (순간)
```
→ GPIO로 직접 연결하면 **라즈베리파이 고장 납니다!**

**L298N의 역할:**

**1. 전류 증폭**
```
라즈베리파이 GPIO (16mA)
    ↓ 제어 신호
L298N 드라이버
    ↓ 전력 신호 (1A)
DC 모터
```
- GPIO는 "제어 신호"만 보냄 (ON/OFF 명령)
- L298N이 실제 "전력"을 모터에 공급

**2. 전압 분리**
```
라즈베리파이: 3.3V 로직, 5V 전원
모터: 7.4V 필요
```
L298N은 두 전압 시스템을 분리해줍니다.

**3. 방향 제어 (H-Bridge)**
```
IN1=HIGH, IN2=LOW  → 정방향 (전진)
IN1=LOW, IN2=HIGH  → 역방향 (후진)
IN1=LOW, IN2=LOW   → 정지
```
4개 트랜지스터로 모터 방향 전환 (H-Bridge 회로)

**연결 구조:**
```
[라즈베리파이]                [L298N]                [모터]
GPIO 12 (PWM) ──────────→ ENA ───┐
GPIO 17 ─────────────────→ IN1 ──┼─→ OUT1,OUT2 ──→ 모터
GPIO 18 ─────────────────→ IN2 ──┘
                           12V ──→ [배터리 7.4V]
GND ──────────────────────→ GND
```

**대안은 없나요?**
있습니다! 하지만 각각 장단점이 있습니다.

| 드라이버 | 최대 전류 | 가격 | 용도 |
|---------|----------|------|------|
| L298N | 2A | $6 | 교육용 (튼튼함) |
| TB6612 | 1.2A | $4 | 소형 RC카 |
| DRV8833 | 1.5A | $3 | 경량화 프로젝트 |

L298N을 선택한 이유:
- 교육용: 회로가 단순해서 설명하기 쉬움
- 안정성: 과전류 보호 기능 내장
- 호환성: 대부분의 DC 모터와 호환

**실수 사례:**
처음에 "GPIO로 직접 LED 켰으니 모터도 되겠지?"라고 생각했습니다.
→ GPIO 16mA → 모터 300mA 필요 → 라즈베리파이 GPIO 보호 회로 작동 (다행히 고장 안 남)
→ L298N 구매 ($6)

**교훈:**
전력 전자는 조심해야 합니다. 전류를 계산하고, 적절한 드라이버를 사용하세요!

---

### Q23. 배터리는 왜 7.4V를 선택했나요?

**답변:**
모터 사양과 라즈베리파이 전원 분리를 고려한 결정입니다.

**모터 사양:**
- DC 모터 정격: 6V~12V
- 최적 동작 전압: 7.4V (제조사 권장)
- 6V: 느림 (RPM 부족)
- 12V: 빠름 but 과열 위험

**배터리 선택지:**

**1. 18650 리튬이온 배터리**
```
1셀(1S): 3.7V → 너무 낮음 (모터 느림)
2셀(2S): 7.4V → 선택! (최적)
3셀(3S): 11.1V → 과함 (모터 과열)
```

**2. 니켈수소(NiMH) 배터리**
```
AA 6개 직렬: 7.2V (1.2V x 6)
```
- 장점: 저렴, 구하기 쉬움
- 단점: 무겁고, 용량 적음 (2000mAh vs 3000mAh)

**3. LiPo 배터리**
```
2S LiPo: 7.4V
```
- 장점: 가볍고 출력 높음
- 단점: 폭발 위험 (교육용으로 부적합!)

→ **최종 선택: 18650 2S (7.4V, 3000mAh)**

**용량 계산:**
```
소비 전력:
- 라즈베리파이: 5V 1A = 5W
- 모터 2개: 7.4V 0.6A = 4.4W
- 센서 4개: 5V 0.1A = 0.5W
합계: 약 10W

배터리 용량:
- 7.4V 3000mAh = 22.2Wh

사용 시간:
22.2Wh / 10W = 2.2시간 (이론값)
실제: 약 2시간 (효율 90%)
```

**충전 방식:**
```
18650 배터리 → BMS (Battery Management System) → 충전기
```
- BMS: 과충전, 과방전 보호
- 비용: $3 추가

**안전 장치:**
```python
# 전압 모니터링 (INA219 센서)
if battery_voltage < 6.5:
    print("배터리 부족! 충전 필요")
    motor.stop()
    # 안전하게 종료
```

**대안: USB 파워뱅크?**
고려했지만:
- 5V 출력 → 모터에 부족
- 승압 컨버터 필요 (5V → 7.4V) → 효율 떨어짐
→ 차라리 7.4V 배터리 직접 사용이 낫다

---

## 6. 성장 및 학습 질문

### Q24. 이 프로젝트를 통해 가장 많이 성장한 부분은?

**답변:**
**"소프트웨어와 하드웨어의 통합"** 능력이 가장 크게 성장했습니다.

**이전의 나:**
- 배경: 소프트웨어 엔지니어 (Python, 웹)
- 경험: IoT 책 집필 (기초 수준)
- 한계: "코드만 짜면 해결될 거야"라는 생각

**프로젝트 중 깨달음:**
```
소프트웨어적 해결책:
- 센서 간섭 → 필터링 알고리즘으로 해결?
- 결과: 실패 (근본 원인은 물리적 간섭)

하드웨어적 해결책:
- 센서 간 시간 간격 15ms + 물리적 차폐벽
- 결과: 성공!
```

→ **교훈**: 문제의 본질을 파악하고, SW/HW 통합 솔루션 찾기

**구체적 성장 영역:**

**1. 시스템 사고 (Systems Thinking)**
```
[Before] 모듈별 개발
센서 ─→ 알고리즘 ─→ 모터
각자 독립적으로 개발

[After] 전체 시스템 고려
센서 ←─→ 전원 ←─→ 모터
       ↕
     라즈베리파이
모든 것이 서로 영향을 줌
```

**예시:**
- 모터 구동 → 전압 강하 → 센서 오작동
- 센서 측정 중 → 모터 대기 → 반응 속도 저하
→ 트레이드오프를 이해하고 균형점 찾기

**2. 디버깅 능력**
```
[Before] print() 디버깅
문제 → print 찍어보기 → 해결

[After] 계층적 디버깅
1. 증상 관찰 (재부팅)
2. 로그 분석 (journalctl)
3. 전압 측정 (멀티미터)
4. 타이밍 분석 (오실로스코프)
5. 가설 수립 → 실험 → 검증
```

**3. 현실 세계의 불완전성 수용**
```
[Before] 완벽한 시뮬레이터
- 센서 노이즈 0%
- 정확한 제어
- 재현 가능

[After] 지저분한 현실
- 센서 노이즈 ±5cm
- 모터 슬립
- 배터리 전압 변화
```
→ "좋은 소프트웨어 = 현실의 불완전성을 다루는 코드"

**4. 기술 선택 능력**
```
[Before] 최신 기술 선호
"GPU 쓰면 빠르겠지?"
"YOLO 쓰면 멋있겠지?"

[After] 목적에 맞는 기술
"교육용이니 초음파면 충분"
"라즈베리파이로도 45ms 달성"
```
→ 과잉 엔지니어링 지양, 실용성 중시

**수치로 보는 성장:**
- 하드웨어 디버깅 시간: 3일 → 1시간 (프로젝트 후반)
- 문제 해결 정확도: 추측 기반 → 데이터 기반
- 전원 설계 이해도: 0% → 70% (여전히 배울 게 많음)

**앞으로의 목표:**
- PCB 설계 배우기 (브레드보드 벗어나기)
- RTOS (Real-Time OS) 경험
- 양산 설계 (프로토타입 → 제품)

---

### Q25. 강화학습을 처음 배우는 거였나요? 어떻게 공부했나요?

**답변:**
네, 이 프로젝트가 강화학습 첫 경험이었습니다.

**학습 과정:**

**Phase 1: 개념 이해 (2주)**

**교재:**
1. **"Reinforcement Learning: An Introduction" (Sutton & Barto)**
   - 1~4장 정독 (MDP, Bellman Equation, Q-learning)
   - 나머지는 필요할 때 참고

2. **OpenAI Spinning Up**
   - 온라인 무료 자료
   - DQN 알고리즘 설명이 명확함

**핵심 개념 정리:**
```
강화학습 = 시행착오 학습

State (상태) → Agent (에이전트) → Action (행동)
              ↑                        ↓
         Reward (보상) ← Environment (환경)

목표: 누적 보상 최대화
방법: Q-function 학습
  Q(s, a) = "상태 s에서 행동 a를 했을 때 미래 보상 총합"
```

**Phase 2: 실습 (3주)**

**1. Gym 환경으로 연습**
```python
import gym

env = gym.make('CartPole-v1')

# Q-learning 직접 구현
# 1000줄 짜리 코드 작성 → 작동 안 됨
# 디버깅 → 수정 → 재시도

# 3일 만에 CartPole 해결!
```

**2. 논문 읽기: DQN (Mnih et al., 2015)**
- "Playing Atari with Deep Reinforcement Learning"
- 처음엔 이해 안 됨 → 코드 구현하면서 이해
- GitHub에서 구현 예제 찾아서 분석

**3. PyTorch 튜토리얼**
```python
# PyTorch 공식 DQN 튜토리얼
# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html

# 한 줄씩 따라 치면서 이해
# 200줄짜리 코드 → 내 프로젝트에 적용
```

**Phase 3: 응용 (프로젝트 기간 내내)**

**실험 노트 작성:**
```
실험 1: Epsilon=1.0 고정
결과: 랜덤하게만 움직임 (학습 안 됨)
원인: 탐험만 하고 활용 안 함

실험 2: Epsilon=0.0 고정
결과: 초기 행동만 반복 (학습 느림)
원인: 활용만 하고 탐험 안 함

실험 3: Epsilon decay (1.0 → 0.01)
결과: 500 에피소드 만에 80% 성공률!
```

**어려웠던 점:**

**1. 수학 (Bellman Equation)**
```
Q(s, a) = r + γ * max Q(s', a')
```
- 처음: "이게 뭔 소리야?"
- 해결: 직접 손으로 계산해봄 (3x3 격자에서)
- 깨달음: "아, 현재 보상 + 미래 최대 보상이구나!"

**2. 코드 구현 (신경망)**
```python
# 신경망 구조를 어떻게 짜야 하지?
# 입력: 8차원
# 출력: 3차원 (Q-value)
# 히든 레이어는 몇 개? 뉴런은?

# 답: 실험으로 찾기
# [8] → [128] → [128] → [3]  (성공!)
```

**3. 하이퍼파라미터 튜닝**
- Learning rate: 0.001 vs 0.0001?
- Batch size: 32 vs 64?
- Gamma (할인율): 0.9 vs 0.99?

→ 실험 100번 돌려보면서 최적값 찾음

**학습 팁:**

**1. 코드부터 짜기**
- 이론만 읽으면 이해 안 됨
- 직접 구현하면서 "아하!" 순간 옴

**2. 작은 문제부터**
- CartPole (1D) → GridWorld (2D) → RC카 (실제)
- 단계적으로 난이도 높이기

**3. 시각화**
```python
# 학습 과정 그래프
plt.plot(episode, reward)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
```
- 그래프로 보면 학습 여부 한눈에 파악

**4. 커뮤니티 활용**
- Reddit r/reinforcementlearning
- Stack Overflow
- GitHub Issues

**결과:**
- 강화학습 입문 → 실제 로봇에 적용
- 이론 70% + 실습 30% = 프로젝트 성공

---

### Q26. 다음에 해보고 싶은 개선이나 후속 프로젝트가 있나요?

**답변:**
몇 가지 흥미로운 확장 방향을 생각하고 있습니다.

**단기 개선 (3개월 내):**

**1. 웹 대시보드 (원격 모니터링)**
```python
# Flask 서버로 실시간 데이터 전송
from flask import Flask, jsonify

@app.route('/telemetry')
def get_telemetry():
    return jsonify({
        'sensors': [85, 120, 45, 90],
        'position': (x, y),
        'battery': 7.2,
        'score': 145
    })
```
- 웹에서 RC카 상태 실시간 확인
- 비상 정지 버튼
- 학습 진행 상황 그래프

**2. 멀티 에이전트 협업**
```
RC카 2대:
- 서로 통신 (WiFi)
- 역할 분담: 1대는 정찰, 1대는 물류 운반
- 강화학습: 협력 보상 추가
```
- 교육 효과: 분산 시스템 개념 학습
- 기술: Multi-Agent RL (MARL)

**3. 간단한 비전 추가 (저해상도)**
```
라즈베리파이 카메라 (64x64 해상도)
    ↓
TFLite MobileNet (사전 학습)
    ↓
객체 감지 (사람, 장애물)
    ↓
초음파와 융합 (Sensor Fusion)
```
- 비용: +$15
- 추론 시간: +30ms (총 75ms, 여전히 실시간)
- 장점: 투명 물체, 얇은 막대 감지 가능

**중기 프로젝트 (6개월~1년):**

**1. SLAM (Simultaneous Localization and Mapping)**
```
목표: RC카가 스스로 지도 작성
센서: 초음파 4개 + IMU(관성센서)
알고리즘: Particle Filter or EKF-SLAM
```
- 시작 위치 모를 때도 자율 주행
- 실내 지도 자동 생성
- 교육 효과: 로봇공학 심화 과정

**2. 실외 버전 (GPS + 태양광)**
```
센서 추가:
- GPS 모듈 ($20)
- 자이로/가속도계 ($5)
- 태양광 패널 (선택)

목표:
- 실외 자율 주행 (공원, 운동장)
- 긴 거리 미션 (50m+)
```

**3. 교육 플랫폼화**
```
웹 기반 교육 플랫폼:
1. 학생들이 브라우저에서 코드 작성
2. 시뮬레이터에서 테스트
3. 실제 RC카에 배포 (원격)
4. 결과 확인 및 순위 (게임화)
```
- 목표: "자율주행 코딩 경진대회"
- 비즈니스: 구독 모델 ($10/월)

**장기 비전 (1년+):**

**1. 실제 자동차 크기 (1:10 스케일)**
```
현재: 30cm RC카
목표: 1m 정도 차량

추가 요소:
- 조향 모터 (스티어링)
- 서스펜션 (울퉁불퉁한 지면)
- 더 강력한 센서 (라이다)
- Jetson Nano (더 강력한 컴퓨팅)
```

**2. 실제 도로 주행 (규제 범위 내)**
```
시나리오:
- 폐쇄된 테스트 트랙
- 차선 인식
- 신호등 인식
- 장애물 회피

법적 이슈:
- 자동차관리법 확인
- 보험 가입
- 안전 기준 준수
```

**3. 오픈소스 프로젝트화**
```
GitHub 공개:
- 하드웨어 설계 (회로도)
- 소프트웨어 (코드)
- 교육 자료 (튜토리얼)

커뮤니티:
- 다른 개발자들의 기여
- 다양한 센서 조합 시도
- 경진대회 개최
```

**개인적으로 가장 하고 싶은 것:**
**"Sim-to-Real 연구"** - 시뮬레이터에서 학습한 모델을 실제 환경에 더 효과적으로 전이하는 방법
- Domain Randomization
- Reality Gap 최소화 기법
- Transfer Learning 적용

이 분야는 아직 연구가 활발하고, 실용적 가치도 높아서 논문 수준의 기여도 가능할 것 같습니다.

---

## 📌 면접 대비 핵심 포인트 정리

### 질문 구성 (총 26개)
1. **기술 개념 질문 (9개)**: DQN, 강화학습, Q-learning, Exploration, 카메라 미사용, 보상 설계, **CNN**, **PPO**, **TFLite**
2. **구현 및 설계 질문 (4개)**: 라즈베리파이 선택, 시뮬레이터 5버전, ActionCache, 모델 경량화
3. **트러블슈팅 질문 (4개)**: 센서 간섭, Sim-to-Real Gap, 바퀴 속도 차이, 재부팅 문제
4. **프로젝트 관리 질문 (3개)**: 기간/역할, 우선순위, 개선점
5. **하드웨어 질문 (3개)**: GPIO, 모터 드라이버, 배터리
6. **성장 및 학습 질문 (3개)**: 성장 부분, 강화학습 공부, 다음 프로젝트

### 기술적 깊이 보여주기
1. **개념 설명**: 단순 정의 → 프로젝트 적용 예시
2. **트레이드오프**: "A vs B" 비교 및 선택 근거
   - CNN vs 초음파: 교육 목적, 비용, 복잡도
   - DQN vs PPO: 이산 행동 vs 연속 행동
   - PyTorch vs TFLite: 학습 vs 추론
3. **수치 제시**: "약 2배 개선" (구체적)

### STAR 기법 활용
- **Situation**: 어떤 상황이었나?
- **Task**: 무엇을 해결해야 했나?
- **Action**: 어떻게 접근했나?
- **Result**: 결과는? (수치화)

### 실패 경험도 가치 있게
- "센서 간섭 문제로 3일 고생" → 포기 X
- "물리적 원리 이해" + "SW/HW 통합 해결"
- → 성장 스토리

### 열정과 호기심 표현
- "다음에 해보고 싶은 것"
- "배우면서 느낀 점"
- "왜 이 기술을 선택했는지" (CNN, PPO를 안 쓴 이유)

### 🔥 면접 단골 질문 대비
**Q7-Q9 (새로 추가)는 면접에서 자주 나옵니다!**
- **"CNN 써보셨어요?"** → Q7로 대응
- **"왜 DQN이고 PPO는 안 썼나요?"** → Q8로 대응
- **"TFLite가 뭔가요?"** → Q9로 대응

---

**면접 준비 팁:**
1. 각 답변을 소리 내어 연습 (1-3분 분량)
2. 기술 용어는 쉽게 풀어서 설명할 수 있어야 함
3. 프로젝트 데모 영상 준비 (실제 작동 모습)
4. GitHub 레포지토리 정리 (코드 + README)
5. **새로 추가된 질문 (Q7~Q9) 집중 연습**: 기술 비교 능력 보여주기!

**화이팅!** 🚀
