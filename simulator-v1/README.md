# Simulator v1 - 과적합 문제가 있는 원본 버전

## 📋 프로젝트 설명

이 버전은 **과적합(Overfitting) 문제가 발생하는 원본 버전**입니다.
강화학습의 과적합 문제를 이해하고 해결 방법을 학습하기 위한 교육용 예제입니다.

## 🧠 에이전트가 받는 정보 (상태 벡터)

### 에이전트는 무엇을 알고 출발하는가?

**중요**: 에이전트는 목적지의 **절대 좌표를 모릅니다**. 대신 **상대적 방향**만 알고 움직입니다.

#### 상태 벡터 구성 (총 11차원)

```python
상태 = [
    # 1. 주변 8칸 장애물 정보 (8차원)
    wall_up_left, wall_up, wall_up_right,
    wall_left,              wall_right,
    wall_down_left, wall_down, wall_down_right,

    # 2. 현재 방향 (1차원, 정규화)
    direction / 4.0,  # 0=위, 1=오른쪽, 2=아래, 3=왼쪽

    # 3. 목적지 상대 방향 (2차원, 정규화) ⭐ 핵심!
    dx_to_goal / GRID_WIDTH,   # "목적지가 X축으로 저쪽"
    dy_to_goal / GRID_HEIGHT   # "목적지가 Y축으로 저쪽"
]
```

#### ❌ 에이전트가 모르는 것
- 목적지의 절대 좌표 (goal_x, goal_y)
- "목적지가 맵의 (50, 50)이다" 같은 숫자 정보
- 전체 맵의 구조 (전지적 시점 불가)

#### ✅ 에이전트가 아는 것
- **현재 위치에서 목적지가 어느 방향인지** (상대 방향)
  ```python
  dx_to_goal = +0.5  # 오른쪽으로 가야 함
  dy_to_goal = -0.3  # 위쪽으로 가야 함
  ```
- 주변 8칸의 장애물 위치
- 현재 바라보는 방향

#### 🎯 목적 자체는 알고 있는가?

**네, 알고 있습니다!** (보상 설계를 통해)

```python
# 보상 설계
if 목적지 도달:
    reward = +100  # 큰 양의 보상
elif 목적지에 가까워짐:
    reward = +0.5  # 작은 양의 보상
elif 충돌:
    reward = -100  # 큰 음의 보상
```

에이전트는 이 보상을 통해:
- "목적지에 도달하는 것이 좋다"
- "목적지로 가까워지는 것이 좋다"
- "충돌은 나쁘다"

를 학습하므로, **목적 자체(목적지로 가는 것)는 알고 시작**합니다.

#### 🤔 그럼 어떻게 움직이는가?

**매 순간마다:**
1. "지금 위치에서 목적지가 저쪽 방향이다" (상대 방향)
2. "주변에 벽이 이렇게 있다" (장애물 정보)
3. "직진/좌회전/우회전 중 어떤 게 좋을까?" (Policy 판단)
4. 행동 실행
5. 보상 받기 ("목적지에 가까워졌네!" or "충돌했네!")

→ **"저 방향으로 가면 보상이 좋다"는 수준으로 학습하는 구조**

#### 비유
```
❌ GPS 네비게이션 (절대 좌표)
"서울시 강남구 테헤란로 123번지로 가세요"

✅ 나침반 (상대 방향)
"목적지가 북동쪽 방향입니다. 주변 장애물을 피하며 그쪽으로 가세요"
```

---

## 🎯 주요 특징

- **단일 맵**: 하나의 고정된 맵에서만 학습
- **고정된 시작점/목적지**: 항상 (2, 2) → (27, 27)
- **고정된 초기 방향**: 항상 위쪽(0) 방향으로 시작
- **빠른 Epsilon Decay**: 0.995 (탐험 시간 부족)
- **Gradient Clipping 없음**: 학습 불안정
- **경로 다양성 보상 없음**: 같은 경로만 반복

## ⚠️ 문제점 (과적합 발생)

### 1. 한쪽으로만 이동하는 문제

**증상:**
- 에이전트가 한 번 최적 경로를 찾으면 계속 그 경로만 사용
- 오른쪽/아래쪽으로만 이동
- 왼쪽/위쪽 방향을 거의 사용하지 않음

**원인:**
- 고정된 초기 방향 (항상 위쪽)
- 매 에피소드마다 같은 위치, 같은 방향에서 시작
- 왼쪽/위쪽 방향을 탐험할 기회가 없음

### 2. 특정 경로에 고착화

**증상:**
- 학습 초기에 찾은 경로만 계속 사용
- 다른 경로를 시도하지 않음
- 경로 다양성이 없음

**원인:**
- 빠른 Epsilon Decay (0.995)
- 100 에피소드만 지나도 epsilon이 0.6 이하로 떨어짐
- 탐험 시간이 부족함
- 경로 다양성 보상이 없음

### 3. 단일 맵 학습

**증상:**
- 특정 맵의 특정 경로만 암기
- 다른 맵에서 작동하지 않음
- 일반화 불가능

**원인:**
- 하나의 맵만 반복 학습
- 고정된 시작점/목적지
- 맵 구조가 항상 동일

### 4. 학습 불안정성

**증상:**
- 학습이 불안정함
- 가중치가 너무 커질 수 있음
- 성능이 일정하지 않음

**원인:**
- Gradient Clipping 없음
- 정규화 없음

## 📊 예상되는 학습 결과

### Episode 0-100: 초보 단계 🔴
- AI가 랜덤하게 움직임 (하지만 위쪽에서 시작)
- 벽에 자주 부딪힘
- 평균 3-5스텝 생존
- **오른쪽/아래쪽으로만 이동하는 패턴 시작**

### Episode 100-300: 학습 시작 🟡
- 조금씩 패턴 발견
- 벽을 피하기 시작
- 10-20스텝 생존
- **한쪽으로만 이동하는 패턴 강화**

### Episode 300-500: 과적합 발생 ❌
- 벽을 거의 안 부딪힘
- 100-500스텝 생존
- **하지만 항상 같은 경로만 사용**
- **다른 경로를 시도하지 않음**
- **왼쪽/위쪽 방향을 거의 사용하지 않음**

## 🚀 사용 방법

### 1. 패키지 설치

```bash
python -m pip install pygame-ce numpy torch matplotlib
```

### 2. 훈련 시작

```bash
cd simulator-v1
python train.py
```

### 3. 테스트

```bash
python main.py
```

## 📈 학습 결과 분석

훈련 후 다음을 확인하세요:

1. **경로 다양성**: 항상 같은 경로만 사용하는지 확인
2. **방향 사용**: 왼쪽/위쪽 방향을 사용하는지 확인
3. **일반화**: 다른 맵에서도 작동하는지 확인 (작동하지 않을 것)

## 🔍 문제 확인 방법

1. **시각적 확인**: 화면에서 자동차가 항상 같은 경로로 가는지 확인
2. **방향 분석**: 왼쪽/위쪽 방향을 사용하는지 확인
3. **다른 맵 테스트**: 맵을 바꿔서 테스트 (작동하지 않을 것)

## 📚 다음 단계

이 버전의 문제점을 이해했다면:

1. **v2 확인**: 과적합 해결 방법을 적용한 버전
2. **v3 확인**: 여러 맵과 랜덤 시작점/목적지를 사용하는 완전한 일반화 버전
3. **v4 확인**: Learning Rate Scheduling을 적용한 학습 속도 향상 버전
4. **v5 확인**: Policy > Cache 원칙을 구현한 강화학습 철학 이해 버전

## 💡 학습 목표

이 버전을 통해 다음을 학습할 수 있습니다:

- 과적합 문제의 원인 이해
- 고정된 초기 조건의 문제점
- 빠른 Epsilon Decay의 문제점
- 단일 맵 학습의 한계
- 경로 다양성의 중요성

---

## 🔮 다음 단계 개선 사항

이 버전의 문제점들은 다음 버전에서 해결됩니다:

### ✅ v2에서 해결될 사항
- **랜덤 초기 방향**: 모든 방향 탐험 가능
- **느린 Epsilon Decay**: 충분한 탐험 시간 확보
- **Gradient Clipping**: 학습 안정성 향상
- **경로 다양성 보상**: 다양한 경로 탐험 유도

### 📋 현재 알려진 문제점
1. **한쪽으로만 이동**: 고정된 초기 방향으로 인한 문제
2. **경로 고착화**: 빠른 Epsilon Decay로 인한 문제
3. **학습 불안정**: Gradient Clipping 없음
4. **단일 맵 학습**: 일반화 불가능

**다음 버전**: [`simulator-v2/README.md`](../simulator-v2/README.md)에서 해결 방법 확인

---

**⚠️ 주의**: 이 버전은 교육용으로, 실제 프로젝트에는 사용하지 마세요!
